{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "## Imports ##\n",
    "#############\n",
    "\n",
    "# external libraries\n",
    "import random\n",
    "import math\n",
    "import igraph\n",
    "import nltk\n",
    "import importlib\n",
    "import csv\n",
    "import pickle\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as light\n",
    "\n",
    "\n",
    "# our outsourced code\n",
    "import features_nodewise as nw\n",
    "import features_pairwise as pw\n",
    "import preprocessing as prep\n",
    "import multi_func as mf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## Read data ##\n",
    "###############\n",
    "\n",
    "# Training\n",
    "with open(\"./data/training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "    training_set = [element[0].split(\" \") for element in training_set]\n",
    "    \n",
    "# Testing -> we call it competition_set\n",
    "with open (\"./data/testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    competition_set = list(reader)\n",
    "    competition_set = [element[0].split(\" \") for element in competition_set]\n",
    "# Node info\n",
    "with open(\"./data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "abstracts = [element[5] for element in node_info]\n",
    "titles = [element[2] for element in node_info]\n",
    "IDs = [element[0] for element in node_info]\n",
    "publication_years = prep.to_feature_shape([int(info[1]) for info in node_info])\n",
    "train_true_labels = [int(element[2]) for element in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 27770, n_features: 2890\n",
      "n_samples: 27770, n_features: 10000\n",
      "n_samples: 27770, n_features: 150000\n",
      "Performing dimensionality reduction using LSA\n",
      "Explained variance of the SVD step: 16%\n",
      "Performing dimensionality reduction using LSA\n",
      "Explained variance of the SVD step: 7%\n",
      "[nltk_data] Downloading package punkt to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "## Preprocess data ##\n",
    "#####################\n",
    "\n",
    "# Construct term-document-tfidf matrices\n",
    "t_titles = prep.tfidf(titles)\n",
    "t = prep.tfidf(abstracts)\n",
    "t_ngrams = prep.tfidf(abstracts, r= (2,3), midf = 2, madf=0.5,feats=150000, sublinear = True)\n",
    "\n",
    "# Reduce matrix dimensionality\n",
    "l = nw.LSA(t,n_components=100)\n",
    "l_ngrams = nw.LSA(t_ngrams,n_components=300)\n",
    "\n",
    "# Build KDTrees to accelerate nearest-neighbour searches\n",
    "kdtree = nw.KDTree(l)\n",
    "kdtree_n = nw.KDTree(l_ngrams)\n",
    "\n",
    "# Build graph from gold standard training data\n",
    "train_IDs = np.array(IDs)\n",
    "train_edges = [(element[0],element[1]) for element in training_set if element[2]=='1']\n",
    "train_graph = prep.article_graph(train_IDs,train_edges)\n",
    "\n",
    "# dicts to accelerate ID <-> index searches\n",
    "node_dict = prep.to_dict(IDs,range(len(node_info)))\n",
    "index_dict = prep.to_dict(range(len(IDs)),IDs)\n",
    "\n",
    "# Load stemmer and stopwords for later title & abstract processing\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## Create dict to store features ##\n",
    "###################################\n",
    "competition_features_dict = dict()\n",
    "\n",
    "all_feature_names = ['overlap_title',\n",
    "                      'comm_auth',\n",
    "                      'temp_diff',\n",
    "                      'citation_check',\n",
    "                      'max_sim',\n",
    "                      'peer_popularity',\n",
    "                      'succ_pred',\n",
    "                      'LSA_distance',\n",
    "                      'title_sim',\n",
    "                      'temporal_fit',\n",
    "                      'N_LSA_distance',\n",
    "                      'path_length',\n",
    "                      'node_degree',\n",
    "                      'reverse_max_sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 5\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 1535\n",
      "1000 / 1535\n",
      "overlap_title 1535\n",
      "comm_auth 1535\n",
      "temp_diff 1535\n",
      "citation_check 1535\n",
      "max_sim 1535\n",
      "peer_popularity 1535\n",
      "succ_pred 1535\n",
      "LSA_distance 1535\n",
      "title_sim 1535\n",
      "temporal_fit 1535\n",
      "N_LSA_distance 1535\n",
      "path_length 1535\n",
      "node_degree 1535\n",
      "reverse_max_sim 1535\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "## Compute features on COMPETITION_SET ##\n",
    "#########################################\n",
    "\n",
    "competition_features_dict = pw.compute_all_features(competition_set,\n",
    "                                                train_graph,\n",
    "                                                IDs,\n",
    "                                                node_info,\n",
    "                                                stemmer,\n",
    "                                                stpwds,\n",
    "                                                kdtree, \n",
    "                                                l, \n",
    "                                                l_ngrams,\n",
    "                                                t_titles,\n",
    "                                                node_dict, \n",
    "                                                index_dict,\n",
    "                                                False,\n",
    "                                                publication_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## Combine features ##\n",
    "######################\n",
    "\n",
    "competition_features = np.concatenate( [competition_features_dict['overlap_title'],\n",
    "                                        competition_features_dict['comm_auth'],\n",
    "                                        competition_features_dict['temp_diff'],\n",
    "                                        competition_features_dict['citation_check'],\n",
    "                                        competition_features_dict['max_sim'],\n",
    "                                        competition_features_dict['peer_popularity'],\n",
    "                                        competition_features_dict['succ_pred'],\n",
    "                                        competition_features_dict['LSA_distance'],\n",
    "                                        competition_features_dict['title_sim'],\n",
    "                                        competition_features_dict['temporal_fit'],\n",
    "                                        competition_features_dict['N_LSA_distance'],\n",
    "                                        competition_features_dict['path_length'],\n",
    "                                        competition_features_dict['node_degree'],\n",
    "                                        competition_features_dict['reverse_max_sim']]\n",
    "                                        ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.20.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/lucas/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "## Load classifier & predict ##\n",
    "###############################\n",
    "\n",
    "# Select a subset of features to use\n",
    "selection = [0, 1, 2,  3,  5,  6,  7, 8, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, \n",
    "             29, 30, 31, 32, 33, 34, 35, 36, 37, 38]\n",
    "\n",
    "with open(\"lightgbm_model\",\"rb\") as f:\n",
    "    lgb = pickle.load(f)\n",
    "\n",
    "# Predictions\n",
    "preds_lgb = lgb.predict(competition_features[:,selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## Print predictions to file ##\n",
    "###############################\n",
    "\n",
    "with open(\"predictions.csv\",\"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow([\"id\",\"category\"])\n",
    "    for i,pred in enumerate(preds_lgb):\n",
    "        writer.writerow([i,pred])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
