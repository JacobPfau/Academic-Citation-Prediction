{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "## Imports ##\n",
    "#############\n",
    "\n",
    "# external libraries\n",
    "import random\n",
    "import math\n",
    "import igraph\n",
    "import nltk\n",
    "import importlib\n",
    "import csv\n",
    "import pickle\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# our outsourced code\n",
    "import features_nodewise as nw\n",
    "import features_pairwise as pw\n",
    "import preprocessing as prep\n",
    "import multi_func as mf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## Read data ##\n",
    "###############\n",
    "\n",
    "# Training\n",
    "with open(\"./data/training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "    training_set = [element[0].split(\" \") for element in training_set]\n",
    "    \n",
    "# Testing -> we call it competition_set\n",
    "with open (\"./data/testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    competition_set = list(reader)\n",
    "    competition_set = [element[0].split(\" \") for element in competition_set]\n",
    "\n",
    "# Node info\n",
    "with open(\"./data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "###### EASY TESTING ####### DELETE THIS LATER ########\n",
    "# IDs_present = []\n",
    "# for edge in competition_set:\n",
    "#     IDs_present.append(edge[0])\n",
    "#     IDs_present.append(edge[1])\n",
    "# for edge in training_set:\n",
    "#     IDs_present.append(edge[0])\n",
    "#     IDs_present.append(edge[1])\n",
    "# IDs_present = list(set(IDs_present))\n",
    "# \n",
    "# new_node_info = []\n",
    "# for info in node_info:\n",
    "#     if info[0] in IDs_present:\n",
    "#         new_node_info.append(info)\n",
    "# node_info = new_node_info\n",
    "########## END OF EASY TESTING ######################\n",
    "\n",
    "\n",
    "abstracts = [element[5] for element in node_info]\n",
    "titles = [element[2] for element in node_info]\n",
    "IDs = [element[0] for element in node_info]\n",
    "publication_years = prep.to_feature_shape([int(info[1]) for info in node_info])\n",
    "train_true_labels = [int(element[2]) for element in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 27770, n_features: 2890\n",
      "n_samples: 27770, n_features: 10000\n",
      "n_samples: 27770, n_features: 150000\n",
      "Performing dimensionality reduction using LSA\n",
      "Explained variance of the SVD step: 16%\n",
      "Performing dimensionality reduction using LSA\n",
      "Explained variance of the SVD step: 7%\n",
      "[nltk_data] Downloading package punkt to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "## Preprocess data ##\n",
    "#####################\n",
    "\n",
    "# Construct term-document-tfidf matrices\n",
    "t_titles = prep.tfidf(titles)\n",
    "t = prep.tfidf(abstracts)\n",
    "t_ngrams = prep.tfidf(abstracts, r= (2,3), midf = 2, madf=0.5,feats=150000, sublinear = True)\n",
    "\n",
    "# Reduce matrix dimensionality\n",
    "l = nw.LSA(t,n_components=100)\n",
    "l_ngrams = nw.LSA(t_ngrams,n_components=300)\n",
    "\n",
    "# Build KDTrees to accelerate nearest-neighbour searches\n",
    "kdtree = nw.KDTree(l)\n",
    "kdtree_n = nw.KDTree(l_ngrams)\n",
    "\n",
    "# Build graph from gold standard training data\n",
    "train_IDs = np.array(IDs)\n",
    "train_edges = [(element[0],element[1]) for element in training_set if element[2]=='1']\n",
    "train_graph = prep.article_graph(train_IDs,train_edges)\n",
    "\n",
    "# dicts to accelerate ID <-> index searches\n",
    "node_dict = prep.to_dict(IDs,range(len(node_info)))\n",
    "index_dict = prep.to_dict(range(len(IDs)),IDs)\n",
    "\n",
    "# Load stemmer and stopwords for later title & abstract processing\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## Create dicts to store features ##\n",
    "####################################\n",
    "train_features_dict = dict()\n",
    "competition_features_dict = dict()\n",
    "\n",
    "all_feature_names = ['overlap_title',\n",
    "                      'comm_auth',\n",
    "                      'temp_diff',\n",
    "                      'citation_check',\n",
    "                      'max_sim',\n",
    "                      'peer_popularity',\n",
    "                      'succ_pred',\n",
    "                      'LSA_distance',\n",
    "                      'title_sim',\n",
    "                      'temporal_fit',\n",
    "                      'N_LSA_distance',\n",
    "                      'path_length',\n",
    "                      'node_degree',\n",
    "                      'reverse_max_sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 6\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 1\n",
      "0 / 2696\n",
      "1000 / 2696\n",
      "2000 / 2696\n",
      "overlap_title 2696\n",
      "comm_auth 2696\n",
      "temp_diff 2696\n",
      "citation_check 2696\n",
      "max_sim 2696\n",
      "peer_popularity 2696\n",
      "succ_pred 2696\n",
      "LSA_distance 2696\n",
      "title_sim 2696\n",
      "temporal_fit 2696\n",
      "N_LSA_distance 2696\n",
      "path_length 2696\n",
      "node_degree 2696\n",
      "reverse_max_sim 2696\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "## Compute features on TRAINING_SET ##\n",
    "######################################\n",
    "\n",
    "train_features_dict = pw.compute_all_features(training_set,\n",
    "                                                train_graph,\n",
    "                                                IDs,\n",
    "                                                node_info,\n",
    "                                                stemmer,\n",
    "                                                stpwds,\n",
    "                                                kdtree, \n",
    "                                                l, \n",
    "                                                l_ngrams,\n",
    "                                                t_titles,\n",
    "                                                node_dict, \n",
    "                                                index_dict,\n",
    "                                                True,\n",
    "                                                publication_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 5\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 1535\n",
      "1000 / 1535\n",
      "overlap_title 1535\n",
      "comm_auth 1535\n",
      "temp_diff 1535\n",
      "citation_check 1535\n",
      "max_sim 1535\n",
      "peer_popularity 1535\n",
      "succ_pred 1535\n",
      "LSA_distance 1535\n",
      "title_sim 1535\n",
      "temporal_fit 1535\n",
      "N_LSA_distance 1535\n",
      "path_length 1535\n",
      "node_degree 1535\n",
      "reverse_max_sim 1535\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "## Compute features on COMPETITION_SET ##\n",
    "#########################################\n",
    "\n",
    "competition_features_dict = pw.compute_all_features(competition_set,\n",
    "                                                train_graph,\n",
    "                                                IDs,\n",
    "                                                node_info,\n",
    "                                                stemmer,\n",
    "                                                stpwds,\n",
    "                                                kdtree, \n",
    "                                                l, \n",
    "                                                l_ngrams,\n",
    "                                                t_titles,\n",
    "                                                node_dict, \n",
    "                                                index_dict,\n",
    "                                                False,\n",
    "                                                publication_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "## Combine & normalize features ##\n",
    "##################################\n",
    "\n",
    "train_features = np.concatenate([train_features_dict['overlap_title'],\n",
    "                                 train_features_dict['comm_auth'],\n",
    "                                 train_features_dict['temp_diff'],\n",
    "                                 train_features_dict['citation_check'],\n",
    "                                 train_features_dict['max_sim'],\n",
    "                                 train_features_dict['peer_popularity'],\n",
    "                                 train_features_dict['succ_pred'],\n",
    "                                 train_features_dict['LSA_distance'],\n",
    "                                 train_features_dict['title_sim'],\n",
    "                                 train_features_dict['temporal_fit'],\n",
    "                                 train_features_dict['N_LSA_distance'],\n",
    "                                 train_features_dict['path_length'],\n",
    "                                 train_features_dict['node_degree'],\n",
    "                                 train_features_dict['reverse_max_sim']]                                            \n",
    "                                 ,axis = 1)\n",
    "\n",
    "competition_features = np.concatenate( [competition_features_dict['overlap_title'],\n",
    "                                        competition_features_dict['comm_auth'],\n",
    "                                        competition_features_dict['temp_diff'],\n",
    "                                        competition_features_dict['citation_check'],\n",
    "                                        competition_features_dict['max_sim'],\n",
    "                                        competition_features_dict['peer_popularity'],\n",
    "                                        competition_features_dict['succ_pred'],\n",
    "                                        competition_features_dict['LSA_distance'],\n",
    "                                        competition_features_dict['title_sim'],\n",
    "                                        competition_features_dict['temporal_fit'],\n",
    "                                        competition_features_dict['N_LSA_distance'],\n",
    "                                        competition_features_dict['path_length'],\n",
    "                                        competition_features_dict['node_degree'],\n",
    "                                        competition_features_dict['reverse_max_sim']]\n",
    "                                        ,axis = 1)\n",
    "\n",
    "# normalization\n",
    "from sklearn.preprocessing import StandardScaler as SS\n",
    "scaler = SS()\n",
    "normalized_train_features = scaler.fit_transform(train_features)\n",
    "normalized_competition_features = scaler.fit_transform(competition_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "## Train classifier & predict ##\n",
    "################################\n",
    "\n",
    "import lightgbm as light\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "# Select a subset of features to use\n",
    "selection = [2,  3,  5,  6,  7, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, \n",
    "             29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
    "\n",
    "lgb = light.LGBMClassifier()\n",
    "# Training\n",
    "lgb.fit(train_features[:,selection], train_true_labels)\n",
    "# Predictions\n",
    "preds_lgb = lgb.predict(competition_features[:,selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## Extend the training graph ##\n",
    "###############################\n",
    "\n",
    "# We now incorporate the predictions just made in the training set, recompute features and\n",
    "\n",
    "predicted_edges = []\n",
    "for i in range(len(competition_set)):\n",
    "    if int(preds_lgb[i]) == 1:\n",
    "        predicted_edges.append((competition_set[i][0],competition_set[i][1]))\n",
    "    \n",
    "    \n",
    "train_graph.add_edges(predicted_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 6\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 1\n",
      "0 / 2696\n",
      "1000 / 2696\n",
      "2000 / 2696\n",
      "overlap_title 2696\n",
      "comm_auth 2696\n",
      "temp_diff 2696\n",
      "citation_check 2696\n",
      "max_sim 2696\n",
      "peer_popularity 2696\n",
      "succ_pred 2696\n",
      "LSA_distance 2696\n",
      "title_sim 2696\n",
      "temporal_fit 2696\n",
      "N_LSA_distance 2696\n",
      "path_length 2696\n",
      "node_degree 2696\n",
      "reverse_max_sim 2696\n",
      "0 / 5\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 1535\n",
      "1000 / 1535\n",
      "overlap_title 1535\n",
      "comm_auth 1535\n",
      "temp_diff 1535\n",
      "citation_check 1535\n",
      "max_sim 1535\n",
      "peer_popularity 1535\n",
      "succ_pred 1535\n",
      "LSA_distance 1535\n",
      "title_sim 1535\n",
      "temporal_fit 1535\n",
      "N_LSA_distance 1535\n",
      "path_length 1535\n",
      "node_degree 1535\n",
      "reverse_max_sim 1535\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "## REcompute all features ##\n",
    "############################\n",
    "\n",
    "# Based on the extended train_graph we recompute all features\n",
    "\n",
    "########################################\n",
    "## REcompute features on TRAINING_SET ##\n",
    "########################################\n",
    "\n",
    "train_features_dict = pw.compute_all_features(training_set,\n",
    "                                                train_graph,\n",
    "                                                IDs,\n",
    "                                                node_info,\n",
    "                                                stemmer,\n",
    "                                                stpwds,\n",
    "                                                kdtree, \n",
    "                                                l, \n",
    "                                                l_ngrams,\n",
    "                                                t_titles,\n",
    "                                                node_dict, \n",
    "                                                index_dict,\n",
    "                                                True,\n",
    "                                                publication_years)\n",
    "\n",
    "###########################################\n",
    "## REcompute features on COMPETITION_SET ##\n",
    "###########################################\n",
    "\n",
    "# add the prediction (0/1) to the competition set. That information is used to remove that edge from the graph when recomputing the features\n",
    "competition_set_extended = competition_set\n",
    "for i in range(len(competition_set)):\n",
    "    competition_set_extended[i].append(preds_lgb[i])\n",
    "    \n",
    "competition_features_dict = pw.compute_all_features(competition_set_extended,\n",
    "                                                train_graph,\n",
    "                                                IDs,\n",
    "                                                node_info,\n",
    "                                                stemmer,\n",
    "                                                stpwds,\n",
    "                                                kdtree, \n",
    "                                                l, \n",
    "                                                l_ngrams,\n",
    "                                                t_titles,\n",
    "                                                node_dict, \n",
    "                                                index_dict,\n",
    "                                                True,\n",
    "                                                publication_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "## Combine & normalize features ##\n",
    "##################################\n",
    "\n",
    "train_features = np.concatenate([train_features_dict['overlap_title'],\n",
    "                                 train_features_dict['comm_auth'],\n",
    "                                 train_features_dict['temp_diff'],\n",
    "                                 train_features_dict['citation_check'],\n",
    "                                 train_features_dict['max_sim'],\n",
    "                                 train_features_dict['peer_popularity'],\n",
    "                                 train_features_dict['succ_pred'],\n",
    "                                 train_features_dict['LSA_distance'],\n",
    "                                 train_features_dict['title_sim'],\n",
    "                                 train_features_dict['temporal_fit'],\n",
    "                                 train_features_dict['N_LSA_distance'],\n",
    "                                 train_features_dict['path_length'],\n",
    "                                 train_features_dict['node_degree'],\n",
    "                                 train_features_dict['reverse_max_sim']]                                            \n",
    "                                 ,axis = 1)\n",
    "\n",
    "competition_features = np.concatenate( [competition_features_dict['overlap_title'],\n",
    "                                        competition_features_dict['comm_auth'],\n",
    "                                        competition_features_dict['temp_diff'],\n",
    "                                        competition_features_dict['citation_check'],\n",
    "                                        competition_features_dict['max_sim'],\n",
    "                                        competition_features_dict['peer_popularity'],\n",
    "                                        competition_features_dict['succ_pred'],\n",
    "                                        competition_features_dict['LSA_distance'],\n",
    "                                        competition_features_dict['title_sim'],\n",
    "                                        competition_features_dict['temporal_fit'],\n",
    "                                        competition_features_dict['N_LSA_distance'],\n",
    "                                        competition_features_dict['path_length'],\n",
    "                                        competition_features_dict['node_degree'],\n",
    "                                        competition_features_dict['reverse_max_sim']]\n",
    "                                        ,axis = 1)\n",
    "\n",
    "# normalization\n",
    "from sklearn.preprocessing import StandardScaler as SS\n",
    "scaler = SS()\n",
    "normalized_train_features = scaler.fit_transform(train_features)\n",
    "normalized_competition_features = scaler.fit_transform(competition_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "## Train classifier & predict ##\n",
    "################################\n",
    "\n",
    "import lightgbm as light\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "# Select a subset of features to use\n",
    "selection = [2,  3,  5,  6,  7, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, \n",
    "             29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
    "\n",
    "lgb = light.LGBMClassifier()\n",
    "# Training\n",
    "lgb.fit(train_features[:,selection], train_true_labels)\n",
    "# Predictions\n",
    "preds_lgb_2 = lgb.predict(competition_features[:,selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
