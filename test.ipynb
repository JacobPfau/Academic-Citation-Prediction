{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'features_pairwise' from '/home/jacob/JUPYTER PROJECTS/ML/project/features_pairwise.py'>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jacob/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jacob/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "n_samples: 27770, n_features: 10000\n",
      "Performing dimensionality reduction using LSA\n",
      "Explained variance of the SVD step: 19%\n"
     ]
    }
   ],
   "source": [
    "import features_nodewise as nw\n",
    "import features_pairwise as pw\n",
    "import preprocessing as prep\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "with open(\"./data/train_train_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "with open(\"./data/train_test_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "to_keep_train = random.sample( range(len(training_set)),k=int(round(len(training_set)*0.05)) )\n",
    "training_set_reduced = [training_set[i] for i in to_keep_train]\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "to_keep_test = random.sample( range(len(testing_set)),k=int(round(len(testing_set)*0.1)) )\n",
    "testing_set_reduced = [testing_set[i] for i in to_keep_test]\n",
    "\n",
    "with open(\"./data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]\n",
    "node_dict = prep.to_dict( [element[0] for element in node_info],range(len(node_info)) )\n",
    "index_dict = prep.to_dict(range(len(IDs)),IDs)\n",
    "\n",
    "# compute TFIDF vector of each paper\n",
    "corpus = [element[5] for element in node_info]\n",
    "\n",
    "t = prep.tfidf(corpus)\n",
    "l = nw.LSA(t)\n",
    "\n",
    "nodes = set([training_set_reduced[i][0] for i in range(len(training_set_reduced))])\n",
    "nodes = nodes | set([training_set_reduced[i][1] for i in range(len(training_set_reduced))])\n",
    "training_l = [l[node_dict[i]] for i in nodes]\n",
    "kdtree = nw.KDTree(training_l)\n",
    "\n",
    "all_edges = [(element[0],element[1]) for element in training_set_reduced if element[2]==\"1\"]\n",
    "edges = random.sample(all_edges,int(len(all_edges)/2))\n",
    "graph = prep.article_graph(IDs,edges)\n",
    "\n",
    "no_citation = [(element[0],element[1]) for element in training_set if element[2]==\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "true_labels = []\n",
    "\n",
    "for i,triple in enumerate(training_set_reduced):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    f = []\n",
    "    \n",
    "    f.extend(pw.Citation_Check(source,target,kdtree,l,graph,node_dict,index_dict,k=20))\n",
    "    f.extend(list(pw.Max_Sim(source,target,l,graph,node_dict)))\n",
    "    f.append(pw.peer_popularity(graph,source,target))\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "    # convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    # remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\") \n",
    "\n",
    "    overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "    temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    \n",
    "    f.append(overlap_title)\n",
    "    f.append(temp_diff)\n",
    "    f.append(comm_auth)\n",
    "    features.append(f)\n",
    "    true_labels.append(triple[2])\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "        \n",
    "features=np.array(features)\n",
    "true_labels=np.array(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "nodes = set([training_set_reduced[i][0] for i in range(len(training_set_reduced))])\n",
    "nodes = nodes | set([training_set_reduced[i][1] for i in range(len(training_set_reduced))])\n",
    "test_l = [l[node_dict[i]] for i in nodes]\n",
    "test_kdtree = nw.KDTree(test_l)\n",
    "\n",
    "test_all_edges = [(element[0],element[1]) for element in testing_set_reduced if element[2]==\"1\"]\n",
    "test_edges = random.sample(test_all_edges,int(len(test_all_edges)/2))\n",
    "test_graph = prep.article_graph(IDs,test_edges)\n",
    "\n",
    "test_features = []\n",
    "test_true_labels = []\n",
    "for i,triple in enumerate(testing_set_reduced):\n",
    "    \n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    f = []\n",
    "    \n",
    "    f.extend(pw.Citation_Check(source,target,test_kdtree,l,test_graph,node_dict,index_dict,k=20))\n",
    "    f.extend(list(pw.Max_Sim(source,target,l,test_graph,node_dict)))\n",
    "    f.append(pw.peer_popularity(graph,source,target))\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "    # convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    # remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\") \n",
    "\n",
    "    overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "    temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    \n",
    "    f.append(overlap_title)\n",
    "    f.append(temp_diff)\n",
    "    f.append(comm_auth)\n",
    "    test_features.append(f)\n",
    "    test_true_labels.append(triple[2])\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "        \n",
    "test_features=np.array(test_features)\n",
    "test_true_labels=np.array(test_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.131481211990857   0.5809902339004109\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# mean of true but removed edges (with 50% edges removed) is 0.031\n",
    "# mean of no edge pairs (with 50% edges removed) is 0.0003\n",
    "\n",
    "# [pw.Citation_Check(e[0],e[1],kdtree,l,graph,node_dict,index_dict,k=20) for e in random.sample(edges[150000:],20)]\n",
    "# np.mean([pw.Citation_Check(e[0],e[1],kdtree,l,graph,node_dict,index_dict,k=20) for e in random.sample(no_citation,1500)])\n",
    "\n",
    "a = np.mean([np.mean(pw.Max_Sim(e[0],e[1],l,graph,node_dict)) for e in random.sample(edges[150000:],1500)])\n",
    "b = np.mean([np.mean(pw.Max_Sim(e[0],e[1],l,graph,node_dict)) for e in random.sample(no_citation,1500)])\n",
    "\n",
    "print(a,\" \",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries:\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "# from sklearn import cross_validation, \n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "# from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def modelfit(alg, features, true_labels,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "#     if useTrainCV:\n",
    "#         xgb_param = alg.get_xgb_params()\n",
    "#         xgtrain = xgb.DMatrix(features, label=true_labels)\n",
    "#         cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "#             metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "#         alg.set_params(n_estimators=cvresult)\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(features, true_labels,eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(features)\n",
    "    dtrain_predprob = alg.predict_proba(features)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(true_labels, dtrain_predictions))\n",
    "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(true_labels, dtrain_predprob))\n",
    "                    \n",
    "#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.7703\n",
      "AUC Score (Train): 0.863704\n",
      "acc: 0.777376457067805 f1: 0.8230227094753327\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " max_depth=8,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    ")\n",
    "\n",
    "modelfit(xgb1, features, true_labels,useTrainCV=True)\n",
    "\n",
    "preds = xgb1.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6611393859793138 f1: 0.6766917293233081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniconda3/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.LinearSVC()\n",
    "\n",
    "# train\n",
    "classifier.fit(features, true_labels)\n",
    "preds= list(classifier.predict(test_features))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6583483828599573 f1: 0.6749960955801967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "model = lr().fit(features,true_labels)\n",
    "preds = list(model.predict(test_features))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "class Class_Net():\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, batch_size=60):\n",
    "        ''' initialize the classifier with default (best) parameters '''\n",
    "        tf.reset_default_graph()\n",
    "        self.alpha = learning_rate\n",
    "        self.beta = batch_size\n",
    "        self.warm = False\n",
    "\n",
    "    def fit(self,X,Y,warm_start=True,n_epochs=50):\n",
    "        ''' train the network, and if warm_start, then do not reinit. the network\n",
    "            (if it has already been initialized)\n",
    "        '''\n",
    "        self.epochs=n_epochs\n",
    "\n",
    "        self.n_batch = int(len(X)/self.beta)\n",
    "        \n",
    "        if warm_start==False or self.warm==False:\n",
    "            self.x = tf.placeholder(tf.float32,shape=[None,len(X[0])])\n",
    "            self.y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "            \n",
    "            self.wZero = tf.get_variable('wZero',shape=[len(X[0]),50],initializer=tf.glorot_uniform_initializer())\n",
    "            self.bZero = tf.Variable(tf.zeros([50]))\n",
    "\n",
    "            self.wOne = tf.get_variable('wOne',shape=[50,1],initializer=tf.glorot_uniform_initializer())\n",
    "            self.bOne = tf.Variable(tf.zeros([1]))\n",
    "            self.keep_prob = 0.9\n",
    "            self.drop_out = tf.nn.dropout(self.x, self.keep_prob)\n",
    "            self.model = tf.nn.sigmoid(tf.matmul(tf.nn.relu(tf.matmul(self.drop_out, self.wZero) \n",
    "                                              + self.bZero),self.wOne)+self.bOne)\n",
    "            self.cost = tf.losses.log_loss(self.y,self.model)\n",
    "            \n",
    "#             self.optimizer = tf.train.GradientDescentOptimizer(self.alpha).minimize(self.cost)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.alpha).minimize(self.cost)\n",
    "\n",
    "            #without dropout\n",
    "#             self.model = tf.matmul(tf.nn.relu(tf.matmul(self.x, self.wZero) + self.bZero),self.wOne)+self.bOne\n",
    "\n",
    "    \n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        with tf.Session() as sess:\n",
    "            if warm_start==False or self.warm==False:\n",
    "                sess.run(self.init)\n",
    "            else:\n",
    "                self.saver.restore(sess, './tempVariables.ckpt')\n",
    "            for epoch in range(self.epochs):\n",
    "                self.avg_cost = 0\n",
    "                for i in range(self.n_batch):\n",
    "                    _, self.c = sess.run([self.optimizer,self.cost], feed_dict={self.x: X[i*self.beta:min([(i+1)*\n",
    "                                    self.beta,len(X)]),:],self.y:Y[i*self.beta:min([(i+1)*self.beta,len(X)])]})\n",
    "                    \n",
    "                    self.avg_cost = self.avg_cost+np.mean(self.c)/self.n_batch\n",
    "                print(\"Epoch:\", '%s' % (epoch+1), \"cost=\", \"%s\"% (self.avg_cost))\n",
    "            self.saver.save(sess,'./tempVariables.ckpt')\n",
    "            \n",
    "        self.warm = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        ''' return a matrix P where P[i,j] = P(Y[i,j]=1), \n",
    "        for all instances i, and labels j. '''\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, './tempVariables.ckpt')\n",
    "            self.preds = sess.run(tf.nn.softmax(self.model), feed_dict={self.x: X}) \n",
    "        return self.preds\n",
    "    \n",
    "    def predict(self,X):\n",
    "        ''' return a matrix of predictions for X '''\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost= 0.4106214834885162\n",
      "Epoch: 2 cost= 0.398600326072086\n",
      "Epoch: 3 cost= 0.3949048651890319\n",
      "Epoch: 4 cost= 0.3904044625434008\n",
      "Epoch: 5 cost= 0.3860095381736755\n",
      "Epoch: 6 cost= 0.38221497562798584\n",
      "Epoch: 7 cost= 0.38106458078731176\n",
      "Epoch: 8 cost= 0.3798977735367689\n",
      "Epoch: 9 cost= 0.3760108270428399\n",
      "Epoch: 10 cost= 0.3769362587820399\n",
      "Epoch: 11 cost= 0.37574045062065115\n",
      "Epoch: 12 cost= 0.3760301809419285\n",
      "Epoch: 13 cost= 0.37467829178680073\n",
      "Epoch: 14 cost= 0.3753610063682903\n",
      "Epoch: 15 cost= 0.37626715735955674\n",
      "Epoch: 16 cost= 0.37440064982934435\n",
      "Epoch: 17 cost= 0.37641546021808275\n",
      "Epoch: 18 cost= 0.37537311342629515\n",
      "Epoch: 19 cost= 0.3745592537251386\n",
      "Epoch: 20 cost= 0.37557570717551486\n",
      "Epoch: 21 cost= 0.3747963829474014\n",
      "Epoch: 22 cost= 0.37446318566799164\n",
      "Epoch: 23 cost= 0.3763311023061928\n",
      "Epoch: 24 cost= 0.3745643599466843\n",
      "Epoch: 25 cost= 0.37467672255906204\n",
      "Epoch: 26 cost= 0.3754866369745949\n",
      "Epoch: 27 cost= 0.3769694824110378\n",
      "Epoch: 28 cost= 0.374538650025021\n",
      "Epoch: 29 cost= 0.3743924487720838\n",
      "Epoch: 30 cost= 0.37318795296278867\n",
      "Epoch: 31 cost= 0.37641997472806393\n",
      "Epoch: 32 cost= 0.37432786415923724\n",
      "Epoch: 33 cost= 0.37411275641484704\n",
      "Epoch: 34 cost= 0.3748769876631824\n",
      "Epoch: 35 cost= 0.3746340719136325\n",
      "INFO:tensorflow:Restoring parameters from ./tempVariables.ckpt\n",
      "acc: 0.5613199802988015 f1: 0.7190325972660357\n"
     ]
    }
   ],
   "source": [
    "net = Class_Net(learning_rate=0.05,batch_size=250)\n",
    "net.fit(features,list(map(lambda x: [x],true_labels)),n_epochs=35)\n",
    "preds=net.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0', '1', '0', '0', '1', '0', '0', '1', '1', '1', '1', '0',\n",
       "       '0', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '0', '0',\n",
       "       '1', '1', '0', '1', '0', '0', '1', '1', '1', '1', '1', '0', '1',\n",
       "       '0', '0', '0', '0', '1', '1', '0', '0', '1', '1', '1'], dtype='<U1')"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels[50:100,]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
