{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'features_pairwise' from '/home/jacob/JUPYTER PROJECTS/ML/project/features_pairwise.py'>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jacob/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jacob/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "n_samples: 27770, n_features: 10000\n",
      "Performing dimensionality reduction using LSA\n",
      "Explained variance of the SVD step: 32%\n"
     ]
    }
   ],
   "source": [
    "import features_nodewise as nw\n",
    "import features_pairwise as pw\n",
    "import preprocessing as prep\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "with open(\"./data/train_train_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "with open(\"./data/train_test_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "to_keep_train = random.sample( range(len(training_set)),k=int(round(len(training_set)*0.05)) )\n",
    "training_set_reduced = [training_set[i] for i in to_keep_train]\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "to_keep_test = random.sample( range(len(testing_set)),k=int(round(len(testing_set)*.37)) )\n",
    "testing_set_reduced = [testing_set[i] for i in to_keep_test]\n",
    "\n",
    "with open(\"./data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]\n",
    "node_dict = prep.to_dict( [element[0] for element in node_info],range(len(node_info)) )\n",
    "index_dict = prep.to_dict(range(len(IDs)),IDs)\n",
    "\n",
    "# compute TFIDF vector of each paper\n",
    "corpus = [element[5] for element in node_info]\n",
    "\n",
    "t = prep.tfidf(corpus)\n",
    "l = nw.LSA(t,n_components=250)\n",
    "\n",
    "true_edges = [(element[0],element[1]) for element in training_set if element[2]==\"1\"]\n",
    "no_citation = [(element[0],element[1]) for element in training_set if element[2]==\"0\"]\n",
    "edges= [(element[0],element[1]) for element in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdtree = nw.KDTree(l)\n",
    "graph = prep.article_graph(IDs,edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "\n",
    "    \n",
    "def weighted_overlap(f,true_pairs,false_pairs,outputs=None):\n",
    "    if outputs==None:\n",
    "        true_f_values = [f(p[0],p[1]) for p in true_pairs]\n",
    "        true_mean = np.mean(true_f_values)\n",
    "        false_f_values = [f(p[0],p[1]) for p in false_pairs]\n",
    "        false_mean = np.mean(false_f_values)\n",
    "        t_true = len(true_f_values)\n",
    "        t_false = len(false_f_values)\n",
    "\n",
    "        if true_mean>=false_mean:\n",
    "            m = max(false_f_values)\n",
    "            #could be improved by iteratively adding instead of redoing len(list) for every val\n",
    "            f_overlaps = [len([f for f in false_f_values if f>=val]) for val in true_f_values if val<=m]\n",
    "            f_overlaps = sum(f_overlaps) / ( t_true*t_false )\n",
    "        else:\n",
    "            m = min(false_f_values)\n",
    "            #could be improved by iteratively adding instead of redoing len(list) for every val\n",
    "            f_overlaps = [len([f for f in false_f_values if f<=val]) for val in true_f_values if val>=m]\n",
    "            f_overlaps = sum(f_overlaps) / ( t_true*t_false )\n",
    "\n",
    "        return f_overlaps\n",
    "    \n",
    "    else:\n",
    "        all_true_f_values = np.array([f(p[0],p[1]) for p in true_pairs])\n",
    "        all_false_f_values = np.array([f(p[0],p[1]) for p in false_pairs])\n",
    "        all_f_overlaps=[]\n",
    "        for o in outputs:\n",
    "            true_f_values = all_true_f_values[:,o]\n",
    "            false_f_values = all_false_f_values[:,o]\n",
    "            true_mean = np.mean(true_f_values)\n",
    "            false_mean = np.mean(false_f_values)\n",
    "            t_true = len(true_f_values)\n",
    "            t_false = len(false_f_values)\n",
    "\n",
    "            if true_mean>=false_mean:\n",
    "                m = max(false_f_values)\n",
    "                #could be improved by iteratively adding instead of redoing len(list) for every val\n",
    "                f_overlaps = [len([f for f in false_f_values if f>=val]) for val in true_f_values if val<=m]\n",
    "                f_overlaps = sum(f_overlaps) / ( t_true*t_false )\n",
    "            else:\n",
    "                m = min(false_f_values)\n",
    "                #could be improved by iteratively adding instead of redoing len(list) for every val\n",
    "                f_overlaps = [len([f for f in false_f_values if f<=val]) for val in true_f_values if val>=m]\n",
    "                f_overlaps = sum(f_overlaps) / ( t_true*t_false )\n",
    "\n",
    "            all_f_overlaps.append(f_overlaps)\n",
    "        return all_f_overlaps\n",
    "\n",
    "def error_overlap(f,g,sample_set):\n",
    "    test_set = np.array(sample_set[:int(len(sample_set)/2)])\n",
    "    train_set = np.array(sample_set[int(len(sample_set)/2):])\n",
    "    modelf = lr(penalty='l1',solver='liblinear').fit(np.array([f(p[0],p[1]) for p in train_set]).reshape(-1, 1)\n",
    "                                                     ,train_set[:,2])\n",
    "    modelg = lr(penalty='l1',solver='liblinear').fit(np.array([g(p[0],p[1]) for p in train_set]).reshape(-1, 1)\n",
    "                                                     ,train_set[:,2])\n",
    "    predsf = modelf.predict(np.array([f(p[0],p[1]) for p in test_set]).reshape(-1, 1))\n",
    "    predsg = modelg.predict(np.array([g(p[0],p[1]) for p in test_set]).reshape(-1, 1))\n",
    "    \n",
    "    first = False\n",
    "    counts=[0,0,0]\n",
    "    for i in range(len(predsf)):\n",
    "        if predsf[i]!=test_set[:,2][i]: \n",
    "            first = True\n",
    "            counts[0]+=1\n",
    "        if predsg[i]!=test_set[:,2][i]: \n",
    "            counts[1]+=1\n",
    "            if first==True: counts[2]+=1\n",
    "        first = False\n",
    "        \n",
    "    return counts[2]/min([counts[0],counts[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_distances as COS\n",
    "import math\n",
    "\n",
    "def Citation_Check(source_ID,target_ID,kdtree,features,graph,node_dict,index_dict,k=500):\n",
    "    \"\"\"\n",
    "    @features param4: the array of features i.e. points which were used to build the kdtree\n",
    "    @return: numpy array [% KNN of source which cite target, % KNN of target which are cited by source]\n",
    "    \"\"\"\n",
    "    close_source = pw.Get_K_NN(source_ID,kdtree,features,node_dict,index_dict,k_val=k)[0]\n",
    "    close_target = pw.Get_K_NN(target_ID,kdtree,features,node_dict,index_dict,k_val=k)[0]\n",
    "    cite_percent = len([n for n in close_source if graph.are_connected(n,target_ID)])/len(close_source)\n",
    "    w_s_p = sum([math.log2(500-i+1) for i,x in enumerate(close_source) if graph.are_connected(x,target_ID)])/5500\n",
    "    w_t_p = sum([math.log2(500-i+1) for i,x in enumerate(close_target) if graph.are_connected(source_ID,x)])/5500\n",
    "    cited_percent = len([n for n in close_target if graph.are_connected(source_ID,n)])/len(close_target)\n",
    "\n",
    "    return np.array([cite_percent,cited_percent,w_s_p,w_t_p])\n",
    "    \n",
    "# seems like Max_Sim does better on graph with noise edges left in???\n",
    "# 0.05    lambda s,t: pw.Max_Sim(s,t,l,graph,node_dict)[3] (similar for 3,4,7)\n",
    "# 0.05    lambda s,t: pw.Citation_Check(s,t,kdtree,l,graph,node_dict,index_dict,k=500)[3] (2,3 better than 0,1) \n",
    "# 0.15    lambda s,t: pw.peer_popularity(graph,s,t)\n",
    "# 1.00    lambda s,t: pw.edge_check(s,t,graph)\n",
    "# 0.11    lambda s,t: pw.LSA_distance(s,t,node_dict,l)\n",
    "# 0.15    lambda s,t: pw.node_degree(s,t,graph)[2]\n",
    "# 0.05    lambda s,t: pw.succ_pred(s,t,graph)[3] (all similar)\n",
    "# 0.37    lambda s,t: pw.baseline(s,t,node_dict,node_info)[3] i.e. time diff\n",
    "# 0.17    graph distance (probably better after removing 50% of noise edges)\n",
    "\n",
    "score_MS = []\n",
    "for n in range(1):\n",
    "     score_MS.append(weighted_overlap(lambda s,t: pw.Max_Sim(s,t,l,graph,node_dict),random.sample(true_edges,1000),random.sample(no_citation,1000),range(9)))\n",
    "\n",
    "score_SP = []\n",
    "for n in range(1):\n",
    "    score_SP.append(weighted_overlap(lambda s,t: pw.succ_pred(s,t,graph),random.sample(true_edges,5000),random.sample(no_citation,5000),range(4)))\n",
    "\n",
    "score_CC = []\n",
    "for n in range(1):\n",
    "     score_CC.append(weighted_overlap(lambda s,t: Citation_Check(s,t,kdtree,l,graph,node_dict,index_dict,k=500),random.sample(true_edges,2000),random.sample(no_citation,2000),range(4)))\n",
    "score_CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#USE: CC:3, SP:3, MS:3\n",
    "\n",
    "\n",
    "#errors_MS3_SP: [0.38, 0.38, 0.43, 0.31]\n",
    "\n",
    "errors_MS3_SP=[]\n",
    "for n in range(4):\n",
    "    errors_MS3_SP.append(error_overlap(lambda s,t: pw.Max_Sim(s,t,l,graph,node_dict)[3],lambda s,t: pw.succ_pred(\n",
    "        s,t,graph)[n],random.sample(training_set,2000)))\n",
    "print(1)\n",
    "#errors_MS7_SP: [0.42,0.34,0.37,0.30]\n",
    "\n",
    "errors_MS7_SP=[]\n",
    "for n in range(4):\n",
    "    errors_MS7_SP.append(error_overlap(lambda s,t: pw.Max_Sim(s,t,l,graph,node_dict)[7],lambda s,t: pw.succ_pred(\n",
    "        s,t,graph)[n],random.sample(training_set,2000)))\n",
    "print(2)\n",
    "\n",
    "# errors_CC_MS: [.61,.46,.28,.25]\n",
    "errors_CC_MS=[]\n",
    "for n in [2,3]:\n",
    "    for m in [3,7]:\n",
    "        errors_CC_MS.append(error_overlap(lambda s,t: pw.Max_Sim(s,t,l,graph,node_dict)[m],\n",
    "            lambda s,t: pw.Citation_Check(s,t,kdtree,l,graph,node_dict,index_dict,k=500)[n],\n",
    "                                          random.sample(training_set,2000)))\n",
    "print(3)\n",
    "\n",
    "#errors_CC_SP: [0.5, 0.43,.51,.43,.65,.5,.31]\n",
    "errors_CC_SP=[]\n",
    "for n in [2,3]:\n",
    "    for m in range(4):\n",
    "        errors_CC_SP.append(error_overlap(lambda s,t: pw.succ_pred(s,t,graph)[m],\n",
    "            lambda s,t: pw.Citation_Check(s,t,kdtree,l,graph,node_dict,index_dict,k=500)[n],\n",
    "                                          random.sample(training_set,2000)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.87064956 0.54104473 0.82889516 ... 0.82901791 0.43395938 0.5270261 ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d919e934a57d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMax_Sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msucc_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmutual_info_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-d919e934a57d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMax_Sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msucc_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmutual_info_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/mutual_info_.py\u001b[0m in \u001b[0;36mmutual_info_regression\u001b[0;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \"\"\"\n\u001b[1;32m    369\u001b[0m     return _estimate_mi(X, y, discrete_features, False, n_neighbors,\n\u001b[0;32m--> 370\u001b[0;31m                         copy, random_state)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/mutual_info_.py\u001b[0m in \u001b[0;36m_estimate_mi\u001b[0;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    246\u001b[0m            \u001b[0mData\u001b[0m \u001b[0mSets\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPLoS\u001b[0m \u001b[0mONE\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2014.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mdiscrete_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    550\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.87064956 0.54104473 0.82889516 ... 0.82901791 0.43395938 0.5270261 ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range(4):\n",
    "    X.append([])\n",
    "    Y.append([])\n",
    "    random.seed(0)\n",
    "    for p in random.sample(training_set,2000):\n",
    "        if i==0:\n",
    "            X[i].append(pw.Max_Sim(p[0],p[1],l,graph,node_dict)[3])\n",
    "        Y[i].append(pw.succ_pred(p[0],p[1],graph)[i])\n",
    "mi = mutual_info_regression(np.array(Y), X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "true_labels = []\n",
    "\n",
    "to_del = []\n",
    "for i,triple in enumerate(training_set):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "    temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    if temp_diff<-1:\n",
    "        to_del.append((source,target))\n",
    "\n",
    "graph.delete_edges(to_del)\n",
    "\n",
    "random.seed(0)\n",
    "t_r = random.sample(training_set,5000)\n",
    "# nodes = set([t_r[i][0] for i in range(len(t_r))])\n",
    "# nodes = nodes | set([t_r[i][1] for i in range(len(t_r))])\n",
    "# l1 = [l[node_dict[i]] for i in nodes]\n",
    "# kdtree = nw.KDTree(l1)\n",
    "\n",
    "\n",
    "# for i,triple in enumerate(training_set):\n",
    "\n",
    "for i,triple in enumerate(t_r):\n",
    "    f=[]\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "#     index_source = node_dict[source]\n",
    "#     index_target = node_dict[target]\n",
    "#     source_info = node_info[index_source]\n",
    "#     target_info = node_info[index_target]\n",
    "\n",
    "#     # convert to lowercase and tokenize\n",
    "#     source_title = source_info[2].lower().split(\" \")\n",
    "#     # remove stopwords\n",
    "#     source_title = [token for token in source_title if token not in stpwds]\n",
    "#     source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "#     target_title = target_info[2].lower().split(\" \")\n",
    "#     target_title = [token for token in target_title if token not in stpwds]\n",
    "#     target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "#     source_auth = source_info[3].split(\",\")\n",
    "#     target_auth = target_info[3].split(\",\") \n",
    "\n",
    "#     overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "#     temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "#     comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    \n",
    "    f.append(pw.Max_Sim(source,target,l,graph,node_dict)[3])\n",
    "    f.append(pw.Citation_Check(source,target,kdtree,l,graph,node_dict,index_dict,k=500)[3])\n",
    "#     f.append(pw.peer_popularity(graph,source,target))\n",
    "#     f.append(pw.edge_check(source,target,graph))\n",
    "    f.append(pw.LSA_distance(source,target,node_dict,l,metric='COS'))\n",
    "#     f.append(pw.node_degree(source,target,graph)[2])\n",
    "#     f.append(len(set(source_title).intersection(set(target_title))))\n",
    "#     f.append(int(source_info[1]) - int(target_info[1]))\n",
    "#     f.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    f.append(pw.succ_pred(source,target,graph)[3])\n",
    "\n",
    "    features.append(f)\n",
    "    true_labels.append(triple[2])\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "\n",
    "features=np.array(features)\n",
    "true_labels=np.array(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model from t_r\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "model = lr(penalty='l1',solver='liblinear').fit(features,true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "#for applying model\n",
    "features1 = []\n",
    "true_labels1=[]\n",
    "\n",
    "random.seed(1)\n",
    "t_r1 = random.sample(training_set,5000)\n",
    "\n",
    "for i,triple in enumerate(t_r1):\n",
    "    f=[]\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "\n",
    "    f.append(pw.Max_Sim(source,target,l,graph,node_dict)[3])\n",
    "\n",
    "    f.append(pw.LSA_distance(source,target,node_dict,l,metric='COS'))\n",
    "\n",
    "    f.append(pw.succ_pred(source,target,graph)[3])\n",
    "\n",
    "    features1.append(f)\n",
    "    true_labels1.append(triple[2])\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "\n",
    "features1=np.array(features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then apply to f_r1/features1\n",
    "pred_probs = list(model.predict_proba(features1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete based on preds from t_r1\n",
    "to_del=[]\n",
    "\n",
    "for i,triple in enumerate(t_r1):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "    temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    \n",
    "    if pred_probs[i][1]<=0.15 and temp_diff>=-1:\n",
    "        to_del.append((source,target))\n",
    "\n",
    "graph.delete_edges(to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "#recompute features on t_r1\n",
    "features2 = []\n",
    "true_labels2 = []\n",
    "\n",
    "for i,triple in enumerate(t_r1):\n",
    "    f=[]\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "\n",
    "    f.append(pw.Max_Sim(source,target,l,graph,node_dict)[3])\n",
    "\n",
    "    f.append(pw.LSA_distance(source,target,node_dict,l,metric='COS'))\n",
    "\n",
    "    f.append(pw.succ_pred(source,target,graph)[3])\n",
    "\n",
    "    features2.append(f)\n",
    "    true_labels2.append(triple[2])\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "\n",
    "features2=np.array(features2)\n",
    "true_labels2=np.array(true_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model1 based on removed-edges t_r1+features2\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "model1 = lr(penalty='l1',solver='liblinear').fit(features2,true_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('./features_train/reduced', 'wb') as f:\n",
    "#     pickle.dump([training_set_reduced,graph,l,kdtree,node_dict,index_dict,true_labels],f)\n",
    "# with open('./features_train/reduced_features', 'wb') as f:\n",
    "#     pickle.dump(feature,f)\n",
    "with open('./features_train/reduced', 'rb') as f:\n",
    "    training_set_reduced,graph,l,kdtree,node_dict,index_dict,true_labels=pickle.load(f)\n",
    "with open('./features_train/reduced_features', 'rb') as f:\n",
    "    features=pickle.load(f)\n",
    "#order: max_sim,citation_check,peer_popularity,edge_check,lsa_distance,node_degree,title,time,author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "#currently: 0:CC%,1:CC%,2:Max_Sim..6:Max_Sim,7:avg(max_sim),8:overlap_title,9:temp_diff,10:comm_auth\n",
    "#temp-diff should be normalized or preprocessed\n",
    "#as should common_auth?\n",
    "\n",
    "normalized_features = normalize(features,axis=0)\n",
    "max([features[i][8] for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "te_r = random.sample(testing_set,1000)\n",
    "\n",
    "# nodes = set([te_r[i][0] for i in range(len(te_r))])\n",
    "# nodes = nodes | set([te_r[i][1] for i in range(len(te_r))])\n",
    "# l1 = [l[node_dict[i]] for i in nodes]\n",
    "# test_kdtree = nw.KDTree(l1)\n",
    "\n",
    "test_all_edges = [(element[0],element[1]) for element in testing_set]\n",
    "test_graph = prep.article_graph(IDs,test_all_edges)\n",
    "\n",
    "test_features = []\n",
    "test_true_labels = []\n",
    "\n",
    "to_del = []\n",
    "for i,triple in enumerate(testing_set):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "    temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    if temp_diff<-1:\n",
    "        to_del.append((source,target))\n",
    "\n",
    "test_graph.delete_edges(to_del)\n",
    "\n",
    "temp=[]\n",
    "for i,triple in enumerate(te_r):\n",
    "    \n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "#     index_source = node_dict[source]\n",
    "#     index_target = node_dict[target]\n",
    "    \n",
    "#     source_info = node_info[index_source]\n",
    "#     target_info = node_info[index_target]\n",
    "\n",
    "#     # convert to lowercase and tokenize\n",
    "#     source_title = source_info[2].lower().split(\" \")\n",
    "#     # remove stopwords\n",
    "#     source_title = [token for token in source_title if token not in stpwds]\n",
    "#     source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "#     target_title = target_info[2].lower().split(\" \")\n",
    "#     target_title = [token for token in target_title if token not in stpwds]\n",
    "#     target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "#     source_auth = source_info[3].split(\",\")\n",
    "#     target_auth = target_info[3].split(\",\") \n",
    "\n",
    "#     overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "#     temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "#     comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    \n",
    "    f = []\n",
    "    f.append(pw.Max_Sim(source,target,l,test_graph,node_dict)[3])\n",
    "    f.append(pw.Citation_Check(source,target,kdtree,l,test_graph,node_dict,index_dict,k=500)[3])\n",
    "#     f.append(pw.peer_popularity(test_graph,source,target))\n",
    "#     f.append(pw.edge_check(source,target,test_graph))\n",
    "    f.append(pw.LSA_distance(source,target,node_dict,l,metric='COS'))\n",
    "#     f.append(pw.node_degree(source,target,test_graph)[2])\n",
    "#     f.append(overlap_title)\n",
    "#     f.append(temp_diff)\n",
    "#     f.append(comm_auth)\n",
    "    f.append(pw.succ_pred(source,target,graph)[3])\n",
    "    \n",
    "    \n",
    "    test_features.append(f)\n",
    "    test_true_labels.append(triple[2])\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "        \n",
    "test_features=np.array(test_features)\n",
    "test_true_labels=np.array(test_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply model to predict which edges to drop\n",
    "pred_probs = list(model.predict_proba(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'te_r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-11136b57601d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mto_del\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtriple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'te_r' is not defined"
     ]
    }
   ],
   "source": [
    "#generate features to input to model1\n",
    "\n",
    "test_features1 = []\n",
    "test_true_labels1 = []\n",
    "\n",
    "#delete based on preds from t_r1\n",
    "to_del=[]\n",
    "\n",
    "for i,triple in enumerate(te_r):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "    temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    \n",
    "    if pred_probs[i][1]<=0.15 and temp_diff>=-1:\n",
    "        to_del.append((source,target))\n",
    "\n",
    "test_graph.delete_edges(to_del)\n",
    "\n",
    "temp=[]\n",
    "for i,triple in enumerate(te_r):\n",
    "    \n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    \n",
    "    f = []\n",
    "    f.append(pw.Max_Sim(source,target,l,test_graph,node_dict)[3])\n",
    "    f.append(pw.LSA_distance(source,target,node_dict,l,metric='COS'))\n",
    "    f.append(pw.succ_pred(source,target,graph)[3])\n",
    "    \n",
    "    test_features1.append(f)\n",
    "    test_true_labels1.append(triple[2])\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "        \n",
    "test_features1=np.array(test_features1)\n",
    "test_true_labels1=np.array(test_true_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.92 f1: 0.9236641221374047\n"
     ]
    }
   ],
   "source": [
    "preds = list(model1.predict(test_features1))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('./features_test/reduced', 'wb') as f:\n",
    "#     pickle.dump([testing_set_reduced,test_graph,l,test_kdtree,node_dict,index_dict,test_true_labels],f)\n",
    "# with open('./features_test/reduced_features', 'wb') as f:\n",
    "#     pickle.dump(test_features,f)\n",
    "# order: max_sim,citation_check,peer_popularity,edge_check,lsa_distance,node_degree,title,time,author\n",
    "with open('./features_test/reduced', 'rb') as f:\n",
    "    testing_set_reduced,test_graph,l,test_kdtree,node_dict,index_dict,test_true_labels = pickle.load(f)\n",
    "with open('./features_test/reduced_features', 'rb') as f:\n",
    "    test_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# for citation_check:\n",
    "# mean of true but removed edges (with 50% edges removed) is 0.031\n",
    "# mean of no edge pairs (with 50% edges removed) is 0.0003\n",
    "\n",
    "[pw.Citation_Check(e[0],e[1],kdtree,l,graph,node_dict,index_dict,k=20) for e in random.sample(edges[150000:],20)]\n",
    "np.mean([pw.Citation_Check(e[0],e[1],kdtree,l,graph,node_dict,index_dict,k=20) for e in random.sample(no_citation,1500)])\n",
    "\n",
    "#for Max_Sim:\n",
    "#mean of true... is 1.13\n",
    "#mean of no edge... is 0.58\n",
    "a = np.mean([np.mean(pw.Max_Sim(e[0],e[1],l,graph,node_dict)) for e in random.sample(edges[150000:],1500)])\n",
    "b = np.mean([np.mean(pw.Max_Sim(e[0],e[1],l,graph,node_dict)) for e in random.sample(no_citation,1500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries:\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "# from sklearn import cross_validation, \n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "# from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def modelfit(alg, features, true_labels,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "#     if useTrainCV:\n",
    "#         xgb_param = alg.get_xgb_params()\n",
    "#         xgtrain = xgb.DMatrix(features, label=true_labels)\n",
    "#         cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "#             metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "#         alg.set_params(n_estimators=cvresult)\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(features, true_labels,eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(features)\n",
    "    dtrain_predprob = alg.predict_proba(features)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(true_labels, dtrain_predictions))\n",
    "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(true_labels, dtrain_predprob))\n",
    "                    \n",
    "#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9694\n",
      "AUC Score (Train): 0.995091\n",
      "acc: 0.873 f1: 0.865037194473964\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.01,#0.01\n",
    " max_depth=8, \n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " scale_pos_weight=0.67,\n",
    " objective= 'binary:logistic',\n",
    ")\n",
    "\n",
    "modelfit(xgb1, features, true_labels,useTrainCV=True)\n",
    "\n",
    "preds = xgb1.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  7, 11,  3, 13, 14, 16,  4,  0, 12,  2,  1,  5, 15, 17,  9,\n",
       "         8, 10]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(list(map(lambda x: abs(x),model.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.04329983,  37.74762202, -11.5304449 ,  31.1264739 ]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "xgb.plot_importance(xgb1, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.show()\n",
    "##order: 0-5max_sim,6-7citation_check,8peer_popularity,9edge_check,\n",
    "#10lsa_distance,11-14node_degree,15title,16time,17author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_features=features[:,[0,  2,  1, 15, 17,  9,8, 10]]\n",
    "short_test_features = test_features[:,[0,  2,  1, 15, 17,  9,8, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.921 f1: 0.9277218664226898\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import statistics as s\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = svm.LinearSVC()\n",
    "# classifier = SVC(gamma='auto')\n",
    "# try with gridsearch on C and gamma \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n",
    "\n",
    "# normalize\n",
    "features1 = np.zeros(features.shape)\n",
    "features1[:,0]=(features[:,0]-np.mean(features[:,0])) /s.stdev(features[:,0])\n",
    "features1[:,1]=(features[:,1]-np.mean(features[:,1])) /s.stdev(features[:,1])\n",
    "features1[:,2]=(features[:,2]-np.mean(features[:,2])) /s.stdev(features[:,2])\n",
    "features1[:,3]=(features[:,3]-np.mean(features[:,3])) /s.stdev(features[:,3])\n",
    "\n",
    "test_features1 = np.zeros(test_features.shape)\n",
    "test_features1[:,0]=(test_features[:,0]-np.mean(test_features[:,0])) /s.stdev(test_features[:,0])\n",
    "test_features1[:,1]=(test_features[:,1]-np.mean(test_features[:,1])) /s.stdev(test_features[:,1])\n",
    "test_features1[:,2]=(test_features[:,2]-np.mean(test_features[:,2])) /s.stdev(test_features[:,2])\n",
    "test_features1[:,3]=(test_features[:,3]-np.mean(test_features[:,3])) /s.stdev(test_features[:,3])\n",
    "\n",
    "                                                               \n",
    "\n",
    "classifier.fit(features1, true_labels)\n",
    "preds= list(classifier.predict(test_features1))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.922 f1: 0.9233791748526522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "model = lr(penalty='l1',solver='liblinear').fit(features,true_labels)\n",
    "cf = model.get_params()\n",
    "preds = list(model.predict(test_features))\n",
    "preds_probs = list(model.predict_proba(test_features))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(preds!=test_true_labels)[0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['207157',\n",
       " '2002',\n",
       " 'gauge invariant action for superstring in ramond-ramond plane-wave',\n",
       " 'Machiko Hatsuda, Kiyoshi Kamimura, Makoto Sakaguchi',\n",
       " 'Nucl.Phys.',\n",
       " 'background we present a gauge invariant action for a superstring in the plane wave background with ramond-ramond rr five-form flux the wess-zumino term is given explicitly in a bilinear form of the left invariant currents by introducing a fermionic center to define the nondegenerate group metric the reparametrization invariance generators whose combinations are conformal generators and fermionic constraints half of which generate kappa-symmetry are obtained equations of motion are obtained in conformal invariant and background covariant manners']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_info[node_dict[te_r[184][0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9909164',\n",
       " '2000',\n",
       " 'more d-branes in the nappi-witten background',\n",
       " \"JM Figueroa-O'Farrill, S Stanciu\",\n",
       " 'JHEP',\n",
       " 'we re-examine the problem of determining the possible d-branes in the nappi-witten background in addition to the known branes we find that there are also d-instantons flat euclidean d-strings and curved d-membranes admitting parallel spinors all of which can be interpreted as twisted conjugacy classes in the nappi-witten group']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_info[node_dict[te_r[184][1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(np.array(preds)=='1')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.898 f1: 0.8950617283950617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nNhbr = KNeighborsClassifier()\n",
    "nNhbr.fit(features,true_labels) # do Ytrain.ravel() for length one Y values\n",
    "preds = nNhbr.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.859 f1: 0.8507936507936508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dTree = DecisionTreeClassifier()\n",
    "dTree.fit(features,true_labels) # do Ytrain.ravel() for length one Y values\n",
    "preds = dTree.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "class Class_Net():\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, batch_size=60):\n",
    "        ''' initialize the classifier with default (best) parameters '''\n",
    "        tf.reset_default_graph()\n",
    "        self.alpha = learning_rate\n",
    "        self.beta = batch_size\n",
    "        self.warm = False\n",
    "\n",
    "    def fit(self,X,Y,warm_start=True,n_epochs=50):\n",
    "        ''' train the network, and if warm_start, then do not reinit. the network\n",
    "            (if it has already been initialized)\n",
    "        '''\n",
    "        self.epochs=n_epochs\n",
    "\n",
    "        self.n_batch = int(len(X)/self.beta)\n",
    "        \n",
    "        if warm_start==False or self.warm==False:\n",
    "            self.x = tf.placeholder(tf.float32,shape=[None,len(X[0])])\n",
    "            self.y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "            \n",
    "            self.wZero = tf.get_variable('wZero',shape=[len(X[0]),50],initializer=tf.glorot_uniform_initializer())\n",
    "            self.bZero = tf.Variable(tf.zeros([50]))\n",
    "\n",
    "            self.wOne = tf.get_variable('wOne',shape=[50,1],initializer=tf.glorot_uniform_initializer())\n",
    "            self.bOne = tf.Variable(tf.zeros([1]))\n",
    "            self.keep_prob = 1\n",
    "            self.drop_out = tf.nn.dropout(self.x, self.keep_prob)\n",
    "            self.model = tf.nn.sigmoid(tf.matmul(tf.nn.relu(tf.matmul(self.drop_out, self.wZero) \n",
    "                                              + self.bZero),self.wOne)+self.bOne)\n",
    "            self.cost = tf.losses.log_loss(self.y,self.model)\n",
    "            \n",
    "#             self.optimizer = tf.train.GradientDescentOptimizer(self.alpha).minimize(self.cost)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.alpha).minimize(self.cost)\n",
    "\n",
    "            #without dropout\n",
    "#             self.model = tf.matmul(tf.nn.relu(tf.matmul(self.x, self.wZero) + self.bZero),self.wOne)+self.bOne\n",
    "\n",
    "    \n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        with tf.Session() as sess:\n",
    "            if warm_start==False or self.warm==False:\n",
    "                sess.run(self.init)\n",
    "            else:\n",
    "                self.saver.restore(sess, './tempVariables.ckpt')\n",
    "            for epoch in range(self.epochs):\n",
    "                self.avg_cost = 0\n",
    "                for i in range(self.n_batch):\n",
    "                    _, self.c = sess.run([self.optimizer,self.cost], feed_dict={self.x: X[i*self.beta:min([(i+1)*\n",
    "                                    self.beta,len(X)]),:],self.y:Y[i*self.beta:min([(i+1)*self.beta,len(X)])]})\n",
    "                    \n",
    "                    self.avg_cost = self.avg_cost+np.mean(self.c)/self.n_batch\n",
    "                print(\"Epoch:\", '%s' % (epoch+1), \"cost=\", \"%s\"% (self.avg_cost))\n",
    "            self.saver.save(sess,'./tempVariables.ckpt')\n",
    "            \n",
    "        self.warm = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        ''' return a matrix P where P[i,j] = P(Y[i,j]=1), \n",
    "        for all instances i, and labels j. '''\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, './tempVariables.ckpt')\n",
    "            self.preds = sess.run(tf.nn.softmax(self.model), feed_dict={self.x: X}) \n",
    "        return self.preds\n",
    "    \n",
    "    def predict(self,X):\n",
    "        ''' return a matrix of predictions for X '''\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost= 0.470547448694706\n",
      "Epoch: 2 cost= 0.23319510295987123\n",
      "Epoch: 3 cost= 0.19172057390213015\n",
      "Epoch: 4 cost= 0.18152257233858102\n",
      "Epoch: 5 cost= 0.17774522311985488\n",
      "Epoch: 6 cost= 0.17575791824609047\n",
      "Epoch: 7 cost= 0.17449985306710003\n",
      "Epoch: 8 cost= 0.1736179099604488\n",
      "Epoch: 9 cost= 0.1729239934310317\n",
      "Epoch: 10 cost= 0.17238157924264658\n",
      "Epoch: 11 cost= 0.17196933042258022\n",
      "Epoch: 12 cost= 0.17160887118428947\n",
      "Epoch: 13 cost= 0.1713329852372408\n",
      "Epoch: 14 cost= 0.1710657555609941\n",
      "Epoch: 15 cost= 0.1707602054625749\n",
      "Epoch: 16 cost= 0.17038857806473962\n",
      "Epoch: 17 cost= 0.17017941460013383\n",
      "Epoch: 18 cost= 0.17001727323979132\n",
      "Epoch: 19 cost= 0.1698780763521791\n",
      "Epoch: 20 cost= 0.16972051694989207\n",
      "Epoch: 21 cost= 0.16957205753773447\n",
      "Epoch: 22 cost= 0.16937380149960515\n",
      "Epoch: 23 cost= 0.1692746995761991\n",
      "Epoch: 24 cost= 0.169113512262702\n",
      "Epoch: 25 cost= 0.16891514588147408\n",
      "Epoch: 26 cost= 0.168740381449461\n",
      "Epoch: 27 cost= 0.16857087582349775\n",
      "Epoch: 28 cost= 0.16835596967488525\n",
      "Epoch: 29 cost= 0.1681946907564997\n",
      "Epoch: 30 cost= 0.1679003911092877\n",
      "Epoch: 31 cost= 0.16760278224945083\n",
      "Epoch: 32 cost= 0.1667554998025298\n",
      "Epoch: 33 cost= 0.16613748725503674\n",
      "Epoch: 34 cost= 0.16532940823584794\n",
      "Epoch: 35 cost= 0.16475815385580056\n",
      "INFO:tensorflow:Restoring parameters from ./tempVariables.ckpt\n",
      "acc: 0.527 f1: 0.6902423051735429\n"
     ]
    }
   ],
   "source": [
    "net = Class_Net(learning_rate=0.005,batch_size=50)\n",
    "net.fit(features,list(map(lambda x: [x],true_labels)),n_epochs=35)\n",
    "preds=net.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
