{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f4090a4b4cde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prep' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(prep)\n",
    "importlib.reload(pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jacob_pfau/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jacob_pfau/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import features_nodewise as nw\n",
    "import features_pairwise as pw\n",
    "import preprocessing as prep\n",
    "\n",
    "import importlib\n",
    "# import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "\n",
    "# Read train / test node pairs\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "with open(\"./train_train_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "with open(\"./train_test_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "    \n",
    "with open(\"./testing_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    competition_set  = list(reader)\n",
    "    \n",
    "with open(\"./rf_predictions.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    competition_predictions  = list(reader)\n",
    "    \n",
    "competition_set = [element[0].split(\" \") for element in competition_set]\n",
    "\n",
    "random.seed(0)\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "to_keep_train = random.sample( range(len(training_set)),k=int(round(len(training_set)*0.05)) )\n",
    "training_set_reduced = [training_set[i] for i in to_keep_train]\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "to_keep_test = random.sample( range(len(testing_set)),k=int(round(len(testing_set)*1)) )\n",
    "testing_set_reduced = [testing_set[i] for i in to_keep_test]\n",
    "\n",
    "with open(\"./node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "    \n",
    "# corpus = [element[5] for element in node_info]\n",
    "# titles = [element[2] for element in node_info]\n",
    "\n",
    "# t_titles = prep.tfidf(titles)\n",
    "# t = prep.tfidf(corpus)\n",
    "# l = nw.LSA(t,n_components=100)\n",
    "\n",
    "# t_ngrams = prep.tfidf(corpus, r= (2,3), midf = 2, madf=0.5,feats=150000, sublinear = True)\n",
    "# l_ngrams = nw.LSA(t_ngrams,n_components=300)\n",
    "\n",
    "# kdtree = nw.KDTree(l)\n",
    "# kdtree_n = nw.KDTree(l_ngrams)\n",
    "\n",
    "# IDs = [element[0] for element in node_info]\n",
    "# node_dict = prep.to_dict( [element[0] for element in node_info],range(len(node_info)) )\n",
    "# index_dict = prep.to_dict(range(len(IDs)),IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_sim = t_titles*t_titles.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "competition_set_withpreds = copy.deepcopy(competition_set)\n",
    "for i in range(len(competition_set)):\n",
    "    competition_set_withpreds[i].append(competition_predictions[i+1][1])\n",
    "competition_set_withpreds = np.array(competition_set_withpreds)\n",
    "\n",
    "all_set = np.concatenate((training_set,testing_set))\n",
    "all_set = np.concatenate((all_set,competition_set_withpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# create graphs (train vs test), (reduced vs full)\n",
    "# for some reason np.mean([nw.node_degree(t[0],gold_train_graph) for t in random.sample(training_set,10000)]) is less than mean for competition graph?\n",
    "\n",
    "train_IDs = np.array(node_info)[:,0] \n",
    "# train_edges = [(element[0],element[1]) for element in training_set]\n",
    "# train_graph = prep.article_graph(train_IDs,train_edges)\n",
    "train_edges = [(element[0],element[1]) for element in training_set if element[2]=='1']\n",
    "train_graph = prep.article_graph(train_IDs,train_edges)\n",
    "\n",
    "#test graph is train graph\n",
    "# test_IDs = train_IDs\n",
    "# test_edges = train_edges\n",
    "# test_graph = prep.article_graph(test_IDs,test_edges)\n",
    "\n",
    "competition_IDs = train_IDs\n",
    "competition_edges = train_edges\n",
    "competition_edges.extend([(element[0],element[1]) for element in testing_set if element[2]=='1'])\n",
    "competition_edges.extend([(element[0],element[1]) for element in competition_set_withpreds if element[2]=='1'])\n",
    "competition_graph = prep.article_graph(competition_IDs,competition_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read all the features that we have stored in files\n",
    "import os.path\n",
    "\n",
    "def to_feature_shape(feat):\n",
    "    feat = np.array(feat)\n",
    "    if len(feat.shape) == 1:#not a real array but just a long list\n",
    "        feat = np.reshape(feat,(feat.shape[0],1))\n",
    "    return feat\n",
    "\n",
    "#This method should throw an error if something goes wrong\n",
    "def read_feature(file_path):\n",
    "    f = open(file_path, 'rb')\n",
    "    feat = to_feature_shape(pickle.load(f))\n",
    "    f.close()\n",
    "    return feat\n",
    "    \n",
    "features_to_read = [\"overlap_title\",\n",
    "                    \"comm_auth\",\n",
    "                    \"temp_diff\",\n",
    "                    \"edge_check\",\n",
    "                    \"LSA_distance\",\n",
    "                    \"title_sim\",\n",
    "                    \"N_LSA_distance\",\n",
    "                    \"citation_check\",\n",
    "                    \"max_sim\",\n",
    "                    \"peer_popularity\",\n",
    "                    \"temporal_fit\",\n",
    "                    \"succ_pred\",\n",
    "                    \"node_degree\",\n",
    "                   \"path_length\",\n",
    "                   \"Reverse_Max_Sim\"]\n",
    "\n",
    "\n",
    "\n",
    "train_features_dict = dict()\n",
    "train_features_reduced_dict = dict()\n",
    "test_features_dict = dict()\n",
    "competition_features_dict = dict()\n",
    "\n",
    "for name in features_to_read:\n",
    "    # Train\n",
    "    loaded = False\n",
    "#     file_path = './features_train/'+name+'_goldgraphfixed'\n",
    "# #     if name != 'edge_check' and name!='max_sim' and name!='succ_pred' and name!='citation_check':\n",
    "#     try:\n",
    "#         this_feat = read_feature(file_path)\n",
    "#         train_features_dict[name] = this_feat\n",
    "#         train_features_reduced_dict[name] = this_feat[to_keep_train,:]\n",
    "#         loaded = True\n",
    "#         print(1)\n",
    "#     except:\n",
    "#         pass\n",
    "        \n",
    "#     if loaded==False:\n",
    "#         # Train reduced\n",
    "#         file_path = './features_train/'+name+'_reducedgraph'\n",
    "#         try:\n",
    "#             this_feat = read_feature(file_path)\n",
    "#             train_features_dict[name] = this_feat\n",
    "#             train_features_reduced_dict[name] = this_feat[to_keep_train,:]\n",
    "#             loaded = True\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#     if loaded==False:\n",
    "#         # Train reduced\n",
    "#         file_path = './features_train/'+name\n",
    "#         try:\n",
    "#             this_feat = read_feature(file_path)\n",
    "#             train_features_dict[name] = this_feat\n",
    "#             train_features_reduced_dict[name] = this_feat[to_keep_train,:]\n",
    "#             loaded = True\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#     loaded = False\n",
    "#     # Test\n",
    "# #     if name != 'edge_check' and name!='max_sim' and name!='succ_pred' and name!='citation_check':\n",
    "#     file_path = './features_test/'+name+'_goldgraphfixed'\n",
    "#     try:\n",
    "#         this_feat = read_feature(file_path)\n",
    "#         test_features_dict[name] = this_feat\n",
    "#         loaded = True\n",
    "#         print(2)\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "#     if loaded == False:\n",
    "#         # Test reduced\n",
    "#         file_path = './features_test/'+name+'_reducedgraph'\n",
    "#         try:\n",
    "#             this_feat = read_feature(file_path)\n",
    "#             test_features_dict[name] = this_feat\n",
    "#             loaded = True\n",
    "#         except:\n",
    "#             pass\n",
    "    \n",
    "#     if loaded == False:\n",
    "#         # Test reduced\n",
    "#         file_path = './features_test/'+name\n",
    "#         try:\n",
    "#             this_feat = read_feature(file_path)\n",
    "#             test_features_dict[name] = this_feat\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "    \n",
    "    \n",
    "    file_path = './'+name+'_allgraph'\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        competition_features_dict[name] = this_feat\n",
    "    except:\n",
    "        print('failed')\n",
    "    # Test reduced\n",
    "#     file_path = './features_competition/'+name+'_reducedgraph'\n",
    "#     try:\n",
    "#         this_feat = read_feature(file_path)\n",
    "#         competition_features_dict[name] = this_feat\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "#     file_path = './features_competition/'+name+'_goldgraphfixed'\n",
    "#     try:\n",
    "#         this_feat = read_feature(file_path)\n",
    "#         competition_features_dict[name] = this_feat\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.reshape(train_true_labels,(train_true_labels.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "train_true_labels = read_feature('./train_true_labels')\n",
    "train_true_labels = np.reshape(train_true_labels,(train_true_labels.shape[0],))\n",
    "train_true_labels_reduced = train_true_labels[to_keep_train]\n",
    "test_true_labels = read_feature('./test_true_labels')\n",
    "test_true_labels = np.reshape(test_true_labels,(test_true_labels.shape[0],))\n",
    "test_true_labels_reduced = test_true_labels[to_keep_test]\n",
    "\n",
    "all_true_labels = []\n",
    "for triple in training_set:\n",
    "    all_true_labels.append(triple[2])\n",
    "for triple in testing_set:\n",
    "    all_true_labels.append(triple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current wd:  /home/jacob_pfau\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "###   Write features to disk - Training  ###\n",
    "############################################\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def write_feature_to_disk(feat,file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(feat,file)\n",
    "\n",
    "print(\"Current wd: \",os.getcwd())\n",
    "# write_feature_to_disk(train_features_reduced_dict['title_sim'],'./features_train/title_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all features to one vector\n",
    "train_features_dict.keys()\n",
    "test_features_dict.keys()\n",
    "for key,feat in train_features_dict.items():\n",
    "    print(key,feat.shape)\n",
    "# train_features = np.concatenate([train_features_dict['overlap_title'],\n",
    "#                                          train_features_dict['comm_auth'],\n",
    "#                                          train_features_dict['temp_diff'],\n",
    "#                                          train_features_dict['citation_check'],\n",
    "#                                          train_features_dict['max_sim'],\n",
    "#                                          train_features_dict['peer_popularity'],\n",
    "#                                          train_features_dict['edge_check'],\n",
    "#                                          train_features_dict['succ_pred'],\n",
    "#                                          train_features_dict['LSA_distance'],\n",
    "#                                          train_features_dict['title_sim'],\n",
    "#                                          train_features_dict['temporal_fit'],\n",
    "#                                         train_features_dict['N_LSA_distance'],\n",
    "#                                         train_features_dict['path_length'],\n",
    "#                                         train_features_dict['node_degree']]                                            \n",
    "#                                         ,axis = 1)\n",
    "\n",
    "# train_features_reduced = np.concatenate([train_features_reduced_dict['overlap_title'],\n",
    "#                                          train_features_reduced_dict['comm_auth'],\n",
    "#                                          train_features_reduced_dict['temp_diff'],\n",
    "#                                          train_features_reduced_dict['citation_check'],\n",
    "#                                          train_features_reduced_dict['max_sim'],\n",
    "#                                          train_features_reduced_dict['peer_popularity'],\n",
    "#                                          train_features_reduced_dict['edge_check'],\n",
    "#                                          train_features_reduced_dict['succ_pred'],\n",
    "#                                          train_features_reduced_dict['LSA_distance'],\n",
    "#                                          train_features_reduced_dict['title_sim'],\n",
    "#                                          train_features_reduced_dict['temporal_fit'],\n",
    "#                                         train_features_reduced_dict['N_LSA_distance'],\n",
    "#                                         train_features_reduced_dict['path_length'],\n",
    "#                                         train_features_reduced_dict['node_degree']]\n",
    "#                                         ,axis = 1)\n",
    "\n",
    "# test_features = np.concatenate(        [test_features_dict['overlap_title'],\n",
    "#                                         test_features_dict['comm_auth'],\n",
    "#                                         test_features_dict['temp_diff'],\n",
    "#                                         test_features_dict['citation_check'],\n",
    "#                                         test_features_dict['max_sim'],\n",
    "#                                         test_features_dict['peer_popularity'],\n",
    "#                                         test_features_dict['edge_check'],\n",
    "#                                         test_features_dict['succ_pred'],\n",
    "#                                         test_features_dict['LSA_distance'],\n",
    "#                                         test_features_dict['title_sim'],\n",
    "#                                         test_features_dict['temporal_fit'],\n",
    "#                                        test_features_dict['N_LSA_distance'],\n",
    "#                                        test_features_dict['path_length'],\n",
    "#                                        test_features_dict['node_degree']]\n",
    "#                                         ,axis = 1)\n",
    "\n",
    "\n",
    "competition_features = np.concatenate(  [competition_features_dict['overlap_title'],\n",
    "                                        competition_features_dict['comm_auth'],\n",
    "                                        competition_features_dict['temp_diff'],\n",
    "                                        competition_features_dict['citation_check'],\n",
    "                                        competition_features_dict['max_sim'],\n",
    "                                        competition_features_dict['peer_popularity'],\n",
    "                                        competition_features_dict['edge_check'],\n",
    "                                        competition_features_dict['succ_pred'],\n",
    "                                        competition_features_dict['LSA_distance'],\n",
    "                                        competition_features_dict['title_sim'],\n",
    "                                        competition_features_dict['temporal_fit'],\n",
    "                                        competition_features_dict['N_LSA_distance'],\n",
    "                                        competition_features_dict['path_length'],\n",
    "                                        competition_features_dict['node_degree'],\n",
    "                                        competition_features_dict['Reverse_Max_Sim']]\n",
    "                                        ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures as PF\n",
    "\n",
    "poly = PF(degree=2, interaction_only=False)\n",
    "\n",
    "selection = [1, 7, 9, 10, 11, 15, 16, 19, 21, 22, 23, 6, 25, 26, 27]\n",
    "# selection = list(range(train_features_reduced.shape[1]))\n",
    "\n",
    "train_sp = poly.fit_transform(train_features[:,selection])\n",
    "test_sp = poly.fit_transform(test_features[:,selection])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as SS\n",
    "scaler = SS()\n",
    "normalized_competition_features = scaler.fit_transform(competition_features)\n",
    "# normalized_test_features = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_competition = len(competition_set)\n",
    "l_test = len(testing_set)\n",
    "reserved = l_competition+30000\n",
    "label_reserved = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features_reduced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-971ed7a3e340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hinge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# selection = [23, 1, 16, 20, 22, 6, 21]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features_reduced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msvm_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_features_reduced' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import copy\n",
    "\n",
    "classifier = svm.LinearSVC(loss='hinge')\n",
    "# selection = [23, 1, 16, 20, 22, 6, 21]\n",
    "selection = list(range(train_features_reduced.shape[1]))\n",
    "svm_s = copy.copy(selection)\n",
    "\n",
    "# classifier.fit(train_features_reduced[:,selection], train_true_labels_reduced)\n",
    "# preds_svm = list(classifier.predict(test_features[:,selection]))\n",
    "classifier.fit(normalized_competition_features[:-reserved,selection], all_true_labels[:-reserved])\n",
    "svm_c = classifier\n",
    "preds_svm = list(classifier.predict(normalized_competition_features[-reserved:-l_competition,selection]))\n",
    "\n",
    "acc = metrics.accuracy_score(list(map(int,all_true_labels[-reserved:-l_competition])), list(map(int,preds_svm)))\n",
    "f1 = metrics.f1_score(list(map(int,all_true_labels[-reserved:-l_competition])), list(map(int,preds_svm)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9649333333333333 f1: 0.9672028931288191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr \n",
    "selection = [ 1,  7,  9, 10, 11, 15, 16, 19, 21, 22, 23]\n",
    "selection.extend([6,25,26,27])\n",
    "selection.append(31)\n",
    "lr_s = copy.copy(selection)\n",
    "# selection = [i for i in range(16)]\n",
    "# selection.extend([41,103,115])\n",
    "\n",
    "model = lr(penalty='l1').fit(train_features_reduced[:,selection], train_true_labels_reduced[:])\n",
    "lr_c = model\n",
    "preds_lg = list(model.predict(test_features[:,selection]))\n",
    "\n",
    "# model = lr(penalty='l1').fit(train_features_reduced, train_true_labels)\n",
    "# preds_lg = list(model.predict(test_features))\n",
    "\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_lg)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_lg)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argsort(list(map(lambda x: abs(x),model.coef_)))[0][:14]\n",
    "# sorted(list(map(lambda x: abs(x),model.coef_))[0],reverse=True)\n",
    "[poly.powers_[i,:] for i in [31,28,9,17,13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9581021178788376 f1: 0.9613848202396804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "selection = [1, 2, 6, 13, 16, 20, 21, 22]\n",
    "knn_s = copy.copy(selection)\n",
    "\n",
    "nNhbr = KNeighborsClassifier(n_neighbors=9,weights='distance')\n",
    "nNhbr.fit(train_features_reduced[:,selection],train_true_labels_reduced) # do Ytrain.ravel() for length one Y values\n",
    "knn_c = nNhbr\n",
    "preds_knn = nNhbr.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_knn)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_knn)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9557872270563126 f1: 0.9595274951532184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# selection = [0, 1, 2, 8, 13, 16, 20, 22]\n",
    "selection = [ 0,  2,  5,  6,  7,  8,  9, 10, 11, 13, 14, 15, 16, 19, 20, 21, 22,23, 25, 26, 27]\n",
    "dt_s = copy.copy(selection)\n",
    "\n",
    "dTree = DecisionTreeClassifier()\n",
    "dTree.fit(train_features_reduced[:,selection],train_true_labels_reduced) # do Ytrain.ravel() for length one Y values\n",
    "dt_c = dTree\n",
    "\n",
    "preds_dt = dTree.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_dt)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_dt)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.50986568e-06, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.99997490e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "       [9.43410527e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "       ...,\n",
       "       [1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [9.24313612e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "       [7.56863879e-02, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.concatenate((probs_lr.reshape(-1,1),probs_dt.reshape(-1,1)),axis=1)\n",
    "np.concatenate((test,probs_dt.reshape(-1,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs_svm = svm_c.decision_function(normalized_test_features[:,svm_s])\n",
    "probs_lr = lr_c.predict_proba(test_features[:,lr_s])[:,0]\n",
    "probs_knn = knn_c.predict_proba(test_features[:,knn_s])[:,0]\n",
    "all_probs = [probs_svm,probs_lr,probs_knn]\n",
    "\n",
    "probs_features = all_probs[0].reshape(-1,1)\n",
    "for i in range(1,len(all_probs)):\n",
    "    probs_features = np.concatenate((probs_features,all_probs[i].reshape(-1,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9712067292138467 f1: 0.9731976148888756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "joint_model = lr(penalty='l1').fit(probs_features[:30000,:], test_true_labels[:30000])\n",
    "preds_lg = list(joint_model.predict(probs_features[30000:,:]))\n",
    "\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels[30000:])), list(map(int,preds_lg)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels[30000:])), list(map(int,preds_lg)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.970852428964253 f1: 0.9730234136409909\n"
     ]
    }
   ],
   "source": [
    "# Joined forces\n",
    "joined_DTree = DecisionTreeClassifier()\n",
    "\n",
    "preds_test_svm = np.reshape(preds_svm,(len(preds_svm),1))\n",
    "preds_test_lg = np.reshape(preds_lg,(len(preds_lg),1))\n",
    "preds_test_knn = np.reshape(preds_knn,(len(preds_knn),1))\n",
    "preds_test_dt = np.reshape(preds_dt,(len(preds_dt),1))\n",
    "combined_preds = np.concatenate([preds_test_svm,preds_test_lg,preds_test_knn,preds_test_dt],axis=1)\n",
    "\n",
    "joined_DTree.fit(combined_preds[0:50000,:], test_true_labels[0:50000])\n",
    "preds_joined = joined_DTree.predict(combined_preds[50000:,:])\n",
    "\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels[50000:])), list(map(int,preds_joined)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels[50000:])), list(map(int,preds_joined)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which features are important?\n",
    "# Histogram of the feature frequency for all selections that reached > 90% acc\n",
    "frequency = [0]*total_num_features\n",
    "\n",
    "num_good_preds = 0\n",
    "min_acc = 0.93\n",
    "for i,acc in enumerate(accs):\n",
    "    if acc > min_acc:\n",
    "        num_good_preds += 1\n",
    "        for f in feature_selections[i]:\n",
    "            frequency[f] += 1\n",
    "#frequency = [freq/num_good_preds for freq in frequency]\n",
    "print(\"\")\n",
    "print(\"number of classifiers: \",len(accs))\n",
    "print(\"number of accs >\",min_acc,\": \",sum([1 for acc in accs if acc > min_acc]))\n",
    "plt.figure()\n",
    "plt.bar(x=range(len(frequency)),height=frequency)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9740436709899852 f1: 0.9761527670935336\n"
     ]
    }
   ],
   "source": [
    "# Adaboost DecisionTrees\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "selection =  [ 0,  1,  2,  4,  5,  6,  7, 10, 15, 16, 18, 19, 20, 21, 22, 23, 25, 26]\n",
    "\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=4, min_samples_leaf = 1, min_samples_split = 2),\n",
    "                         n_estimators=750,learning_rate=0.01)\n",
    "ada.fit(train_features[:100000,selection],train_true_labels[:100000])\n",
    "preds_ada = ada.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_ada)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_ada)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)\n",
    "#0.9761527670935336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9645706780495813 f1: 0.9675674050918271\n"
     ]
    }
   ],
   "source": [
    "# ExtraTreesClassifier\n",
    "#fiddle with n_estimators and min_samples_leaf\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# selection = [1, 2, 6, 13, 16, 20, 22, 23]\n",
    "selection = [1, 2,  5,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
    "\n",
    "#add one to min_sample_leaf for full train_features (or change to 4)\n",
    "extraTrees = ExtraTreesClassifier(n_estimators=750,max_depth=90,min_samples_split=10,min_samples_leaf=0.00001)\n",
    "\n",
    "extraTrees.fit(competition_features[:-reserved,selection], all_true_labels[:-reserved])\n",
    "preds_extra = list(extraTrees.predict(competition_features[-reserved:-l_competition,selection]))\n",
    "\n",
    "acc = metrics.accuracy_score(list(map(int,all_true_labels[-reserved:-l_competition])), list(map(int,preds_extra)))\n",
    "f1 = metrics.f1_score(list(map(int,all_true_labels[-reserved:-l_competition])), list(map(int,preds_extra)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.762773513793945\n",
      "acc: 0.9787 f1: 0.9803354362209571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#f1: 0.9803 750 estimators takes ~5 mins and ~10 mins with 1500 estimators\n",
    "t0 = time.time()\n",
    "selection = [2,  3,  5,  6,  7, 10, 11, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, \n",
    "             30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
    "\n",
    "randForest = RandomForestClassifier(n_estimators = 750, min_samples_split = 5, min_samples_leaf = 4, \n",
    "                                    max_depth = 60, n_jobs = -1)\n",
    "\n",
    "randForest.fit(competition_features[:-reserved,selection], all_true_labels[:-label_reserved])\n",
    "preds_randForest = randForest.predict(competition_features[-reserved:-l_competition,selection])\n",
    "t1 = time.time()-t0\n",
    "print(t1)\n",
    "\n",
    "acc = metrics.accuracy_score(list(map(int,all_true_labels[-label_reserved:])), list(map(int,preds_randForest)))\n",
    "f1 = metrics.f1_score(list(map(int,all_true_labels[-label_reserved:])), list(map(int,preds_randForest)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = [2,  3,  5,  6,  7,  8,  9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "preds_randForest = randForest.predict(competition_features[-l_competition:,selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rf = list(preds_randForest)\n",
    "\n",
    "# write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n",
    "preds_rf = list(zip(range(len(competition_set)), preds_rf))\n",
    "\n",
    "with open(\"rf_predictions1.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    for row in preds_rf:\n",
    "        csv_out.writerow(row)\n",
    "        \n",
    "# with open('random_forest_model', 'wb') as file:\n",
    "#         pickle.dump(randForest,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9716795271712363 f1: 0.9739366926040642\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "selection = [2,22, 23, 25, 26, 7, 15, 21, 10, 5, 6,  16, 19, 27, 28, 29, 30, 31]\n",
    "# selection = [i for i in range(16)]\n",
    "# selection.extend([ 95, 102])\n",
    "\n",
    "xgb = XGBClassifier(objective= 'binary:logistic', subsample = 0.8, colsample_bytree=0.8, learning_rate=0.01, \n",
    "                     max_depth=5, min_child_weight = 4, gamma=0, reg_lambda=2)\n",
    "\n",
    "xgb.fit(train_features[:100000,selection],train_true_labels[:100000])\n",
    "preds_xgb = xgb.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels[:])), list(map(int,preds_xgb)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels[:])), list(map(int,preds_xgb)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "xgb = XGBClassifier(objective= 'binary:logistic', subsample = 0.8, colsample_bytree=0.8, learning_rate=0.01, \n",
    "                     max_depth=5, min_child_weight = 4, gamma=0, reg_lambda=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-017eb0d460a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m xgb = XGBClassifier(objective= 'binary:logistic', subsample = 0.8, colsample_bytree=0.8, learning_rate=0.01, \n\u001b[0m\u001b[1;32m      4\u001b[0m                      max_depth=5, min_child_weight = 4, gamma=0, reg_lambda=2)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "selection = [0,2,7,8,9,13,16,19,20,21,22,23,24,25,26,27]\n",
    "selection = [18,19,20,21]\n",
    "xgb = XGBClassifier(objective= 'binary:logistic', subsample = 0.8, colsample_bytree=0.8, learning_rate=0.01, \n",
    "                     max_depth=5, min_child_weight = 4, gamma=0, reg_lambda=2)\n",
    "\n",
    "xgb.fit(train_features[:25000,selection],train_true_labels[:25000])\n",
    "preds_xgb = xgb.predict(train_features[25000:30000,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,train_true_labels[25000:30000])), list(map(int,preds_xgb)))\n",
    "f1 = metrics.f1_score(list(map(int,train_true_labels[25000:30000])), list(map(int,preds_xgb)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_xgb = list(preds_xgb)\n",
    "\n",
    "# write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n",
    "preds_xgb = list(zip(range(len(competition_set)), preds_xgb))\n",
    "\n",
    "with open(\"xgb_predictions.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    for row in preds_xgb:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-5fe441d8fb56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcombined_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds_test_svm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds_test_lg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds_test_knn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds_test_et\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds_test_xgb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mVotingClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_true_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpreds_joined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVotingClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#### NEEDS TO BE FIXED\n",
    "preds_test_svm = np.reshape(preds_svm,(len(preds_svm),1))\n",
    "preds_test_lg = np.reshape(preds_lg,(len(preds_lg),1))\n",
    "preds_test_knn = np.reshape(preds_knn,(len(preds_knn),1))\n",
    "preds_test_et = np.reshape(preds_extra,(len(preds_extra),1))\n",
    "preds_test_xgb = np.reshape(preds_xgb,(len(preds_xgb),1))\n",
    "\n",
    "combined_preds = np.concatenate([preds_test_svm,preds_test_lg,preds_test_knn,preds_test_et,preds_test_xgb],axis=1)\n",
    "\n",
    "VotingClassifier.fit(combined_preds[0:50000,:], test_true_labels[0:50000])\n",
    "preds_joined = VotingClassifier.predict(combined_preds[50000:,:])\n",
    "\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels[50000:])), list(map(int,preds_joined)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels[50000:])), list(map(int,preds_joined)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_int = []\n",
    "for c in all_true_labels:\n",
    "    labels_int.append(int(c))\n",
    "labels_int = np.array(labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4225006103515625\n",
      "acc: 0.9789666666666667 f1: 0.9805756503001385\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as light\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "selection = [2,  3,  5,  6,  7, 10, 11, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, \n",
    "             30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
    "\n",
    "#boosting_type='rf',subsample_freq=1, subsample = 0.3, colsample_bytree = 0.5\n",
    "lgb = light.LGBMClassifier()\n",
    "\n",
    "lgb.fit(competition_features[:-reserved,selection], labels_int[:-label_reserved])\n",
    "preds_lgb = lgb.predict(competition_features[-reserved:-l_competition,selection])\n",
    "t1 = time.time()-t0\n",
    "print(t1)\n",
    "\n",
    "acc = metrics.accuracy_score(list(map(int,labels_int[-label_reserved:])), list(map(int,preds_lgb)))\n",
    "f1 = metrics.f1_score(list(map(int,labels_int[-label_reserved:])), list(map(int,preds_lgb)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1_macro\n",
      "\n",
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  32 | elapsed:   20.7s remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  32 | elapsed:   20.9s remaining:   53.3s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  32 | elapsed:   27.3s remaining:   39.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  32 | elapsed:   29.1s remaining:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  32 | elapsed:   34.0s remaining:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  32 | elapsed:   35.5s remaining:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  32 | elapsed:   41.1s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:   48.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': 45, 'min_child_samples': 50, 'num_leaves': 90}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.978 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': 45, 'min_child_samples': 50, 'num_leaves': 15}\n",
      "0.978 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': 45, 'min_child_samples': 50, 'num_leaves': 90}\n",
      "0.977 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': 45, 'min_child_samples': 3000, 'num_leaves': 15}\n",
      "0.978 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': 45, 'min_child_samples': 3000, 'num_leaves': 90}\n",
      "0.978 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 50, 'num_leaves': 15}\n",
      "0.978 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 50, 'num_leaves': 90}\n",
      "0.977 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 3000, 'num_leaves': 15}\n",
      "0.978 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 3000, 'num_leaves': 90}\n",
      "0.977 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.3, 'max_depth': 45, 'min_child_samples': 50, 'num_leaves': 15}\n",
      "0.975 (+/-0.001) for {'boosting_type': 'gbdt', 'learning_rate': 0.3, 'max_depth': 45, 'min_child_samples': 50, 'num_leaves': 90}\n",
      "0.978 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.3, 'max_depth': 45, 'min_child_samples': 3000, 'num_leaves': 15}\n",
      "0.978 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.3, 'max_depth': 45, 'min_child_samples': 3000, 'num_leaves': 90}\n",
      "0.977 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.3, 'max_depth': -1, 'min_child_samples': 50, 'num_leaves': 15}\n",
      "0.975 (+/-0.001) for {'boosting_type': 'gbdt', 'learning_rate': 0.3, 'max_depth': -1, 'min_child_samples': 50, 'num_leaves': 90}\n",
      "0.978 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.3, 'max_depth': -1, 'min_child_samples': 3000, 'num_leaves': 15}\n",
      "0.978 (+/-0.000) for {'boosting_type': 'gbdt', 'learning_rate': 0.3, 'max_depth': -1, 'min_child_samples': 3000, 'num_leaves': 90}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     13710\n",
      "           1       0.98      0.98      0.98     16290\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     30000\n",
      "   macro avg       0.98      0.98      0.98     30000\n",
      "weighted avg       0.98      0.98      0.98     30000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tuned_parameters = [{'num_leaves': [15,90],'min_child_samples': [50,3000],\n",
    "                    'max_depth': [45,-1], 'learning_rate': [0.1,0.3], 'boosting_type': ['gbdt']}]\n",
    "\n",
    "# [{'num_leaves': [15,30,60,90],'min_child_samples': [10,20,100,500,1500,3000],\n",
    "#                     'max_depth': [3,5,15,50,100,-1]}]\n",
    "selection = [2,  3,  5,  6,  7, 10, 11, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, \n",
    "             30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
    "\n",
    "scores = ['f1_macro'] #'accuracy_score'\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(light.LGBMClassifier(), tuned_parameters, cv=2,\n",
    "                       scoring='%s' % score, verbose=10, n_jobs=-1)\n",
    "    clf.fit(competition_features[:-reserved,selection], labels_int[:-label_reserved])\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = labels_int[-label_reserved:], clf.predict(competition_features[-reserved:-l_competition,selection])\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 25 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  50 | elapsed:  6.2min remaining: 28.2min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  50 | elapsed:  6.3min remaining: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  50 | elapsed:  6.5min remaining:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  50 | elapsed: 12.2min remaining: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  50 | elapsed: 12.3min remaining:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  50 | elapsed: 12.4min remaining:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  50 | elapsed: 12.7min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 16.3min finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0dd2c5cc5bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mrf_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomForest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompetition_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_true_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 333\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "randomForest = RandomForestClassifier(n_estimators = 500)\n",
    "selection = [2,  3,  5,  6,  7, 10, 11, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, \n",
    "             30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
    "\n",
    "\n",
    "\n",
    "min_samples_split = [2,5,10,20]\n",
    "min_samples_leaf = [2,4,8,16]\n",
    "max_depth = [30,60,120]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'min_samples_leaf': min_samples_leaf, 'min_samples_split': min_samples_split, 'max_depth': max_depth}\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "rf_random = RandomizedSearchCV(estimator = randomForest, param_distributions = random_grid, n_iter = 25, cv = 2, verbose = 10, random_state=42, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(competition_features[:200000,selection], all_true_labels[:200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-ac7e80b7bc89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rank_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# top[:5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# xgb_random.best_index_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# mode([rf_random.cv_results_['params'][i]['bootstrap'] for i in top[:15]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "top = np.argsort(rf_random.cv_results_['rank_test_score'])\n",
    "# top[:5]\n",
    "# xgb_random.best_index_\n",
    "# mode([rf_random.cv_results_['params'][i]['bootstrap'] for i in top[:15]])\n",
    "[ada_random.cv_results_['params'][i]['learning_rate'] for i in top[:10]]\n",
    "#n_estimators: 750\n",
    "#learning_rate: 0.01\n",
    "\n",
    "#max_depth = 4\n",
    "#min_samples_leaf = 1\n",
    "#min_samples_split = 2\n",
    "#min_weight_Fraction_leaf = 1e-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "selection = [ 1,  7,  9, 10, 11, 15, 16, 19, 21, 22, 23]\n",
    "selection.extend([6,25,26,27])\n",
    "\n",
    "#37,38,40: 0.966\n",
    "#42,43,50 .967\n",
    "#53,54,55\n",
    "#62,63,65\n",
    "#67,68,66\n",
    "#86,87,88\n",
    "#106-108\n",
    "def WRANDSEARCH(estimator, features, labels, baseline, num_features = 16, iterations = 50, \n",
    "                rate = 1.5, emp_avg = 0.968, emp_max = 0.975, emp_min = 0.96):\n",
    "    \n",
    "    cv0 = random.sample(range(len(features)),int(len(features)/2))\n",
    "    cv1 = list(set(range(len(features)))-set(cv0))\n",
    "    \n",
    "    weights = np.ones(features.shape[1])\n",
    "    tot_features = len(weights)\n",
    "    \n",
    "    #adjust weights so that by final iteration 50% of features selected are from high performing features and 50% are from the rest\n",
    "    #as it stands this does not work properly for the reduction of probability\n",
    "    cap = (num_features-len(baseline))/2\n",
    "    evals_per_feature = iterations*(num_features-len(baseline))/tot_features\n",
    "    alpha = rate*(cap-1)/evals_per_feature\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        if random.random()>0.5:\n",
    "            selection = baseline\n",
    "        else:\n",
    "            removed = [random.choice(baseline), random.choice(baseline)]\n",
    "            selection = [i for i in baseline if not i in removed]\n",
    "            \n",
    "        selection.extend(random.choices(range(27, features.shape[1]), weights = weights[27:], k = num_features-len(selection)))\n",
    "        \n",
    "        model = estimator.fit(features[cv0,:][:,selection],labels[cv0])\n",
    "        preds = model.predict(features[cv1,:][:,selection])\n",
    "        f1 = metrics.f1_score(list(map(int,labels[cv1])), list(map(int,preds)))\n",
    "        model = estimator.fit(features[cv1,:][:,selection],labels[cv1])\n",
    "        preds = model.predict(features[cv0,:][:,selection])\n",
    "        f1 = (metrics.f1_score(list(map(int,labels[cv1])), list(map(int,preds))) + f1)/2\n",
    "        \n",
    "        if i==0:\n",
    "            avg = 2*f1/3+emp_avg/3\n",
    "            \n",
    "        if f1-avg>0:\n",
    "            for j in selection:\n",
    "                weights[j] = weights[j]+alpha*(f1-avg)/(emp_max-avg)\n",
    "        else:\n",
    "            for j in selection:\n",
    "                weights[j] = weights[j]*(1-0.35*(avg-f1)/(avg-emp_min))\n",
    "            \n",
    "        avg = (avg*3+f1)/4\n",
    "        \n",
    "        print(i, '/', iterations)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 50\n",
      "1 / 50\n",
      "2 / 50\n",
      "3 / 50\n",
      "4 / 50\n",
      "5 / 50\n",
      "6 / 50\n",
      "7 / 50\n",
      "8 / 50\n",
      "9 / 50\n",
      "10 / 50\n",
      "11 / 50\n",
      "12 / 50\n",
      "13 / 50\n",
      "14 / 50\n",
      "15 / 50\n",
      "16 / 50\n",
      "17 / 50\n",
      "18 / 50\n",
      "19 / 50\n",
      "20 / 50\n",
      "21 / 50\n",
      "22 / 50\n",
      "23 / 50\n",
      "24 / 50\n",
      "25 / 50\n",
      "26 / 50\n",
      "27 / 50\n",
      "28 / 50\n",
      "29 / 50\n",
      "30 / 50\n",
      "31 / 50\n",
      "32 / 50\n",
      "33 / 50\n",
      "34 / 50\n",
      "35 / 50\n",
      "36 / 50\n",
      "37 / 50\n",
      "38 / 50\n",
      "39 / 50\n",
      "40 / 50\n",
      "41 / 50\n",
      "42 / 50\n",
      "43 / 50\n",
      "44 / 50\n",
      "45 / 50\n",
      "46 / 50\n",
      "47 / 50\n",
      "48 / 50\n",
      "49 / 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.89805278, 1.79589894, 1.        , 1.78291101,\n",
       "       1.        , 1.87606643, 1.        , 1.        , 1.83137194,\n",
       "       1.        , 1.90559243, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.92520885, 1.93640245, 1.        , 1.        ,\n",
       "       1.93564076, 1.93544767, 1.8534672 , 1.9270696 , 1.9209067 ,\n",
       "       1.91657448, 1.82074365, 1.80740913, 1.        , 1.00834313,\n",
       "       1.0032723 , 1.        , 1.00834313, 1.        , 1.00504387,\n",
       "       1.        , 1.03667017, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.78414592, 1.        ,\n",
       "       1.00625022, 1.        , 1.        , 1.        , 1.00049745,\n",
       "       1.00040459, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.00696388, 1.        , 1.9463327 , 1.        , 1.        ,\n",
       "       1.        , 1.0042149 , 1.        , 1.        , 1.        ,\n",
       "       1.00142482, 1.        , 1.00464836, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.00688077, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.86308314,\n",
       "       1.        , 1.00159776, 1.00159776, 1.06094796, 1.08603222,\n",
       "       1.        , 1.00696388, 1.00191304, 1.        , 1.        ,\n",
       "       1.92708286, 1.00531596, 1.        , 1.        , 1.00003107,\n",
       "       1.        , 1.00308204, 1.94044491, 1.0087655 , 1.00183555,\n",
       "       1.00618592, 1.00003107, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.0042149 , 1.01248941, 1.        , 1.00038791,\n",
       "       1.01248941, 1.04370495, 1.        , 1.        , 1.        ,\n",
       "       1.00183555, 1.        , 1.        , 1.00142482, 1.00191304,\n",
       "       1.00142723, 1.        , 1.00038791, 1.03392059, 1.        ,\n",
       "       1.0076386 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.00541057, 1.08497141,\n",
       "       1.06094796, 1.00026421, 1.        , 1.02685504, 1.        ,\n",
       "       1.        , 1.        , 1.00308204, 1.        , 1.01899156,\n",
       "       1.        , 1.        , 1.08197779])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr \n",
    "\n",
    "classifier = XGBClassifier(objective= 'binary:logistic', subsample = 0.8, colsample_bytree=0.8, learning_rate=0.01, \n",
    "                     max_depth=5, min_child_weight = 4, gamma=0, reg_lambda=2)\n",
    "\n",
    "weights = WRANDSEARCH(classifier, train_sp[:60000], train_true_labels[:60000], [1,  2,  4,  6,  9, 11, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27], \n",
    "                      num_features = 21, iterations = 50)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(weights>1.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 35 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "estimator = RandomForestClassifier(n_estimators = 500, min_samples_split = 5, min_samples_leaf = 4, \n",
    "                                    max_depth = 60, n_jobs = -1)\n",
    "selection = [2,  3,  5,  6,  7, 10, 11, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, \n",
    "             30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
    "# = ExtraTreesClassifier(n_estimators=500,max_depth=50,min_samples_split=10,min_samples_leaf=0.00001)\n",
    "# = DecisionTreeClassifier()\n",
    "# = lr(penalty='l1')\n",
    "# = svm.LinearSVC(loss='hinge',max_iter=10000)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "selector = RFECV(estimator, step=2, cv=2,min_features_to_select=15,verbose=10)\n",
    "selector = selector.fit(competition_features[:150000,selection], all_true_labels[:150000])\n",
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  7,  8, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20,\n",
       "        21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]),)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(selector.ranking_==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multi_func as mf\n",
    "\n",
    "tocompute_features = {'succ_pred':False, 'Max_Sim':False, 'Citation_Check':False, 'node_degree':False, 'Reverse_Max_Sim':True}\n",
    "p = mf.params(competition_graph, kdtree,l,node_dict,index_dict, to_do = tocompute_features, chunk_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "grouped_set = [all_set[2000*i:2000*(i+1)].tolist() for i in range(math.ceil(len(all_set)/2000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 2\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "0 / 5\n",
      "222.75855231285095\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import multi_func as mf\n",
    "import time\n",
    "\n",
    "pool = mp.Pool(24)\n",
    "\n",
    "t0 = time.time() \n",
    "path_dict_list = pool.map(p.all_paths_noparams, grouped_set, chunksize = 10)\n",
    "\n",
    "pool.close()\n",
    "\n",
    "t1 = time.time()-t0\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_path_dict = dict()\n",
    "for i in IDs:\n",
    "    all_path_dict[i] = dict()\n",
    "    \n",
    "for d in path_dict_list:\n",
    "    for e in d:\n",
    "        for f in d[e]:\n",
    "            all_path_dict[e][f] = d[e][f]\n",
    "\n",
    "with open('all_path_dict', 'wb') as file:\n",
    "        pickle.dump(all_path_dict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('all_path_dict', 'rb')\n",
    "all_path_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 1\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "0 / 2\n",
      "0 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "1 / 2\n",
      "223.579683303833\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "import multi_func as mf\n",
    "\n",
    "pool = mp.Pool(24)\n",
    "\n",
    "t0 = time.time() \n",
    "chunked_output = zip(*pool.map(p.by_chunk_noparams, grouped_set, chunksize = 10))\n",
    "\n",
    "pool.close()\n",
    "\n",
    "t1 = time.time()-t0\n",
    "print(t1)\n",
    "list_chunked_output = list(chunked_output)\n",
    "\n",
    "# out1, out2, out3 = zip(*pool.map(calc_stuff, range(0, 10 * offset, offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_create = ['Reverse_Max_Sim']\n",
    "#(sp,ms,cc,nd)\n",
    "\n",
    "insert_features_dict = dict()\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "\n",
    "for i in [4]:\n",
    "    for feature_list in list_chunked_output[i]:\n",
    "        insert_features_dict['Reverse_Max_Sim'].extend(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse_Max_Sim 648160\n"
     ]
    }
   ],
   "source": [
    "for (name,value) in insert_features_dict.items():\n",
    "    print(name,len(value))\n",
    "    insert_features_dict[name] = to_feature_shape(value)\n",
    "    \n",
    "for feat in features_to_create:\n",
    "    write_feature_to_disk(insert_features_dict[feat],'./'+feat+'_allgraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 44\n",
      "1 / 44\n",
      "2 / 44\n",
      "3 / 44\n",
      "4 / 44\n",
      "5 / 44\n",
      "6 / 44\n",
      "7 / 44\n",
      "8 / 44\n",
      "9 / 44\n",
      "10 / 44\n",
      "11 / 44\n",
      "12 / 44\n",
      "13 / 44\n",
      "14 / 44\n",
      "15 / 44\n",
      "16 / 44\n",
      "17 / 44\n",
      "18 / 44\n",
      "19 / 44\n",
      "20 / 44\n",
      "21 / 44\n",
      "22 / 44\n",
      "23 / 44\n",
      "24 / 44\n",
      "25 / 44\n",
      "26 / 44\n",
      "27 / 44\n",
      "28 / 44\n",
      "29 / 44\n",
      "30 / 44\n",
      "31 / 44\n",
      "32 / 44\n",
      "33 / 44\n",
      "34 / 44\n",
      "35 / 44\n",
      "36 / 44\n",
      "37 / 44\n",
      "38 / 44\n",
      "39 / 44\n",
      "40 / 44\n",
      "41 / 44\n",
      "42 / 44\n",
      "43 / 44\n",
      "68.4776840209961\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "features_to_create = ['Max_Sim']\n",
    "insert_features_dict = dict()\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "set_to_use = training_set[:2200]\n",
    "t0 = time.time()\n",
    "\n",
    "sp,ms,cc,nd = pw.by_chunk(set_to_use,train_graph, kdtree,l,node_dict,index_dict, pairs_subset_edges=True, chunk_size=50,\n",
    "                          to_do = {'succ_pred':False, 'Max_Sim':True, 'Citation_Check':False, 'node_degree':False}, k_cc=500, metric_ms='COS', n_ms=3)\n",
    "insert_features_dict['Max_Sim'].extend(ms)\n",
    "t1 = time.time()-t0\n",
    "\n",
    "        \n",
    "# for (name,value) in insert_features_dict.items():\n",
    "#     print(name,len(value))\n",
    "#     insert_features_dict[name] = to_feature_shape(value)\n",
    "\n",
    "print(t1)\n",
    "# for feat in features_to_create:\n",
    "#     write_feature_to_disk(insert_features_dict[feat],'./features_train/'+feat+'_goldgraphfixed')\n",
    "\n",
    "# features_to_create = ['node_degree']\n",
    "# insert_features_dict = dict()\n",
    "# for feat in features_to_create:\n",
    "#     insert_features_dict[feat] = []\n",
    "# set_to_use = testing_set\n",
    "\n",
    "# sp,ms,cc,nd = pw.by_chunk(testing_set,train_graph, kdtree,l,node_dict,index_dict, pairs_subset_edges=False, chunk_size=1000,\n",
    "#                           to_do = {'succ_pred':False, 'Max_Sim':False, 'Citation_Check':False, 'node_degree':True}, k_cc=500, metric_ms='COS', n_ms=3)\n",
    "# insert_features_dict['node_degree'].extend(nd)\n",
    "        \n",
    "# for (name,value) in insert_features_dict.items():\n",
    "#     print(name,len(value))\n",
    "#     insert_features_dict[name] = to_feature_shape(value)\n",
    "    \n",
    "# for feat in features_to_create:\n",
    "#     write_feature_to_disk(insert_features_dict[feat],'./features_test/'+feat+'_goldgraphfixed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 554602\n",
      "100 / 554602\n",
      "200 / 554602\n",
      "300 / 554602\n",
      "400 / 554602\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-8216f043102e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m#     insert_features_dict[\"node_degree\"].append(degree)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0msucc_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msucc_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0minsert_features_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"succ_pred\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msucc_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/JUPYTER PROJECTS/ML/project/features_pairwise.py\u001b[0m in \u001b[0;36msucc_pred\u001b[0;34m(source_ID, target_ID, graph)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeleted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mdeleted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/igraph/__init__.py\u001b[0m in \u001b[0;36madd_edge\u001b[0;34m(self, source, target, **kwds)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0meid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mecount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/igraph/__init__.py\u001b[0m in \u001b[0;36madd_edges\u001b[0;34m(self, es)\u001b[0m\n\u001b[1;32m    253\u001b[0m           \u001b[0mendpoints\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mVertices\u001b[0m \u001b[0mare\u001b[0m \u001b[0menumerated\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \"\"\"\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mGraphBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_vertex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################################\n",
    "###  Construct features on TRAINING_SET  ###\n",
    "############################################\n",
    "\n",
    "\n",
    "train_true_labels = []\n",
    "features_to_create = [#'peer_popularity', 'max_sim', 'edge_check', 'title_sim', 'citation_check', 'node_degree','succ_pred', 'N_LSA_distance']\n",
    "#'peer_popularity', 'max_sim', 'edge_check', 'title_sim', 'citation_check', 'node_degree','succ_pred', 'N_LSA_distance'\n",
    "    \n",
    "# Where to insert created features\n",
    "insert_features_dict = train_features_dict\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "set_to_use = training_set\n",
    "title_sim_ones = []\n",
    "title_sim_zeros = []\n",
    "\n",
    "for i,triple in enumerate(set_to_use):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "    # convert to lowercase and tokenize\n",
    "#     source_title = source_info[2].lower().split(\" \")\n",
    "    # remove stopwords\n",
    "#     source_title = [token for token in source_title if token not in stpwds]\n",
    "#     source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "#     target_title = target_info[2].lower().split(\" \")\n",
    "#     target_title = [token for token in target_title if token not in stpwds]\n",
    "#     target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "#     source_auth = source_info[3].split(\",\")\n",
    "#     target_auth = target_info[3].split(\",\") \n",
    "    \n",
    "    # Creating features\n",
    "    # Baseline #\n",
    "    #overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "    #insert_features_dict[\"overlap_title\"].append(overlap_title)\n",
    "    #temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    #insert_features_dict[\"temp_diff\"].append(temp_diff)\n",
    "    #comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    #insert_features_dict[\"comm_auth\"].append(comm_auth)\n",
    "\n",
    "#     peer_pop = pw.peer_popularity(train_graph,source,target)\n",
    "#     insert_features_dict[\"peer_popularity\"].append(peer_pop)\n",
    "\n",
    "#     max_sim = pw.Max_Sim(source,target,l,train_graph,node_dict)\n",
    "#     insert_features_dict[\"max_sim\"].append(max_sim)\n",
    "\n",
    "#     edge_check = pw.edge_check(source,target,train_graph)\n",
    "#     insert_features_dict[\"edge_check\"].append(edge_check)\n",
    "\n",
    "    #LSA_dist = pw.LSA_distance(source,target,node_dict,l)\n",
    "    #insert_features_dict[\"LSA_distance\"].append(LSA_dist)\n",
    "    \n",
    "#     title_weighted = title_sim[index_source,index_target]\n",
    "#     insert_features_dict[\"title_sim\"].append(title_weighted)\n",
    "#     if(int(triple[2]) == 0):\n",
    "#         title_sim_zeros.append(title_weighted)\n",
    "#     else:\n",
    "#         title_sim_ones.append(title_weighted)\n",
    "#     citation_check = pw.Citation_Check(source,target,kdtree,l,train_graph,node_dict,index_dict,k=500)\n",
    "#     insert_features_dict[\"citation_check\"].append(citation_check)\n",
    "\n",
    "#     degree = pw.node_degree(source,target,train_graph)\n",
    "#     insert_features_dict[\"node_degree\"].append(degree)\n",
    "\n",
    "    succ_pred = pw.succ_pred(source,target,train_graph)\n",
    "    insert_features_dict[\"succ_pred\"].append(succ_pred)\n",
    "    \n",
    "#     N_LSA_dist = pw.LSA_distance(source,target,node_dict,l_ngrams)\n",
    "#     insert_features_dict[\"N_LSA_distance\"].append(N_LSA_dist)\n",
    "\n",
    "#     temporal_fit = pw.temp_fit(source,target,train_graph,node_dict,publication_years)\n",
    "#     insert_features_dict[\"temporal_fit\"].append(temporal_fit)\n",
    "    \n",
    "#     path_length = pw.path_length(source, target, training_paths_dict)\n",
    "#     insert_features_dict[\"path_length\"].append(path_length)\n",
    "    \n",
    "#     train_true_labels.append(triple[2])\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(i,\"/\",len(set_to_use))\n",
    "\n",
    "# Reshape features into np column arrays, one row per node pair\n",
    "for (name,value) in insert_features_dict.items():\n",
    "    print(name,len(value))\n",
    "    insert_features_dict[name] = to_feature_shape(value)\n",
    "    \n",
    "for feat in features_to_create:\n",
    "    write_feature_to_disk(insert_features_dict[feat],'./features_train/'+feat+'_goldgraphfixed')\n",
    "        \n",
    "# Concatenate all features\n",
    "# feats_train = np.concatenate([feat for feat in insert_features_dict.values()])\n",
    "# train_true_labels = np.array(train_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = './features_train/'+'max_sim'+'_reducedgraph'\n",
    "try:\n",
    "    this_feat = read_feature(file_path)\n",
    "except:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 175\n",
      "20 / 175\n",
      "40 / 175\n",
      "60 / 175\n",
      "80 / 175\n",
      "100 / 175\n",
      "120 / 175\n",
      "140 / 175\n",
      "160 / 175\n"
     ]
    }
   ],
   "source": [
    "testing_paths_dict = prep.all_paths(testing_set,train_graph,pairs_subset_edges=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 60910\n",
      "1000 / 60910\n",
      "2000 / 60910\n",
      "3000 / 60910\n",
      "4000 / 60910\n",
      "5000 / 60910\n",
      "6000 / 60910\n",
      "7000 / 60910\n",
      "8000 / 60910\n",
      "9000 / 60910\n",
      "10000 / 60910\n",
      "11000 / 60910\n",
      "12000 / 60910\n",
      "13000 / 60910\n",
      "14000 / 60910\n",
      "15000 / 60910\n",
      "16000 / 60910\n",
      "17000 / 60910\n",
      "18000 / 60910\n",
      "19000 / 60910\n",
      "20000 / 60910\n",
      "21000 / 60910\n",
      "22000 / 60910\n",
      "23000 / 60910\n",
      "24000 / 60910\n",
      "25000 / 60910\n",
      "26000 / 60910\n",
      "27000 / 60910\n",
      "28000 / 60910\n",
      "29000 / 60910\n",
      "30000 / 60910\n",
      "31000 / 60910\n",
      "32000 / 60910\n",
      "33000 / 60910\n",
      "34000 / 60910\n",
      "35000 / 60910\n",
      "36000 / 60910\n",
      "37000 / 60910\n",
      "38000 / 60910\n",
      "39000 / 60910\n",
      "40000 / 60910\n",
      "41000 / 60910\n",
      "42000 / 60910\n",
      "43000 / 60910\n",
      "44000 / 60910\n",
      "45000 / 60910\n",
      "46000 / 60910\n",
      "47000 / 60910\n",
      "48000 / 60910\n",
      "49000 / 60910\n",
      "50000 / 60910\n",
      "51000 / 60910\n",
      "52000 / 60910\n",
      "53000 / 60910\n",
      "54000 / 60910\n",
      "55000 / 60910\n",
      "56000 / 60910\n",
      "57000 / 60910\n",
      "58000 / 60910\n",
      "59000 / 60910\n",
      "60000 / 60910\n",
      "succ_pred 60910\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "###  Construct features on TESTING_SET  ###\n",
    "###########################################\n",
    "\n",
    "features_to_create = ['succ_pred']\n",
    "#'peer_popularity', 'max_sim', 'edge_check', 'title_sim', 'citation_check','node_degree','succ_pred','N_LSA_distance'\n",
    "\n",
    "# Where to insert created features\n",
    "insert_features_dict = test_features_dict\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "set_to_use = testing_set\n",
    "\n",
    "test_true_labels = []\n",
    "for i,triple in enumerate(set_to_use):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "    # convert to lowercase and tokenize\n",
    "#     source_title = source_info[2].lower().split(\" \")\n",
    "    # remove stopwords\n",
    "#     source_title = [token for token in source_title if token not in stpwds]\n",
    "#     source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "#     target_title = target_info[2].lower().split(\" \")\n",
    "#     target_title = [token for token in target_title if token not in stpwds]\n",
    "#     target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "#     source_auth = source_info[3].split(\",\")\n",
    "#     target_auth = target_info[3].split(\",\") \n",
    "    \n",
    "    \n",
    "    # Creating features\n",
    "    # Baseline #\n",
    "    #overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "    #insert_features_dict[\"overlap_title\"].append(overlap_title)\n",
    "    #temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    #insert_features_dict[\"temp_diff\"].append(temp_diff)\n",
    "    #comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    #insert_features_dict[\"comm_auth\"].append(comm_auth)\n",
    "\n",
    "#     peer_pop = pw.peer_popularity(train_graph,source,target)\n",
    "#     insert_features_dict[\"peer_popularity\"].append(peer_pop)\n",
    "\n",
    "#     max_sim = pw.Max_Sim(source,target,l,train_graph,node_dict)\n",
    "#     insert_features_dict[\"max_sim\"].append(max_sim)\n",
    "\n",
    "#     edge_check = pw.edge_check(source,target,train_graph)\n",
    "#     insert_features_dict[\"edge_check\"].append(edge_check)\n",
    "\n",
    "    #LSA_dist = pw.LSA_distance(source,target,node_dict,l)\n",
    "    #insert_features_dict[\"LSA_distance\"].append(LSA_dist)\n",
    "    \n",
    "#     title_weighted = title_sim[index_source,index_target]\n",
    "#     insert_features_dict[\"title_sim\"].append(title_weighted)\n",
    "    \n",
    "#     citation_check = pw.Citation_Check(source,target,kdtree,l,train_graph,node_dict,index_dict,k=500)\n",
    "#     insert_features_dict[\"citation_check\"].append(citation_check)\n",
    "    \n",
    "    succ_pred = pw.succ_pred(source,target,train_graph)\n",
    "    insert_features_dict[\"succ_pred\"].append(succ_pred)\n",
    "    \n",
    "#     degree = pw.node_degree(source,target,train_graph)\n",
    "#     insert_features_dict[\"node_degree\"].append(degree)\n",
    "    \n",
    "#     N_LSA_dist = pw.LSA_distance(source,target,node_dict,l_ngrams)\n",
    "#     insert_features_dict[\"N_LSA_distance\"].append(N_LSA_dist)\n",
    "\n",
    "#     temporal_fit = pw.temp_fit(source,target,train_graph,node_dict,publication_years)\n",
    "#     insert_features_dict[\"temporal_fit\"].append(temporal_fit)\n",
    "    \n",
    "#     path_length = pw.path_length(source, target, testing_paths_dict)\n",
    "#     insert_features_dict[\"path_length\"].append(path_length)\n",
    "    \n",
    "#     test_true_labels.append(triple[2])\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(i,\"/\",len(set_to_use))\n",
    "\n",
    "# Reshape features into np column arrays, one row per node pair\n",
    "for (name,value) in insert_features_dict.items():\n",
    "    print(name,len(value))\n",
    "    insert_features_dict[name] = to_feature_shape(value)\n",
    "        \n",
    "# Concatenate all features\n",
    "# feats_test = np.concatenate([feat for feat in insert_features_dict.values()])\n",
    "# test_true_labels = np.array(test_true_labels)\n",
    "\n",
    "for feat in features_to_create:\n",
    "    write_feature_to_disk(insert_features_dict[feat],'./features_test/'+feat+'_goldgraphfixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_info_array = np.array(node_info)\n",
    "publication_years = node_info_array[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 648160\n",
      "1000 / 648160\n",
      "2000 / 648160\n",
      "3000 / 648160\n",
      "4000 / 648160\n",
      "5000 / 648160\n",
      "6000 / 648160\n",
      "7000 / 648160\n",
      "8000 / 648160\n",
      "9000 / 648160\n",
      "10000 / 648160\n",
      "11000 / 648160\n",
      "12000 / 648160\n",
      "13000 / 648160\n",
      "14000 / 648160\n",
      "15000 / 648160\n",
      "16000 / 648160\n",
      "17000 / 648160\n",
      "18000 / 648160\n",
      "19000 / 648160\n",
      "20000 / 648160\n",
      "21000 / 648160\n",
      "22000 / 648160\n",
      "23000 / 648160\n",
      "24000 / 648160\n",
      "25000 / 648160\n",
      "26000 / 648160\n",
      "27000 / 648160\n",
      "28000 / 648160\n",
      "29000 / 648160\n",
      "30000 / 648160\n",
      "31000 / 648160\n",
      "32000 / 648160\n",
      "33000 / 648160\n",
      "34000 / 648160\n",
      "35000 / 648160\n",
      "36000 / 648160\n",
      "37000 / 648160\n",
      "38000 / 648160\n",
      "39000 / 648160\n",
      "40000 / 648160\n",
      "41000 / 648160\n",
      "42000 / 648160\n",
      "43000 / 648160\n",
      "44000 / 648160\n",
      "45000 / 648160\n",
      "46000 / 648160\n",
      "47000 / 648160\n",
      "48000 / 648160\n",
      "49000 / 648160\n",
      "50000 / 648160\n",
      "51000 / 648160\n",
      "52000 / 648160\n",
      "53000 / 648160\n",
      "54000 / 648160\n",
      "55000 / 648160\n",
      "56000 / 648160\n",
      "57000 / 648160\n",
      "58000 / 648160\n",
      "59000 / 648160\n",
      "60000 / 648160\n",
      "61000 / 648160\n",
      "62000 / 648160\n",
      "63000 / 648160\n",
      "64000 / 648160\n",
      "65000 / 648160\n",
      "66000 / 648160\n",
      "67000 / 648160\n",
      "68000 / 648160\n",
      "69000 / 648160\n",
      "70000 / 648160\n",
      "71000 / 648160\n",
      "72000 / 648160\n",
      "73000 / 648160\n",
      "74000 / 648160\n",
      "75000 / 648160\n",
      "76000 / 648160\n",
      "77000 / 648160\n",
      "78000 / 648160\n",
      "79000 / 648160\n",
      "80000 / 648160\n",
      "81000 / 648160\n",
      "82000 / 648160\n",
      "83000 / 648160\n",
      "84000 / 648160\n",
      "85000 / 648160\n",
      "86000 / 648160\n",
      "87000 / 648160\n",
      "88000 / 648160\n",
      "89000 / 648160\n",
      "90000 / 648160\n",
      "91000 / 648160\n",
      "92000 / 648160\n",
      "93000 / 648160\n",
      "94000 / 648160\n",
      "95000 / 648160\n",
      "96000 / 648160\n",
      "97000 / 648160\n",
      "98000 / 648160\n",
      "99000 / 648160\n",
      "100000 / 648160\n",
      "101000 / 648160\n",
      "102000 / 648160\n",
      "103000 / 648160\n",
      "104000 / 648160\n",
      "105000 / 648160\n",
      "106000 / 648160\n",
      "107000 / 648160\n",
      "108000 / 648160\n",
      "109000 / 648160\n",
      "110000 / 648160\n",
      "111000 / 648160\n",
      "112000 / 648160\n",
      "113000 / 648160\n",
      "114000 / 648160\n",
      "115000 / 648160\n",
      "116000 / 648160\n",
      "117000 / 648160\n",
      "118000 / 648160\n",
      "119000 / 648160\n",
      "120000 / 648160\n",
      "121000 / 648160\n",
      "122000 / 648160\n",
      "123000 / 648160\n",
      "124000 / 648160\n",
      "125000 / 648160\n",
      "126000 / 648160\n",
      "127000 / 648160\n",
      "128000 / 648160\n",
      "129000 / 648160\n",
      "130000 / 648160\n",
      "131000 / 648160\n",
      "132000 / 648160\n",
      "133000 / 648160\n",
      "134000 / 648160\n",
      "135000 / 648160\n",
      "136000 / 648160\n",
      "137000 / 648160\n",
      "138000 / 648160\n",
      "139000 / 648160\n",
      "140000 / 648160\n",
      "141000 / 648160\n",
      "142000 / 648160\n",
      "143000 / 648160\n",
      "144000 / 648160\n",
      "145000 / 648160\n",
      "146000 / 648160\n",
      "147000 / 648160\n",
      "148000 / 648160\n",
      "149000 / 648160\n",
      "150000 / 648160\n",
      "151000 / 648160\n",
      "152000 / 648160\n",
      "153000 / 648160\n",
      "154000 / 648160\n",
      "155000 / 648160\n",
      "156000 / 648160\n",
      "157000 / 648160\n",
      "158000 / 648160\n",
      "159000 / 648160\n",
      "160000 / 648160\n",
      "161000 / 648160\n",
      "162000 / 648160\n",
      "163000 / 648160\n",
      "164000 / 648160\n",
      "165000 / 648160\n",
      "166000 / 648160\n",
      "167000 / 648160\n",
      "168000 / 648160\n",
      "169000 / 648160\n",
      "170000 / 648160\n",
      "171000 / 648160\n",
      "172000 / 648160\n",
      "173000 / 648160\n",
      "174000 / 648160\n",
      "175000 / 648160\n",
      "176000 / 648160\n",
      "177000 / 648160\n",
      "178000 / 648160\n",
      "179000 / 648160\n",
      "180000 / 648160\n",
      "181000 / 648160\n",
      "182000 / 648160\n",
      "183000 / 648160\n",
      "184000 / 648160\n",
      "185000 / 648160\n",
      "186000 / 648160\n",
      "187000 / 648160\n",
      "188000 / 648160\n",
      "189000 / 648160\n",
      "190000 / 648160\n",
      "191000 / 648160\n",
      "192000 / 648160\n",
      "193000 / 648160\n",
      "194000 / 648160\n",
      "195000 / 648160\n",
      "196000 / 648160\n",
      "197000 / 648160\n",
      "198000 / 648160\n",
      "199000 / 648160\n",
      "200000 / 648160\n",
      "201000 / 648160\n",
      "202000 / 648160\n",
      "203000 / 648160\n",
      "204000 / 648160\n",
      "205000 / 648160\n",
      "206000 / 648160\n",
      "207000 / 648160\n",
      "208000 / 648160\n",
      "209000 / 648160\n",
      "210000 / 648160\n",
      "211000 / 648160\n",
      "212000 / 648160\n",
      "213000 / 648160\n",
      "214000 / 648160\n",
      "215000 / 648160\n",
      "216000 / 648160\n",
      "217000 / 648160\n",
      "218000 / 648160\n",
      "219000 / 648160\n",
      "220000 / 648160\n",
      "221000 / 648160\n",
      "222000 / 648160\n",
      "223000 / 648160\n",
      "224000 / 648160\n",
      "225000 / 648160\n",
      "226000 / 648160\n",
      "227000 / 648160\n",
      "228000 / 648160\n",
      "229000 / 648160\n",
      "230000 / 648160\n",
      "231000 / 648160\n",
      "232000 / 648160\n",
      "233000 / 648160\n",
      "234000 / 648160\n",
      "235000 / 648160\n",
      "236000 / 648160\n",
      "237000 / 648160\n",
      "238000 / 648160\n",
      "239000 / 648160\n",
      "240000 / 648160\n",
      "241000 / 648160\n",
      "242000 / 648160\n",
      "243000 / 648160\n",
      "244000 / 648160\n",
      "245000 / 648160\n",
      "246000 / 648160\n",
      "247000 / 648160\n",
      "248000 / 648160\n",
      "249000 / 648160\n",
      "250000 / 648160\n",
      "251000 / 648160\n",
      "252000 / 648160\n",
      "253000 / 648160\n",
      "254000 / 648160\n",
      "255000 / 648160\n",
      "256000 / 648160\n",
      "257000 / 648160\n",
      "258000 / 648160\n",
      "259000 / 648160\n",
      "260000 / 648160\n",
      "261000 / 648160\n",
      "262000 / 648160\n",
      "263000 / 648160\n",
      "264000 / 648160\n",
      "265000 / 648160\n",
      "266000 / 648160\n",
      "267000 / 648160\n",
      "268000 / 648160\n",
      "269000 / 648160\n",
      "270000 / 648160\n",
      "271000 / 648160\n",
      "272000 / 648160\n",
      "273000 / 648160\n",
      "274000 / 648160\n",
      "275000 / 648160\n",
      "276000 / 648160\n",
      "277000 / 648160\n",
      "278000 / 648160\n",
      "279000 / 648160\n",
      "280000 / 648160\n",
      "281000 / 648160\n",
      "282000 / 648160\n",
      "283000 / 648160\n",
      "284000 / 648160\n",
      "285000 / 648160\n",
      "286000 / 648160\n",
      "287000 / 648160\n",
      "288000 / 648160\n",
      "289000 / 648160\n",
      "290000 / 648160\n",
      "291000 / 648160\n",
      "292000 / 648160\n",
      "293000 / 648160\n",
      "294000 / 648160\n",
      "295000 / 648160\n",
      "296000 / 648160\n",
      "297000 / 648160\n",
      "298000 / 648160\n",
      "299000 / 648160\n",
      "300000 / 648160\n",
      "301000 / 648160\n",
      "302000 / 648160\n",
      "303000 / 648160\n",
      "304000 / 648160\n",
      "305000 / 648160\n",
      "306000 / 648160\n",
      "307000 / 648160\n",
      "308000 / 648160\n",
      "309000 / 648160\n",
      "310000 / 648160\n",
      "311000 / 648160\n",
      "312000 / 648160\n",
      "313000 / 648160\n",
      "314000 / 648160\n",
      "315000 / 648160\n",
      "316000 / 648160\n",
      "317000 / 648160\n",
      "318000 / 648160\n",
      "319000 / 648160\n",
      "320000 / 648160\n",
      "321000 / 648160\n",
      "322000 / 648160\n",
      "323000 / 648160\n",
      "324000 / 648160\n",
      "325000 / 648160\n",
      "326000 / 648160\n",
      "327000 / 648160\n",
      "328000 / 648160\n",
      "329000 / 648160\n",
      "330000 / 648160\n",
      "331000 / 648160\n",
      "332000 / 648160\n",
      "333000 / 648160\n",
      "334000 / 648160\n",
      "335000 / 648160\n",
      "336000 / 648160\n",
      "337000 / 648160\n",
      "338000 / 648160\n",
      "339000 / 648160\n",
      "340000 / 648160\n",
      "341000 / 648160\n",
      "342000 / 648160\n",
      "343000 / 648160\n",
      "344000 / 648160\n",
      "345000 / 648160\n",
      "346000 / 648160\n",
      "347000 / 648160\n",
      "348000 / 648160\n",
      "349000 / 648160\n",
      "350000 / 648160\n",
      "351000 / 648160\n",
      "352000 / 648160\n",
      "353000 / 648160\n",
      "354000 / 648160\n",
      "355000 / 648160\n",
      "356000 / 648160\n",
      "357000 / 648160\n",
      "358000 / 648160\n",
      "359000 / 648160\n",
      "360000 / 648160\n",
      "361000 / 648160\n",
      "362000 / 648160\n",
      "363000 / 648160\n",
      "364000 / 648160\n",
      "365000 / 648160\n",
      "366000 / 648160\n",
      "367000 / 648160\n",
      "368000 / 648160\n",
      "369000 / 648160\n",
      "370000 / 648160\n",
      "371000 / 648160\n",
      "372000 / 648160\n",
      "373000 / 648160\n",
      "374000 / 648160\n",
      "375000 / 648160\n",
      "376000 / 648160\n",
      "377000 / 648160\n",
      "378000 / 648160\n",
      "379000 / 648160\n",
      "380000 / 648160\n",
      "381000 / 648160\n",
      "382000 / 648160\n",
      "383000 / 648160\n",
      "384000 / 648160\n",
      "385000 / 648160\n",
      "386000 / 648160\n",
      "387000 / 648160\n",
      "388000 / 648160\n",
      "389000 / 648160\n",
      "390000 / 648160\n",
      "391000 / 648160\n",
      "392000 / 648160\n",
      "393000 / 648160\n",
      "394000 / 648160\n",
      "395000 / 648160\n",
      "396000 / 648160\n",
      "397000 / 648160\n",
      "398000 / 648160\n",
      "399000 / 648160\n",
      "400000 / 648160\n",
      "401000 / 648160\n",
      "402000 / 648160\n",
      "403000 / 648160\n",
      "404000 / 648160\n",
      "405000 / 648160\n",
      "406000 / 648160\n",
      "407000 / 648160\n",
      "408000 / 648160\n",
      "409000 / 648160\n",
      "410000 / 648160\n",
      "411000 / 648160\n",
      "412000 / 648160\n",
      "413000 / 648160\n",
      "414000 / 648160\n",
      "415000 / 648160\n",
      "416000 / 648160\n",
      "417000 / 648160\n",
      "418000 / 648160\n",
      "419000 / 648160\n",
      "420000 / 648160\n",
      "421000 / 648160\n",
      "422000 / 648160\n",
      "423000 / 648160\n",
      "424000 / 648160\n",
      "425000 / 648160\n",
      "426000 / 648160\n",
      "427000 / 648160\n",
      "428000 / 648160\n",
      "429000 / 648160\n",
      "430000 / 648160\n",
      "431000 / 648160\n",
      "432000 / 648160\n",
      "433000 / 648160\n",
      "434000 / 648160\n",
      "435000 / 648160\n",
      "436000 / 648160\n",
      "437000 / 648160\n",
      "438000 / 648160\n",
      "439000 / 648160\n",
      "440000 / 648160\n",
      "441000 / 648160\n",
      "442000 / 648160\n",
      "443000 / 648160\n",
      "444000 / 648160\n",
      "445000 / 648160\n",
      "446000 / 648160\n",
      "447000 / 648160\n",
      "448000 / 648160\n",
      "449000 / 648160\n",
      "450000 / 648160\n",
      "451000 / 648160\n",
      "452000 / 648160\n",
      "453000 / 648160\n",
      "454000 / 648160\n",
      "455000 / 648160\n",
      "456000 / 648160\n",
      "457000 / 648160\n",
      "458000 / 648160\n",
      "459000 / 648160\n",
      "460000 / 648160\n",
      "461000 / 648160\n",
      "462000 / 648160\n",
      "463000 / 648160\n",
      "464000 / 648160\n",
      "465000 / 648160\n",
      "466000 / 648160\n",
      "467000 / 648160\n",
      "468000 / 648160\n",
      "469000 / 648160\n",
      "470000 / 648160\n",
      "471000 / 648160\n",
      "472000 / 648160\n",
      "473000 / 648160\n",
      "474000 / 648160\n",
      "475000 / 648160\n",
      "476000 / 648160\n",
      "477000 / 648160\n",
      "478000 / 648160\n",
      "479000 / 648160\n",
      "480000 / 648160\n",
      "481000 / 648160\n",
      "482000 / 648160\n",
      "483000 / 648160\n",
      "484000 / 648160\n",
      "485000 / 648160\n",
      "486000 / 648160\n",
      "487000 / 648160\n",
      "488000 / 648160\n",
      "489000 / 648160\n",
      "490000 / 648160\n",
      "491000 / 648160\n",
      "492000 / 648160\n",
      "493000 / 648160\n",
      "494000 / 648160\n",
      "495000 / 648160\n",
      "496000 / 648160\n",
      "497000 / 648160\n",
      "498000 / 648160\n",
      "499000 / 648160\n",
      "500000 / 648160\n",
      "501000 / 648160\n",
      "502000 / 648160\n",
      "503000 / 648160\n",
      "504000 / 648160\n",
      "505000 / 648160\n",
      "506000 / 648160\n",
      "507000 / 648160\n",
      "508000 / 648160\n",
      "509000 / 648160\n",
      "510000 / 648160\n",
      "511000 / 648160\n",
      "512000 / 648160\n",
      "513000 / 648160\n",
      "514000 / 648160\n",
      "515000 / 648160\n",
      "516000 / 648160\n",
      "517000 / 648160\n",
      "518000 / 648160\n",
      "519000 / 648160\n",
      "520000 / 648160\n",
      "521000 / 648160\n",
      "522000 / 648160\n",
      "523000 / 648160\n",
      "524000 / 648160\n",
      "525000 / 648160\n",
      "526000 / 648160\n",
      "527000 / 648160\n",
      "528000 / 648160\n",
      "529000 / 648160\n",
      "530000 / 648160\n",
      "531000 / 648160\n",
      "532000 / 648160\n",
      "533000 / 648160\n",
      "534000 / 648160\n",
      "535000 / 648160\n",
      "536000 / 648160\n",
      "537000 / 648160\n",
      "538000 / 648160\n",
      "539000 / 648160\n",
      "540000 / 648160\n",
      "541000 / 648160\n",
      "542000 / 648160\n",
      "543000 / 648160\n",
      "544000 / 648160\n",
      "545000 / 648160\n",
      "546000 / 648160\n",
      "547000 / 648160\n",
      "548000 / 648160\n",
      "549000 / 648160\n",
      "550000 / 648160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551000 / 648160\n",
      "552000 / 648160\n",
      "553000 / 648160\n",
      "554000 / 648160\n",
      "555000 / 648160\n",
      "556000 / 648160\n",
      "557000 / 648160\n",
      "558000 / 648160\n",
      "559000 / 648160\n",
      "560000 / 648160\n",
      "561000 / 648160\n",
      "562000 / 648160\n",
      "563000 / 648160\n",
      "564000 / 648160\n",
      "565000 / 648160\n",
      "566000 / 648160\n",
      "567000 / 648160\n",
      "568000 / 648160\n",
      "569000 / 648160\n",
      "570000 / 648160\n",
      "571000 / 648160\n",
      "572000 / 648160\n",
      "573000 / 648160\n",
      "574000 / 648160\n",
      "575000 / 648160\n",
      "576000 / 648160\n",
      "577000 / 648160\n",
      "578000 / 648160\n",
      "579000 / 648160\n",
      "580000 / 648160\n",
      "581000 / 648160\n",
      "582000 / 648160\n",
      "583000 / 648160\n",
      "584000 / 648160\n",
      "585000 / 648160\n",
      "586000 / 648160\n",
      "587000 / 648160\n",
      "588000 / 648160\n",
      "589000 / 648160\n",
      "590000 / 648160\n",
      "591000 / 648160\n",
      "592000 / 648160\n",
      "593000 / 648160\n",
      "594000 / 648160\n",
      "595000 / 648160\n",
      "596000 / 648160\n",
      "597000 / 648160\n",
      "598000 / 648160\n",
      "599000 / 648160\n",
      "600000 / 648160\n",
      "601000 / 648160\n",
      "602000 / 648160\n",
      "603000 / 648160\n",
      "604000 / 648160\n",
      "605000 / 648160\n",
      "606000 / 648160\n",
      "607000 / 648160\n",
      "608000 / 648160\n",
      "609000 / 648160\n",
      "610000 / 648160\n",
      "611000 / 648160\n",
      "612000 / 648160\n",
      "613000 / 648160\n",
      "614000 / 648160\n",
      "615000 / 648160\n",
      "616000 / 648160\n",
      "617000 / 648160\n",
      "618000 / 648160\n",
      "619000 / 648160\n",
      "620000 / 648160\n",
      "621000 / 648160\n",
      "622000 / 648160\n",
      "623000 / 648160\n",
      "624000 / 648160\n",
      "625000 / 648160\n",
      "626000 / 648160\n",
      "627000 / 648160\n",
      "628000 / 648160\n",
      "629000 / 648160\n",
      "630000 / 648160\n",
      "631000 / 648160\n",
      "632000 / 648160\n",
      "633000 / 648160\n",
      "634000 / 648160\n",
      "635000 / 648160\n",
      "636000 / 648160\n",
      "637000 / 648160\n",
      "638000 / 648160\n",
      "639000 / 648160\n",
      "640000 / 648160\n",
      "641000 / 648160\n",
      "642000 / 648160\n",
      "643000 / 648160\n",
      "644000 / 648160\n",
      "645000 / 648160\n",
      "646000 / 648160\n",
      "647000 / 648160\n",
      "648000 / 648160\n",
      "path_length 648160\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "###  Construct features on COMPETITION_SET  ###\n",
    "###########################################\n",
    "\n",
    "features_to_create = ['path_length']\n",
    "# \"overlap_title\",\n",
    "#                  \"comm_auth\",\n",
    "#                  \"temp_diff\",\n",
    "#                 \"citation_check\",\n",
    "#                 \"max_sim\",\n",
    "#                 \"peer_popularity\",\n",
    "#                 \"edge_check\",\n",
    "#                 \"LSA_distance\",\n",
    "#                 \"node_degree\",\n",
    "#                 \"succ_pred\",\n",
    "#                 \"title_sim\",\n",
    "#                 \"temporal_fit\",\n",
    "#                 \"N_LSA_distance\",                 \n",
    "\n",
    "# Where to insert created features\n",
    "insert_features_dict = dict()\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "set_to_use = all_set\n",
    "\n",
    "for i,triple in enumerate(set_to_use):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "#     #convert to lowercase and tokenize\n",
    "#     source_title = source_info[2].lower().split(\" \")\n",
    "#     #remove stopwords\n",
    "#     source_title = [token for token in source_title if token not in stpwds]\n",
    "#     source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "#     target_title = target_info[2].lower().split(\" \")\n",
    "#     target_title = [token for token in target_title if token not in stpwds]\n",
    "#     target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "#     source_auth = source_info[3].split(\",\")\n",
    "#     target_auth = target_info[3].split(\",\") \n",
    "\n",
    "# sp, ms, cc = pw.by_chunk(competition_set,train_graph, kdtree,l,node_dict,index_dict, pairs_subset_edges=False, chunk_size=1000, k_cc=500, metric_ms='COS', n_ms=3)\n",
    "# insert_features_dict['citation_check'].extend(cc)\n",
    "# insert_features_dict['max_sim'].extend(ms)\n",
    "# insert_features_dict['succ_pred'].extend(sp)\n",
    "    \n",
    "    \n",
    "    # Creating features\n",
    "    # Baseline #\n",
    "#     overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "#     insert_features_dict[\"overlap_title\"].append(overlap_title)\n",
    "#     temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "#     insert_features_dict[\"temp_diff\"].append(temp_diff)\n",
    "#     comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "#     insert_features_dict[\"comm_auth\"].append(comm_auth)\n",
    "\n",
    "#     peer_pop = pw.peer_popularity(competition_graph,source,target)\n",
    "#     insert_features_dict[\"peer_popularity\"].append(peer_pop)\n",
    "\n",
    "#     edge_check = pw.edge_check(source,target,competition_graph)\n",
    "#     insert_features_dict['edge_check'].append(0)\n",
    "\n",
    "#     LSA_dist = pw.LSA_distance(source,target,node_dict,l)\n",
    "#     insert_features_dict[\"LSA_distance\"].append(LSA_dist)\n",
    "    \n",
    "#     ts = title_sim[index_source,index_target]\n",
    "#     insert_features_dict[\"title_sim\"].append(ts)\n",
    "    \n",
    "#     temporal_fit = pw.temp_fit(source,target,competition_graph,node_dict,publication_years)\n",
    "#     insert_features_dict[\"temporal_fit\"].append(temporal_fit)\n",
    "    \n",
    "    \n",
    "    path_length = pw.path_length(source, target, all_path_dict)\n",
    "    insert_features_dict[\"path_length\"].append(path_length)\n",
    "    \n",
    "#     N_LSA_dist = pw.LSA_distance(source,target,node_dict,l_ngrams)\n",
    "#     insert_features_dict[\"N_LSA_distance\"].append(N_LSA_dist)\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(i,\"/\",len(set_to_use))\n",
    "\n",
    "# Reshape features into np column arrays, one row per node pair\n",
    "for (name,value) in insert_features_dict.items():\n",
    "    print(name,len(value))\n",
    "    insert_features_dict[name] = to_feature_shape(value)\n",
    "        \n",
    "# Concatenate all features\n",
    "# feats_test = np.concatenate([feat for feat in insert_features_dict.values()])\n",
    "# test_true_labels = np.array(test_true_labels)\n",
    "\n",
    "for feat in features_to_create:\n",
    "    write_feature_to_disk(insert_features_dict[feat],'./'+feat+'_allgraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feat in features_to_create:\n",
    "    write_feature_to_disk(insert_features_dict[feat],'./features_test/'+feat+'_goldgraphfixed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
