{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "n_samples: 27770, n_features: 6174\n",
      "n_samples: 27770, n_features: 10000\n",
      "Performing dimensionality reduction using LSA\n",
      "Explained variance of the SVD step: 19%\n"
     ]
    }
   ],
   "source": [
    "import features_nodewise as nw\n",
    "import features_pairwise as pw\n",
    "import preprocessing as prep\n",
    "\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "\n",
    "## Read train / test node pairs\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "with open(\"./data/train_train_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "with open(\"./data/train_test_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "random.seed(0)\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "to_keep_train = random.sample( range(len(training_set)),k=int(round(len(training_set)*0.05)) )\n",
    "training_set_reduced = [training_set[i] for i in to_keep_train]\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "to_keep_test = random.sample( range(len(testing_set)),k=int(round(len(testing_set)*1)) )\n",
    "testing_set_reduced = [testing_set[i] for i in to_keep_test]\n",
    "\n",
    "with open(\"./data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "corpus = [element[5] for element in node_info]\n",
    "titles = [element[2] for element in node_info]\n",
    "\n",
    "t_titles = prep.tfidf(titles)\n",
    "t = prep.tfidf(corpus)\n",
    "l = nw.LSA(t)\n",
    "IDs = [element[0] for element in node_info]\n",
    "node_dict = prep.to_dict( [element[0] for element in node_info],range(len(node_info)) )\n",
    "index_dict = prep.to_dict(range(len(IDs)),IDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_sim = t_titles*t_titles.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graphs (train vs test), (reduced vs full)\n",
    "\n",
    "train_IDs = set([training_set[i][0] for i in range(len(training_set))])\n",
    "train_IDs = train_IDs | set([training_set[i][1] for i in range(len(training_set))])\n",
    "train_IDs = list(train_IDs) #igraph doesn't like sets...\n",
    "train_edges = [(element[0],element[1]) for element in training_set]\n",
    "train_graph = prep.article_graph(train_IDs,train_edges)\n",
    "\n",
    "train_IDs_reduced = set([training_set_reduced[i][0] for i in range(len(training_set_reduced))])\n",
    "train_IDs_reduced = train_IDs_reduced | set([training_set_reduced[i][1] for i in range(len(training_set_reduced))])\n",
    "train_IDs_reduced = list(train_IDs_reduced)\n",
    "train_edges_reduced = [(element[0],element[1]) for element in training_set_reduced]\n",
    "train_graph_reduced = prep.article_graph(train_IDs_reduced,train_edges_reduced)\n",
    "\n",
    "test_IDs = set([testing_set[i][0] for i in range(len(testing_set))])\n",
    "test_IDs = test_IDs | set([testing_set[i][1] for i in range(len(testing_set))])\n",
    "test_IDs = list(test_IDs)\n",
    "test_edges = [(element[0],element[1]) for element in testing_set]\n",
    "test_graph = prep.article_graph(test_IDs,test_edges)\n",
    "\n",
    "test_IDs_reduced = set([testing_set_reduced[i][0] for i in range(len(testing_set_reduced))])\n",
    "test_IDs_reduced = test_IDs_reduced | set([testing_set_reduced[i][1] for i in range(len(testing_set_reduced))])\n",
    "test_IDs_reduced = list(test_IDs_reduced)\n",
    "test_edges_reduced = [(element[0],element[1]) for element in testing_set_reduced]\n",
    "test_graph_reduced = prep.article_graph(test_IDs_reduced,test_edges_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9507159'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read all the features that we have stored in files\n",
    "import os.path\n",
    "\n",
    "def to_feature_shape(feat):\n",
    "    feat = np.array(feat)\n",
    "    if len(feat.shape) == 1:#not a real array but just a long list\n",
    "        feat = np.reshape(feat,(feat.shape[0],1))\n",
    "    return feat\n",
    "\n",
    "#This method should throw an error if something goes wrong\n",
    "def read_feature(file_path):\n",
    "    f = open(file_path, 'rb')\n",
    "    feat = to_feature_shape(pickle.load(f))\n",
    "    f.close()\n",
    "    return feat\n",
    "    \n",
    "features_to_read = [\"overlap_title\",\n",
    "                 \"comm_auth\",\n",
    "                 \"temp_diff\",\n",
    "                \"citation_check\",\n",
    "                \"max_sim\",\n",
    "                \"peer_popularity\",\n",
    "                \"edge_check\",\n",
    "                \"LSA_distance\",\n",
    "                \"node_degree\",\n",
    "                \"succ_pred\"]\n",
    "\n",
    "train_features_dict = dict()\n",
    "train_features_reduced_dict = dict()\n",
    "test_features_dict = dict()\n",
    "test_features_reduced_dict = dict()\n",
    "for name in features_to_read:\n",
    "    # Train\n",
    "    file_path = './features_train/'+name\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        train_features_dict[name] = this_feat\n",
    "        train_features_reduced_dict[name] = this_feat[to_keep_train,:]\n",
    "    except:\n",
    "        pass\n",
    "    # Train reduced\n",
    "    file_path = './features_train/'+name+'_reduced'\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        train_features_reduced_dict[name] = this_feat\n",
    "    except:\n",
    "        pass\n",
    "    # Test\n",
    "    file_path = './features_test/'+name\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        test_features_dict[name] = this_feat\n",
    "        test_features_reduced_dict[name] = this_feat[to_keep_test,:]\n",
    "    except:\n",
    "        pass\n",
    "    # Test reduced\n",
    "    file_path = './features_test/'+name+'_reduced'\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        test_features_reduced_dict[name] = this_feat\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c747d028dd67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_true_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_true_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "np.reshape(train_true_labels,(train_true_labels.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true_labels = read_feature('./features_train/true_labels')\n",
    "train_true_labels = np.reshape(train_true_labels,(train_true_labels.shape[0],))\n",
    "train_true_labels_reduced = train_true_labels[to_keep_train]\n",
    "test_true_labels = read_feature('./features_test/true_labels')\n",
    "test_true_labels = np.reshape(test_true_labels,(test_true_labels.shape[0],))\n",
    "test_true_labels_reduced = test_true_labels[to_keep_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 27730\n",
      "1000 / 27730\n",
      "2000 / 27730\n",
      "3000 / 27730\n",
      "4000 / 27730\n",
      "5000 / 27730\n",
      "6000 / 27730\n",
      "7000 / 27730\n",
      "8000 / 27730\n",
      "9000 / 27730\n",
      "10000 / 27730\n",
      "11000 / 27730\n",
      "12000 / 27730\n",
      "13000 / 27730\n",
      "14000 / 27730\n",
      "15000 / 27730\n",
      "16000 / 27730\n",
      "17000 / 27730\n",
      "18000 / 27730\n",
      "19000 / 27730\n",
      "20000 / 27730\n",
      "21000 / 27730\n",
      "22000 / 27730\n",
      "23000 / 27730\n",
      "24000 / 27730\n",
      "25000 / 27730\n",
      "26000 / 27730\n",
      "27000 / 27730\n",
      "overlap_title 27730\n",
      "comm_auth 27730\n",
      "temp_diff 27730\n",
      "citation_check 27730\n",
      "max_sim 27730\n",
      "peer_popularity 27730\n",
      "edge_check 27730\n",
      "LSA_distance 27730\n",
      "succ_pred 27730\n",
      "overlap_title_weighted 52936\n",
      "title_sim 27730\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-afd1e28d3c89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Concatenate all features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mfeats_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minsert_features_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0mtrain_true_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_true_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "###  Construct features on TRAINING_SET  ###\n",
    "############################################\n",
    "\n",
    "#Build KDTree on training_set\n",
    "train_l = [l[node_dict[i]] for i in train_IDs]\n",
    "train_kdtree = nw.KDTree(train_l)\n",
    "\n",
    "train_true_labels = []\n",
    "features_to_create = ['title_sim']\n",
    "# Where to insert created features\n",
    "insert_features_dict = train_features_reduced_dict\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "set_to_use = training_set_reduced\n",
    "title_sim_ones = []\n",
    "title_sim_zeros = []\n",
    "for i,triple in enumerate(set_to_use):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "    # convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    # remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\") \n",
    "    \n",
    "    # Creating features\n",
    "    # Baseline #\n",
    "    #overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "    #insert_features_dict[\"overlap_title\"].append(overlap_title)\n",
    "    #temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    #insert_features_dict[\"temp_diff\"].append(temp_diff)\n",
    "    #comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    #insert_features_dict[\"comm_auth\"].append(comm_auth)\n",
    "\n",
    "    #peer_pop = pw.peer_popularity(train_graph,source,target)\n",
    "    #insert_features_dict[\"peer_popularity\"].append(peer_pop)\n",
    "\n",
    "    #max_sim = pw.Max_Sim(source,target,l,train_graph,node_dict)\n",
    "    #insert_features_dict[\"max_sim\"].append(max_sim)\n",
    "\n",
    "    #edge_check = pw.edge_check(source,target,train_graph)\n",
    "    #insert_features_dict[\"edge_check\"].append(edge_check)\n",
    "\n",
    "    #LSA_dist = pw.LSA_distance(source,target,node_dict,l)\n",
    "    #insert_features_dict[\"LSA_distance\"].append(LSA_dist)\n",
    "    \n",
    "    title_weighted = title_sim[index_source,index_target]\n",
    "    insert_features_dict[\"title_sim\"].append(title_weighted)\n",
    "    if(int(triple[2]) == 0):\n",
    "        title_sim_zeros.append(title_weighted)\n",
    "    else:\n",
    "        title_sim_ones.append(title_weighted)\n",
    "    #citation_check = pw.Citation_Check(source,target,train_kdtree,l,train_graph,node_dict,index_dict,k=500)\n",
    "    #insert_features_dict[\"citation_check\"].append(citation_check)\n",
    "\n",
    "    #degree = pw.node_degree(source,target,train_graph)\n",
    "    #insert_features_dict[\"node_degree\"].append(degree)\n",
    "\n",
    "    #succ_pred = pw.succ_pred(source,target,train_graph)\n",
    "    #insert_features_dict[\"succ_pred\"].append(succ_pred)\n",
    "    \n",
    "    train_true_labels.append(triple[2])\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(i,\"/\",len(set_to_use))\n",
    "\n",
    "# Reshape features into np column arrays, one row per node pair\n",
    "for (name,value) in insert_features_dict.items():\n",
    "    print(name,len(value))\n",
    "    insert_features_dict[name] = to_feature_shape(value)\n",
    "        \n",
    "# Concatenate all features\n",
    "feats_train = np.concatenate([feat for feat in insert_features_dict.values()])\n",
    "train_true_labels = np.array(train_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap mean ones/zeros:  0.8299595141700404 0.14238332148779909\n",
      "title_sim mean ones/zeros:  0.10765941955415233 0.010216734701172472\n"
     ]
    }
   ],
   "source": [
    "title_sim_zeros = []\n",
    "title_sim_ones = []\n",
    "overlap_title_zeros = []\n",
    "overlap_title_ones = []\n",
    "\n",
    "for i in range(len(train_true_labels_reduced)):\n",
    "    if int(train_true_labels_reduced[i]) == 0:\n",
    "        overlap_title_zeros.append(train_features_reduced_dict['overlap_title'][i])\n",
    "        title_sim_zeros.append(train_features_reduced_dict['title_sim'][i])\n",
    "    else:\n",
    "        overlap_title_ones.append(train_features_reduced_dict['overlap_title'][i])\n",
    "        title_sim_ones.append(train_features_reduced_dict['title_sim'][i])\n",
    "    \n",
    "print(\"overlap mean ones/zeros: \", np.mean(overlap_title_ones),np.mean(overlap_title_zeros))\n",
    "print(\"title_sim mean ones/zeros: \", np.mean(title_sim_ones),np.mean(title_sim_zeros))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current wd:  /home/lucas/Documents/École Polytechnique/Courses/ML1/Challenge/git_repository/ML_project\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "###   Write features to disk - Training  ###\n",
    "############################################\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def write_feature_to_disk(feat,file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(feat,file)\n",
    "\n",
    "print(\"Current wd: \",os.getcwd())\n",
    "write_feature_to_disk(train_features_dict['succ_pred'],'./features_train/succ_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8af1c397ccd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_features_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'citation_check'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 60910\n",
      "1000 / 60910\n",
      "2000 / 60910\n",
      "3000 / 60910\n",
      "4000 / 60910\n",
      "5000 / 60910\n",
      "6000 / 60910\n",
      "7000 / 60910\n",
      "8000 / 60910\n",
      "9000 / 60910\n",
      "10000 / 60910\n",
      "11000 / 60910\n",
      "12000 / 60910\n",
      "13000 / 60910\n",
      "14000 / 60910\n",
      "15000 / 60910\n",
      "16000 / 60910\n",
      "17000 / 60910\n",
      "18000 / 60910\n",
      "19000 / 60910\n",
      "20000 / 60910\n",
      "21000 / 60910\n",
      "22000 / 60910\n",
      "23000 / 60910\n",
      "24000 / 60910\n",
      "25000 / 60910\n",
      "26000 / 60910\n",
      "27000 / 60910\n",
      "28000 / 60910\n",
      "29000 / 60910\n",
      "30000 / 60910\n",
      "31000 / 60910\n",
      "32000 / 60910\n",
      "33000 / 60910\n",
      "34000 / 60910\n",
      "35000 / 60910\n",
      "36000 / 60910\n",
      "37000 / 60910\n",
      "38000 / 60910\n",
      "39000 / 60910\n",
      "40000 / 60910\n",
      "41000 / 60910\n",
      "42000 / 60910\n",
      "43000 / 60910\n",
      "44000 / 60910\n",
      "45000 / 60910\n",
      "46000 / 60910\n",
      "47000 / 60910\n",
      "48000 / 60910\n",
      "49000 / 60910\n",
      "50000 / 60910\n",
      "51000 / 60910\n",
      "52000 / 60910\n",
      "53000 / 60910\n",
      "54000 / 60910\n",
      "55000 / 60910\n",
      "56000 / 60910\n",
      "57000 / 60910\n",
      "58000 / 60910\n",
      "59000 / 60910\n",
      "60000 / 60910\n",
      "overlap_title 60910\n",
      "comm_auth 60910\n",
      "temp_diff 60910\n",
      "citation_check 60910\n",
      "max_sim 60910\n",
      "peer_popularity 60910\n",
      "edge_check 60910\n",
      "LSA_distance 60910\n",
      "succ_pred 60910\n",
      "title_sim 60910\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-7d65934e4859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Concatenate all features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mfeats_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minsert_features_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mtest_true_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_true_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "###  Construct features on TESTING_SET  ###\n",
    "###########################################\n",
    "\n",
    "#Build KDTree on training_set\n",
    "train_l = [l[node_dict[i]] for i in train_IDs]\n",
    "train_kdtree = nw.KDTree(train_l)\n",
    "\n",
    "train_true_labels = []\n",
    "features_to_create = ['title_sim']\n",
    "\n",
    "# Where to insert created features\n",
    "insert_features_dict = test_features_dict\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "set_to_use = testing_set\n",
    "\n",
    "test_true_labels = []\n",
    "for i,triple in enumerate(set_to_use):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "    # convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    # remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\") \n",
    "    \n",
    "    \n",
    "    # Creating features\n",
    "    # Baseline #\n",
    "    #overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "    #insert_features_dict[\"overlap_title\"].append(overlap_title)\n",
    "    #temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    #insert_features_dict[\"temp_diff\"].append(temp_diff)\n",
    "    #comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    #insert_features_dict[\"comm_auth\"].append(comm_auth)\n",
    "\n",
    "    #peer_pop = pw.peer_popularity(train_graph,source,target)\n",
    "    #insert_features_dict[\"peer_popularity\"].append(peer_pop)\n",
    "\n",
    "    #max_sim = pw.Max_Sim(source,target,l,train_graph,node_dict)\n",
    "    #insert_features_dict[\"max_sim\"].append(max_sim)\n",
    "\n",
    "    #edge_check = pw.edge_check(source,target,train_graph)\n",
    "    #insert_features_dict[\"edge_check\"].append(edge_check)\n",
    "\n",
    "    #LSA_dist = pw.LSA_distance(source,target,node_dict,l)\n",
    "    #insert_features_dict[\"LSA_distance\"].append(LSA_dist)\n",
    "    \n",
    "    title_weighted = title_sim[index_source,index_target]\n",
    "    insert_features_dict[\"title_sim\"].append(title_weighted)\n",
    "    \n",
    "    #citation_check = pw.Citation_Check(source,target,train_kdtree,l,train_graph,node_dict,index_dict,k=500)\n",
    "    #insert_features_dict[\"citation_check\"].append(citation_check)\n",
    "    \n",
    "    #succ_pred = pw.succ_pred(source,target,train_graph)\n",
    "    #insert_features_dict[\"succ_pred\"].append(succ_pred)\n",
    "    \n",
    "    #degree = pw.node_degree(source,target,train_graph)\n",
    "    #insert_features_dict[\"node_degree\"].append(degree)\n",
    "\n",
    "    test_true_labels.append(triple[2])\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(i,\"/\",len(set_to_use))\n",
    "\n",
    "# Reshape features into np column arrays, one row per node pair\n",
    "for (name,value) in insert_features_dict.items():\n",
    "    print(name,len(value))\n",
    "    insert_features_dict[name] = to_feature_shape(value)\n",
    "        \n",
    "# Concatenate all features\n",
    "feats_test = np.concatenate([feat for feat in insert_features_dict.values()])\n",
    "test_true_labels = np.array(test_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current wd:  /home/lucas/Documents/École Polytechnique/Courses/ML1/Challenge/git_repository/ML_project\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "###   Write features to disk - Testing  ###\n",
    "###########################################\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def write_feature_to_disk(feat,file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(feat,file)\n",
    "\n",
    "print(\"Current wd: \",os.getcwd())\n",
    "write_feature_to_disk(test_features_dict['succ_pred'],'./features_test/succ_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27730, 4)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_reduced_dict['succ_pred'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap_title (27730, 1)\n",
      "comm_auth (27730, 1)\n",
      "temp_diff (27730, 1)\n",
      "citation_check (27730, 2)\n",
      "max_sim (27730, 9)\n",
      "peer_popularity (27730, 1)\n",
      "edge_check (27730, 1)\n",
      "LSA_distance (27730, 1)\n",
      "succ_pred (27730, 4)\n",
      "overlap_title_weighted (52936, 1)\n",
      "title_sim (27730, 1)\n"
     ]
    }
   ],
   "source": [
    "#Combine all features to one vector\n",
    "train_features_reduced_dict.keys()\n",
    "test_features_dict.keys()\n",
    "for key,feat in train_features_reduced_dict.items():\n",
    "    print(key,feat.shape)\n",
    "train_features_reduced = np.concatenate([train_features_reduced_dict['overlap_title'],\n",
    "                                         train_features_reduced_dict['comm_auth'],\n",
    "                                         train_features_reduced_dict['temp_diff'],\n",
    "                                         train_features_reduced_dict['citation_check'],\n",
    "                                         train_features_reduced_dict['max_sim'],\n",
    "                                         train_features_reduced_dict['peer_popularity'],\n",
    "                                         train_features_reduced_dict['edge_check'],\n",
    "                                         train_features_reduced_dict['succ_pred'],\n",
    "                                         train_features_reduced_dict['LSA_distance'],\n",
    "                                         train_features_reduced_dict['title_sim']]\n",
    "                                        ,axis = 1)\n",
    "\n",
    "test_features_reduced = np.concatenate([#test_features_reduced_dict['overlap_title'],\n",
    "                                        #test_features_reduced_dict['comm_auth'],\n",
    "                                        test_features_reduced_dict['temp_diff'],\n",
    "                                        test_features_reduced_dict['citation_check'],\n",
    "                                        test_features_reduced_dict['max_sim'],\n",
    "                                        test_features_reduced_dict['peer_popularity'],\n",
    "                                        #test_features_reduced_dict['edge_check'],\n",
    "                                        test_features_reduced_dict['LSA_distance']]\n",
    "                                        ,axis = 1)\n",
    "\n",
    "test_features = np.concatenate(        [test_features_dict['overlap_title'],\n",
    "                                        test_features_dict['comm_auth'],\n",
    "                                        test_features_dict['temp_diff'],\n",
    "                                        test_features_dict['citation_check'],\n",
    "                                        test_features_dict['max_sim'],\n",
    "                                        test_features_dict['peer_popularity'],\n",
    "                                        test_features_dict['edge_check'],\n",
    "                                        test_features_dict['succ_pred'],\n",
    "                                        test_features_dict['LSA_distance'],\n",
    "                                        test_features_dict['title_sim']]\n",
    "                                        ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27730, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "normalized_train_features = normalize(train_features_reduced,axis=0)\n",
    "normalized_test_features = normalize(test_features_reduced,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.947660482679363 f1: 0.9506577929113141\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.LinearSVC()\n",
    "selection = [21, 1, 14, 18, 20]\n",
    "\n",
    "classifier.fit(train_features_reduced[:,selection], train_true_labels_reduced)\n",
    "preds_svm = list(classifier.predict(test_features[:,selection]))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_svm)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_svm)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9432441306846167 f1: 0.9465183557913952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "selection = [21, 1, 2, 6, 11, 14, 18, 20]\n",
    "model = lr().fit(train_features_reduced[:,selection], train_true_labels_reduced)\n",
    "preds_lg = list(model.predict(test_features[:,selection]))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_lg)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_lg)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.942176982433098 f1: 0.9457084720680725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "selection = [1, 2, 4, 11, 14, 18, 19, 20]\n",
    "nNhbr = KNeighborsClassifier()\n",
    "nNhbr.fit(train_features_reduced[:,selection],train_true_labels_reduced) # do Ytrain.ravel() for length one Y values\n",
    "preds_knn = nNhbr.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_knn)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_knn)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9344442620259399 f1: 0.9393280962727728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "selection = [0, 1, 2, 6, 11, 14, 18, 20]\n",
    "dTree = DecisionTreeClassifier()\n",
    "dTree.fit(train_features_reduced[:,selection],train_true_labels_reduced) # do Ytrain.ravel() for length one Y values\n",
    "preds_dt = dTree.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_dt)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_dt)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9519706691109074 f1: 0.955668358714044\n"
     ]
    }
   ],
   "source": [
    "# Joined forces\n",
    "joined_DTree = DecisionTreeClassifier()\n",
    "\n",
    "preds_test_svm = np.reshape(preds_svm,(len(preds_svm),1))\n",
    "preds_test_lg = np.reshape(preds_lg,(len(preds_lg),1))\n",
    "preds_test_knn = np.reshape(preds_knn,(len(preds_knn),1))\n",
    "preds_test_dt = np.reshape(preds_dt,(len(preds_dt),1))\n",
    "combined_preds = np.concatenate([preds_test_svm,preds_test_lg,preds_test_knn,preds_test_dt],axis=1)\n",
    "\n",
    "joined_DTree.fit(combined_preds[0:50000,:], test_true_labels[0:50000])\n",
    "preds_joined = joined_DTree.predict(combined_preds[50000:,:])\n",
    "\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels[50000:])), list(map(int,preds_joined)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels[50000:])), list(map(int,preds_joined)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7900837300935807 f1: 0.7648075932602457\n",
      "features:  [13, 1, 8, 16, 15, 12, 14]\n",
      "acc: 0.8700213429650304 f1: 0.8682498211046579\n",
      "features:  [15, 11, 18, 6, 16]\n",
      "acc: 0.9041208340174027 f1: 0.9051424487541825\n",
      "features:  [20, 0, 15, 10, 7, 17, 11]\n",
      "acc: 0.9397143326219012 f1: 0.942607064707721\n",
      "features:  [2, 20, 4, 18, 1, 19, 14, 11]\n",
      "acc: 0.9424560827450337 f1: 0.9457673799687447\n",
      "features:  [11, 6, 14, 20, 2, 1, 18, 0]\n",
      "\n",
      "number of classifiers:  1359\n",
      "number of accs > 0.9: 118\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select random features and train a decision tree on it\n",
    "from time import clock\n",
    "\n",
    "random.seed(0)\n",
    "total_num_features = train_features_reduced.shape[1]\n",
    "time_limit = 600\n",
    "start_time = clock()\n",
    "feature_selections = []\n",
    "accs = []\n",
    "f1s = []\n",
    "while clock() < start_time + time_limit:\n",
    "    num_select_features = random.randint(1,10)\n",
    "    selection = random.sample(range(total_num_features),num_select_features)\n",
    "    \n",
    "    model = lr().fit(train_features_reduced[:,selection], train_true_labels_reduced)\n",
    "    preds = list(model.predict(test_features[:,selection]))\n",
    "\n",
    "    #nNhbr = KNeighborsClassifier()\n",
    "    #nNhbr.fit(train_features_reduced[:,selection],train_true_labels_reduced) # do Ytrain.ravel() for length one Y values\n",
    "    #preds = nNhbr.predict(test_features[:,selection])\n",
    "    \n",
    "    #classifier = svm.LinearSVC()\n",
    "    #classifier.fit(train_features_reduced[:,selection], train_true_labels_reduced)\n",
    "    #preds = list(classifier.predict(test_features[:,selection]))\n",
    "\n",
    "    #dTree = DecisionTreeClassifier()\n",
    "    #dTree.fit(train_features_reduced[:,selection],train_true_labels_reduced)\n",
    "    #preds = dTree.predict(test_features[:,selection])\n",
    "    \n",
    "    acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "    f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "    accs.append(acc)\n",
    "    f1s.append(f1)\n",
    "    feature_selections.append(selection)\n",
    "    if acc == max(accs):\n",
    "        print('acc:',acc,'f1:',f1)\n",
    "        print('features: ',selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of classifiers:  1023\n",
      "number of accs > 0.93 :  335\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Which features are important?\n",
    "# Histogram of the feature frequency for all selections that reached > 90% acc\n",
    "frequency = [0]*total_num_features\n",
    "\n",
    "num_good_preds = 0\n",
    "min_acc = 0.93\n",
    "for i,acc in enumerate(accs):\n",
    "    if acc > min_acc:\n",
    "        num_good_preds += 1\n",
    "        for f in feature_selections[i]:\n",
    "            frequency[f] += 1\n",
    "#frequency = [freq/num_good_preds for freq in frequency]\n",
    "print(\"\")\n",
    "print(\"number of classifiers: \",len(accs))\n",
    "print(\"number of accs >\",min_acc,\": \",sum([1 for acc in accs if acc > min_acc]))\n",
    "plt.figure()\n",
    "plt.bar(x=range(len(frequency)),height=frequency)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Model  0  out of  1024\n",
      "Model  1  out of  1024\n",
      "Model  2  out of  1024\n",
      "Model  3  out of  1024\n",
      "Model  4  out of  1024\n",
      "Model  5  out of  1024\n",
      "Model  6  out of  1024\n",
      "Model  7  out of  1024\n",
      "Model  8  out of  1024\n",
      "Model  9  out of  1024\n",
      "2\n",
      "Model  10  out of  1024\n",
      "Model  11  out of  1024\n",
      "Model  12  out of  1024\n",
      "Model  13  out of  1024\n",
      "Model  14  out of  1024\n",
      "Model  15  out of  1024\n",
      "Model  16  out of  1024\n",
      "Model  17  out of  1024\n",
      "Model  18  out of  1024\n",
      "Model  19  out of  1024\n",
      "Model  20  out of  1024\n",
      "Model  21  out of  1024\n",
      "Model  22  out of  1024\n",
      "Model  23  out of  1024\n",
      "Model  24  out of  1024\n",
      "Model  25  out of  1024\n",
      "Model  26  out of  1024\n",
      "Model  27  out of  1024\n",
      "Model  28  out of  1024\n",
      "Model  29  out of  1024\n",
      "Model  30  out of  1024\n",
      "Model  31  out of  1024\n",
      "Model  32  out of  1024\n",
      "Model  33  out of  1024\n",
      "Model  34  out of  1024\n",
      "Model  35  out of  1024\n",
      "Model  36  out of  1024\n",
      "Model  37  out of  1024\n",
      "Model  38  out of  1024\n",
      "Model  39  out of  1024\n",
      "Model  40  out of  1024\n",
      "Model  41  out of  1024\n",
      "Model  42  out of  1024\n",
      "Model  43  out of  1024\n",
      "Model  44  out of  1024\n",
      "Model  45  out of  1024\n",
      "Model  46  out of  1024\n",
      "Model  47  out of  1024\n",
      "Model  48  out of  1024\n",
      "Model  49  out of  1024\n",
      "Model  50  out of  1024\n",
      "Model  51  out of  1024\n",
      "Model  52  out of  1024\n",
      "Model  53  out of  1024\n",
      "Model  54  out of  1024\n",
      "3\n",
      "Model  55  out of  1024\n",
      "Model  56  out of  1024\n",
      "Model  57  out of  1024\n",
      "Model  58  out of  1024\n",
      "Model  59  out of  1024\n",
      "Model  60  out of  1024\n",
      "Model  61  out of  1024\n",
      "Model  62  out of  1024\n",
      "Model  63  out of  1024\n",
      "Model  64  out of  1024\n",
      "Model  65  out of  1024\n",
      "Model  66  out of  1024\n",
      "Model  67  out of  1024\n",
      "Model  68  out of  1024\n",
      "Model  69  out of  1024\n",
      "Model  70  out of  1024\n",
      "Model  71  out of  1024\n",
      "Model  72  out of  1024\n",
      "Model  73  out of  1024\n",
      "Model  74  out of  1024\n",
      "Model  75  out of  1024\n",
      "Model  76  out of  1024\n",
      "Model  77  out of  1024\n",
      "Model  78  out of  1024\n",
      "Model  79  out of  1024\n",
      "Model  80  out of  1024\n",
      "Model  81  out of  1024\n",
      "Model  82  out of  1024\n",
      "Model  83  out of  1024\n",
      "Model  84  out of  1024\n",
      "Model  85  out of  1024\n",
      "Model  86  out of  1024\n",
      "Model  87  out of  1024\n",
      "Model  88  out of  1024\n",
      "Model  89  out of  1024\n",
      "Model  90  out of  1024\n",
      "Model  91  out of  1024\n",
      "Model  92  out of  1024\n",
      "Model  93  out of  1024\n",
      "Model  94  out of  1024\n",
      "Model  95  out of  1024\n",
      "Model  96  out of  1024\n",
      "Model  97  out of  1024\n",
      "Model  98  out of  1024\n",
      "Model  99  out of  1024\n",
      "Model  100  out of  1024\n",
      "Model  101  out of  1024\n",
      "Model  102  out of  1024\n",
      "Model  103  out of  1024\n",
      "Model  104  out of  1024\n",
      "Model  105  out of  1024\n",
      "Model  106  out of  1024\n",
      "Model  107  out of  1024\n",
      "Model  108  out of  1024\n",
      "Model  109  out of  1024\n",
      "Model  110  out of  1024\n",
      "Model  111  out of  1024\n",
      "Model  112  out of  1024\n",
      "Model  113  out of  1024\n",
      "Model  114  out of  1024\n",
      "Model  115  out of  1024\n",
      "Model  116  out of  1024\n",
      "Model  117  out of  1024\n",
      "Model  118  out of  1024\n",
      "Model  119  out of  1024\n",
      "Model  120  out of  1024\n",
      "Model  121  out of  1024\n",
      "Model  122  out of  1024\n",
      "Model  123  out of  1024\n",
      "Model  124  out of  1024\n",
      "Model  125  out of  1024\n",
      "Model  126  out of  1024\n",
      "Model  127  out of  1024\n",
      "Model  128  out of  1024\n",
      "Model  129  out of  1024\n",
      "Model  130  out of  1024\n",
      "Model  131  out of  1024\n",
      "Model  132  out of  1024\n",
      "Model  133  out of  1024\n",
      "Model  134  out of  1024\n",
      "Model  135  out of  1024\n",
      "Model  136  out of  1024\n",
      "Model  137  out of  1024\n",
      "Model  138  out of  1024\n",
      "Model  139  out of  1024\n",
      "Model  140  out of  1024\n",
      "Model  141  out of  1024\n",
      "Model  142  out of  1024\n",
      "Model  143  out of  1024\n",
      "Model  144  out of  1024\n",
      "Model  145  out of  1024\n",
      "Model  146  out of  1024\n",
      "Model  147  out of  1024\n",
      "Model  148  out of  1024\n",
      "Model  149  out of  1024\n",
      "Model  150  out of  1024\n",
      "Model  151  out of  1024\n",
      "Model  152  out of  1024\n",
      "Model  153  out of  1024\n",
      "Model  154  out of  1024\n",
      "Model  155  out of  1024\n",
      "Model  156  out of  1024\n",
      "Model  157  out of  1024\n",
      "Model  158  out of  1024\n",
      "Model  159  out of  1024\n",
      "Model  160  out of  1024\n",
      "Model  161  out of  1024\n",
      "Model  162  out of  1024\n",
      "Model  163  out of  1024\n",
      "Model  164  out of  1024\n",
      "Model  165  out of  1024\n",
      "Model  166  out of  1024\n",
      "Model  167  out of  1024\n",
      "Model  168  out of  1024\n",
      "Model  169  out of  1024\n",
      "Model  170  out of  1024\n",
      "Model  171  out of  1024\n",
      "Model  172  out of  1024\n",
      "Model  173  out of  1024\n",
      "Model  174  out of  1024\n",
      "4\n",
      "Model  175  out of  1024\n",
      "Model  176  out of  1024\n",
      "Model  177  out of  1024\n",
      "Model  178  out of  1024\n",
      "Model  179  out of  1024\n",
      "Model  180  out of  1024\n",
      "Model  181  out of  1024\n",
      "Model  182  out of  1024\n",
      "Model  183  out of  1024\n",
      "Model  184  out of  1024\n",
      "Model  185  out of  1024\n",
      "Model  186  out of  1024\n",
      "Model  187  out of  1024\n",
      "Model  188  out of  1024\n",
      "Model  189  out of  1024\n",
      "Model  190  out of  1024\n",
      "Model  191  out of  1024\n",
      "Model  192  out of  1024\n",
      "Model  193  out of  1024\n",
      "Model  194  out of  1024\n",
      "Model  195  out of  1024\n",
      "Model  196  out of  1024\n",
      "Model  197  out of  1024\n",
      "Model  198  out of  1024\n",
      "Model  199  out of  1024\n",
      "Model  200  out of  1024\n",
      "Model  201  out of  1024\n",
      "Model  202  out of  1024\n",
      "Model  203  out of  1024\n",
      "Model  204  out of  1024\n",
      "Model  205  out of  1024\n",
      "Model  206  out of  1024\n",
      "Model  207  out of  1024\n",
      "Model  208  out of  1024\n",
      "Model  209  out of  1024\n",
      "Model  210  out of  1024\n",
      "Model  211  out of  1024\n",
      "Model  212  out of  1024\n",
      "Model  213  out of  1024\n",
      "Model  214  out of  1024\n",
      "Model  215  out of  1024\n",
      "Model  216  out of  1024\n",
      "Model  217  out of  1024\n",
      "Model  218  out of  1024\n",
      "Model  219  out of  1024\n",
      "Model  220  out of  1024\n",
      "Model  221  out of  1024\n",
      "Model  222  out of  1024\n",
      "Model  223  out of  1024\n",
      "Model  224  out of  1024\n",
      "Model  225  out of  1024\n",
      "Model  226  out of  1024\n",
      "Model  227  out of  1024\n",
      "Model  228  out of  1024\n",
      "Model  229  out of  1024\n",
      "Model  230  out of  1024\n",
      "Model  231  out of  1024\n",
      "Model  232  out of  1024\n",
      "Model  233  out of  1024\n",
      "Model  234  out of  1024\n",
      "Model  235  out of  1024\n",
      "Model  236  out of  1024\n",
      "Model  237  out of  1024\n",
      "Model  238  out of  1024\n",
      "Model  239  out of  1024\n",
      "Model  240  out of  1024\n",
      "Model  241  out of  1024\n",
      "Model  242  out of  1024\n",
      "Model  243  out of  1024\n",
      "Model  244  out of  1024\n",
      "Model  245  out of  1024\n",
      "Model  246  out of  1024\n",
      "Model  247  out of  1024\n",
      "Model  248  out of  1024\n",
      "Model  249  out of  1024\n",
      "Model  250  out of  1024\n",
      "Model  251  out of  1024\n",
      "Model  252  out of  1024\n",
      "Model  253  out of  1024\n",
      "Model  254  out of  1024\n",
      "Model  255  out of  1024\n",
      "Model  256  out of  1024\n",
      "Model  257  out of  1024\n",
      "Model  258  out of  1024\n",
      "Model  259  out of  1024\n",
      "Model  260  out of  1024\n",
      "Model  261  out of  1024\n",
      "Model  262  out of  1024\n",
      "Model  263  out of  1024\n",
      "Model  264  out of  1024\n",
      "Model  265  out of  1024\n",
      "Model  266  out of  1024\n",
      "Model  267  out of  1024\n",
      "Model  268  out of  1024\n",
      "Model  269  out of  1024\n",
      "Model  270  out of  1024\n",
      "Model  271  out of  1024\n",
      "Model  272  out of  1024\n",
      "Model  273  out of  1024\n",
      "Model  274  out of  1024\n",
      "Model  275  out of  1024\n",
      "Model  276  out of  1024\n",
      "Model  277  out of  1024\n",
      "Model  278  out of  1024\n",
      "Model  279  out of  1024\n",
      "Model  280  out of  1024\n",
      "Model  281  out of  1024\n",
      "Model  282  out of  1024\n",
      "Model  283  out of  1024\n",
      "Model  284  out of  1024\n",
      "Model  285  out of  1024\n",
      "Model  286  out of  1024\n",
      "Model  287  out of  1024\n",
      "Model  288  out of  1024\n",
      "Model  289  out of  1024\n",
      "Model  290  out of  1024\n",
      "Model  291  out of  1024\n",
      "Model  292  out of  1024\n",
      "Model  293  out of  1024\n",
      "Model  294  out of  1024\n",
      "Model  295  out of  1024\n",
      "Model  296  out of  1024\n",
      "Model  297  out of  1024\n",
      "Model  298  out of  1024\n",
      "Model  299  out of  1024\n",
      "Model  300  out of  1024\n",
      "Model  301  out of  1024\n",
      "Model  302  out of  1024\n",
      "Model  303  out of  1024\n",
      "Model  304  out of  1024\n",
      "Model  305  out of  1024\n",
      "Model  306  out of  1024\n",
      "Model  307  out of  1024\n",
      "Model  308  out of  1024\n",
      "Model  309  out of  1024\n",
      "Model  310  out of  1024\n",
      "Model  311  out of  1024\n",
      "Model  312  out of  1024\n",
      "Model  313  out of  1024\n",
      "Model  314  out of  1024\n",
      "Model  315  out of  1024\n",
      "Model  316  out of  1024\n",
      "Model  317  out of  1024\n",
      "Model  318  out of  1024\n",
      "Model  319  out of  1024\n",
      "Model  320  out of  1024\n",
      "Model  321  out of  1024\n",
      "Model  322  out of  1024\n",
      "Model  323  out of  1024\n",
      "Model  324  out of  1024\n",
      "Model  325  out of  1024\n",
      "Model  326  out of  1024\n",
      "Model  327  out of  1024\n",
      "Model  328  out of  1024\n",
      "Model  329  out of  1024\n",
      "Model  330  out of  1024\n",
      "Model  331  out of  1024\n",
      "Model  332  out of  1024\n",
      "Model  333  out of  1024\n",
      "Model  334  out of  1024\n",
      "Model  335  out of  1024\n",
      "Model  336  out of  1024\n",
      "Model  337  out of  1024\n",
      "Model  338  out of  1024\n",
      "Model  339  out of  1024\n",
      "Model  340  out of  1024\n",
      "Model  341  out of  1024\n",
      "Model  342  out of  1024\n",
      "Model  343  out of  1024\n",
      "Model  344  out of  1024\n",
      "Model  345  out of  1024\n",
      "Model  346  out of  1024\n",
      "Model  347  out of  1024\n",
      "Model  348  out of  1024\n",
      "Model  349  out of  1024\n",
      "Model  350  out of  1024\n",
      "Model  351  out of  1024\n",
      "Model  352  out of  1024\n",
      "Model  353  out of  1024\n",
      "Model  354  out of  1024\n",
      "Model  355  out of  1024\n",
      "Model  356  out of  1024\n",
      "Model  357  out of  1024\n",
      "Model  358  out of  1024\n",
      "Model  359  out of  1024\n",
      "Model  360  out of  1024\n",
      "Model  361  out of  1024\n",
      "Model  362  out of  1024\n",
      "Model  363  out of  1024\n",
      "Model  364  out of  1024\n",
      "Model  365  out of  1024\n",
      "Model  366  out of  1024\n",
      "Model  367  out of  1024\n",
      "Model  368  out of  1024\n",
      "Model  369  out of  1024\n",
      "Model  370  out of  1024\n",
      "Model  371  out of  1024\n",
      "Model  372  out of  1024\n",
      "Model  373  out of  1024\n",
      "Model  374  out of  1024\n",
      "Model  375  out of  1024\n",
      "Model  376  out of  1024\n",
      "Model  377  out of  1024\n",
      "Model  378  out of  1024\n",
      "Model  379  out of  1024\n",
      "Model  380  out of  1024\n",
      "Model  381  out of  1024\n",
      "Model  382  out of  1024\n",
      "Model  383  out of  1024\n",
      "Model  384  out of  1024\n",
      "5\n",
      "Model  385  out of  1024\n",
      "Model  386  out of  1024\n",
      "Model  387  out of  1024\n",
      "Model  388  out of  1024\n",
      "Model  389  out of  1024\n",
      "Model  390  out of  1024\n",
      "Model  391  out of  1024\n",
      "Model  392  out of  1024\n",
      "Model  393  out of  1024\n",
      "Model  394  out of  1024\n",
      "Model  395  out of  1024\n",
      "Model  396  out of  1024\n",
      "Model  397  out of  1024\n",
      "Model  398  out of  1024\n",
      "Model  399  out of  1024\n",
      "Model  400  out of  1024\n",
      "Model  401  out of  1024\n",
      "Model  402  out of  1024\n",
      "Model  403  out of  1024\n",
      "Model  404  out of  1024\n",
      "Model  405  out of  1024\n",
      "Model  406  out of  1024\n",
      "Model  407  out of  1024\n",
      "Model  408  out of  1024\n",
      "Model  409  out of  1024\n",
      "Model  410  out of  1024\n",
      "Model  411  out of  1024\n",
      "Model  412  out of  1024\n",
      "Model  413  out of  1024\n",
      "Model  414  out of  1024\n",
      "Model  415  out of  1024\n",
      "Model  416  out of  1024\n",
      "Model  417  out of  1024\n",
      "Model  418  out of  1024\n",
      "Model  419  out of  1024\n",
      "Model  420  out of  1024\n",
      "Model  421  out of  1024\n",
      "Model  422  out of  1024\n",
      "Model  423  out of  1024\n",
      "Model  424  out of  1024\n",
      "Model  425  out of  1024\n",
      "Model  426  out of  1024\n",
      "Model  427  out of  1024\n",
      "Model  428  out of  1024\n",
      "Model  429  out of  1024\n",
      "Model  430  out of  1024\n",
      "Model  431  out of  1024\n",
      "Model  432  out of  1024\n",
      "Model  433  out of  1024\n",
      "Model  434  out of  1024\n",
      "Model  435  out of  1024\n",
      "Model  436  out of  1024\n",
      "Model  437  out of  1024\n",
      "Model  438  out of  1024\n",
      "Model  439  out of  1024\n",
      "Model  440  out of  1024\n",
      "Model  441  out of  1024\n",
      "Model  442  out of  1024\n",
      "Model  443  out of  1024\n",
      "Model  444  out of  1024\n",
      "Model  445  out of  1024\n",
      "Model  446  out of  1024\n",
      "Model  447  out of  1024\n",
      "Model  448  out of  1024\n",
      "Model  449  out of  1024\n",
      "Model  450  out of  1024\n",
      "Model  451  out of  1024\n",
      "Model  452  out of  1024\n",
      "Model  453  out of  1024\n",
      "Model  454  out of  1024\n",
      "Model  455  out of  1024\n",
      "Model  456  out of  1024\n",
      "Model  457  out of  1024\n",
      "Model  458  out of  1024\n",
      "Model  459  out of  1024\n",
      "Model  460  out of  1024\n",
      "Model  461  out of  1024\n",
      "Model  462  out of  1024\n",
      "Model  463  out of  1024\n",
      "Model  464  out of  1024\n",
      "Model  465  out of  1024\n",
      "Model  466  out of  1024\n",
      "Model  467  out of  1024\n",
      "Model  468  out of  1024\n",
      "Model  469  out of  1024\n",
      "Model  470  out of  1024\n",
      "Model  471  out of  1024\n",
      "Model  472  out of  1024\n",
      "Model  473  out of  1024\n",
      "Model  474  out of  1024\n",
      "Model  475  out of  1024\n",
      "Model  476  out of  1024\n",
      "Model  477  out of  1024\n",
      "Model  478  out of  1024\n",
      "Model  479  out of  1024\n",
      "Model  480  out of  1024\n",
      "Model  481  out of  1024\n",
      "Model  482  out of  1024\n",
      "Model  483  out of  1024\n",
      "Model  484  out of  1024\n",
      "Model  485  out of  1024\n",
      "Model  486  out of  1024\n",
      "Model  487  out of  1024\n",
      "Model  488  out of  1024\n",
      "Model  489  out of  1024\n",
      "Model  490  out of  1024\n",
      "Model  491  out of  1024\n",
      "Model  492  out of  1024\n",
      "Model  493  out of  1024\n",
      "Model  494  out of  1024\n",
      "Model  495  out of  1024\n",
      "Model  496  out of  1024\n",
      "Model  497  out of  1024\n",
      "Model  498  out of  1024\n",
      "Model  499  out of  1024\n",
      "Model  500  out of  1024\n",
      "Model  501  out of  1024\n",
      "Model  502  out of  1024\n",
      "Model  503  out of  1024\n",
      "Model  504  out of  1024\n",
      "Model  505  out of  1024\n",
      "Model  506  out of  1024\n",
      "Model  507  out of  1024\n",
      "Model  508  out of  1024\n",
      "Model  509  out of  1024\n",
      "Model  510  out of  1024\n",
      "Model  511  out of  1024\n",
      "Model  512  out of  1024\n",
      "Model  513  out of  1024\n",
      "Model  514  out of  1024\n",
      "Model  515  out of  1024\n",
      "Model  516  out of  1024\n",
      "Model  517  out of  1024\n",
      "Model  518  out of  1024\n",
      "Model  519  out of  1024\n",
      "Model  520  out of  1024\n",
      "Model  521  out of  1024\n",
      "Model  522  out of  1024\n",
      "Model  523  out of  1024\n",
      "Model  524  out of  1024\n",
      "Model  525  out of  1024\n",
      "Model  526  out of  1024\n",
      "Model  527  out of  1024\n",
      "Model  528  out of  1024\n",
      "Model  529  out of  1024\n",
      "Model  530  out of  1024\n",
      "Model  531  out of  1024\n",
      "Model  532  out of  1024\n",
      "Model  533  out of  1024\n",
      "Model  534  out of  1024\n",
      "Model  535  out of  1024\n",
      "Model  536  out of  1024\n",
      "Model  537  out of  1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  538  out of  1024\n",
      "Model  539  out of  1024\n",
      "Model  540  out of  1024\n",
      "Model  541  out of  1024\n",
      "Model  542  out of  1024\n",
      "Model  543  out of  1024\n",
      "Model  544  out of  1024\n",
      "Model  545  out of  1024\n",
      "Model  546  out of  1024\n",
      "Model  547  out of  1024\n",
      "Model  548  out of  1024\n",
      "Model  549  out of  1024\n",
      "Model  550  out of  1024\n",
      "Model  551  out of  1024\n",
      "Model  552  out of  1024\n",
      "Model  553  out of  1024\n",
      "Model  554  out of  1024\n",
      "Model  555  out of  1024\n",
      "Model  556  out of  1024\n",
      "Model  557  out of  1024\n",
      "Model  558  out of  1024\n",
      "Model  559  out of  1024\n",
      "Model  560  out of  1024\n",
      "Model  561  out of  1024\n",
      "Model  562  out of  1024\n",
      "Model  563  out of  1024\n",
      "Model  564  out of  1024\n",
      "Model  565  out of  1024\n",
      "Model  566  out of  1024\n",
      "Model  567  out of  1024\n",
      "Model  568  out of  1024\n",
      "Model  569  out of  1024\n",
      "Model  570  out of  1024\n",
      "Model  571  out of  1024\n",
      "Model  572  out of  1024\n",
      "Model  573  out of  1024\n",
      "Model  574  out of  1024\n",
      "Model  575  out of  1024\n",
      "Model  576  out of  1024\n",
      "Model  577  out of  1024\n",
      "Model  578  out of  1024\n",
      "Model  579  out of  1024\n",
      "Model  580  out of  1024\n",
      "Model  581  out of  1024\n",
      "Model  582  out of  1024\n",
      "Model  583  out of  1024\n",
      "Model  584  out of  1024\n",
      "Model  585  out of  1024\n",
      "Model  586  out of  1024\n",
      "Model  587  out of  1024\n",
      "Model  588  out of  1024\n",
      "Model  589  out of  1024\n",
      "Model  590  out of  1024\n",
      "Model  591  out of  1024\n",
      "Model  592  out of  1024\n",
      "Model  593  out of  1024\n",
      "Model  594  out of  1024\n",
      "Model  595  out of  1024\n",
      "Model  596  out of  1024\n",
      "Model  597  out of  1024\n",
      "Model  598  out of  1024\n",
      "Model  599  out of  1024\n",
      "Model  600  out of  1024\n",
      "Model  601  out of  1024\n",
      "Model  602  out of  1024\n",
      "Model  603  out of  1024\n",
      "Model  604  out of  1024\n",
      "Model  605  out of  1024\n",
      "Model  606  out of  1024\n",
      "Model  607  out of  1024\n",
      "Model  608  out of  1024\n",
      "Model  609  out of  1024\n",
      "Model  610  out of  1024\n",
      "Model  611  out of  1024\n",
      "Model  612  out of  1024\n",
      "Model  613  out of  1024\n",
      "Model  614  out of  1024\n",
      "Model  615  out of  1024\n",
      "Model  616  out of  1024\n",
      "Model  617  out of  1024\n",
      "Model  618  out of  1024\n",
      "Model  619  out of  1024\n",
      "Model  620  out of  1024\n",
      "Model  621  out of  1024\n",
      "Model  622  out of  1024\n",
      "Model  623  out of  1024\n",
      "Model  624  out of  1024\n",
      "Model  625  out of  1024\n",
      "Model  626  out of  1024\n",
      "Model  627  out of  1024\n",
      "Model  628  out of  1024\n",
      "Model  629  out of  1024\n",
      "Model  630  out of  1024\n",
      "Model  631  out of  1024\n",
      "Model  632  out of  1024\n",
      "Model  633  out of  1024\n",
      "Model  634  out of  1024\n",
      "Model  635  out of  1024\n",
      "Model  636  out of  1024\n",
      "6\n",
      "Model  637  out of  1024\n",
      "Model  638  out of  1024\n",
      "Model  639  out of  1024\n",
      "Model  640  out of  1024\n",
      "Model  641  out of  1024\n",
      "Model  642  out of  1024\n",
      "Model  643  out of  1024\n",
      "Model  644  out of  1024\n",
      "Model  645  out of  1024\n",
      "Model  646  out of  1024\n",
      "Model  647  out of  1024\n",
      "Model  648  out of  1024\n",
      "Model  649  out of  1024\n",
      "Model  650  out of  1024\n",
      "Model  651  out of  1024\n",
      "Model  652  out of  1024\n",
      "Model  653  out of  1024\n",
      "Model  654  out of  1024\n",
      "Model  655  out of  1024\n",
      "Model  656  out of  1024\n",
      "Model  657  out of  1024\n",
      "Model  658  out of  1024\n",
      "Model  659  out of  1024\n",
      "Model  660  out of  1024\n",
      "Model  661  out of  1024\n",
      "Model  662  out of  1024\n",
      "Model  663  out of  1024\n",
      "Model  664  out of  1024\n",
      "Model  665  out of  1024\n",
      "Model  666  out of  1024\n",
      "Model  667  out of  1024\n",
      "Model  668  out of  1024\n",
      "Model  669  out of  1024\n",
      "Model  670  out of  1024\n",
      "Model  671  out of  1024\n",
      "Model  672  out of  1024\n",
      "Model  673  out of  1024\n",
      "Model  674  out of  1024\n",
      "Model  675  out of  1024\n",
      "Model  676  out of  1024\n",
      "Model  677  out of  1024\n",
      "Model  678  out of  1024\n",
      "Model  679  out of  1024\n",
      "Model  680  out of  1024\n",
      "Model  681  out of  1024\n",
      "Model  682  out of  1024\n",
      "Model  683  out of  1024\n",
      "Model  684  out of  1024\n",
      "Model  685  out of  1024\n",
      "Model  686  out of  1024\n",
      "Model  687  out of  1024\n",
      "Model  688  out of  1024\n",
      "Model  689  out of  1024\n",
      "Model  690  out of  1024\n",
      "Model  691  out of  1024\n",
      "Model  692  out of  1024\n",
      "Model  693  out of  1024\n",
      "Model  694  out of  1024\n",
      "Model  695  out of  1024\n",
      "Model  696  out of  1024\n",
      "Model  697  out of  1024\n",
      "Model  698  out of  1024\n",
      "Model  699  out of  1024\n",
      "Model  700  out of  1024\n",
      "Model  701  out of  1024\n",
      "Model  702  out of  1024\n",
      "Model  703  out of  1024\n",
      "Model  704  out of  1024\n",
      "Model  705  out of  1024\n",
      "Model  706  out of  1024\n",
      "Model  707  out of  1024\n",
      "Model  708  out of  1024\n",
      "Model  709  out of  1024\n",
      "Model  710  out of  1024\n",
      "Model  711  out of  1024\n",
      "Model  712  out of  1024\n",
      "Model  713  out of  1024\n",
      "Model  714  out of  1024\n",
      "Model  715  out of  1024\n",
      "Model  716  out of  1024\n",
      "Model  717  out of  1024\n",
      "Model  718  out of  1024\n",
      "Model  719  out of  1024\n",
      "Model  720  out of  1024\n",
      "Model  721  out of  1024\n",
      "Model  722  out of  1024\n",
      "Model  723  out of  1024\n",
      "Model  724  out of  1024\n",
      "Model  725  out of  1024\n",
      "Model  726  out of  1024\n",
      "Model  727  out of  1024\n",
      "Model  728  out of  1024\n",
      "Model  729  out of  1024\n",
      "Model  730  out of  1024\n",
      "Model  731  out of  1024\n",
      "Model  732  out of  1024\n",
      "Model  733  out of  1024\n",
      "Model  734  out of  1024\n",
      "Model  735  out of  1024\n",
      "Model  736  out of  1024\n",
      "Model  737  out of  1024\n",
      "Model  738  out of  1024\n",
      "Model  739  out of  1024\n",
      "Model  740  out of  1024\n",
      "Model  741  out of  1024\n",
      "Model  742  out of  1024\n",
      "Model  743  out of  1024\n",
      "Model  744  out of  1024\n",
      "Model  745  out of  1024\n",
      "Model  746  out of  1024\n",
      "Model  747  out of  1024\n",
      "Model  748  out of  1024\n",
      "Model  749  out of  1024\n",
      "Model  750  out of  1024\n",
      "Model  751  out of  1024\n",
      "Model  752  out of  1024\n",
      "Model  753  out of  1024\n",
      "Model  754  out of  1024\n",
      "Model  755  out of  1024\n",
      "Model  756  out of  1024\n",
      "Model  757  out of  1024\n",
      "Model  758  out of  1024\n",
      "Model  759  out of  1024\n",
      "Model  760  out of  1024\n",
      "Model  761  out of  1024\n",
      "Model  762  out of  1024\n",
      "Model  763  out of  1024\n",
      "Model  764  out of  1024\n",
      "Model  765  out of  1024\n",
      "Model  766  out of  1024\n",
      "Model  767  out of  1024\n",
      "Model  768  out of  1024\n",
      "Model  769  out of  1024\n",
      "Model  770  out of  1024\n",
      "Model  771  out of  1024\n",
      "Model  772  out of  1024\n",
      "Model  773  out of  1024\n",
      "Model  774  out of  1024\n",
      "Model  775  out of  1024\n",
      "Model  776  out of  1024\n",
      "Model  777  out of  1024\n",
      "Model  778  out of  1024\n",
      "Model  779  out of  1024\n",
      "Model  780  out of  1024\n",
      "Model  781  out of  1024\n",
      "Model  782  out of  1024\n",
      "Model  783  out of  1024\n",
      "Model  784  out of  1024\n",
      "Model  785  out of  1024\n",
      "Model  786  out of  1024\n",
      "Model  787  out of  1024\n",
      "Model  788  out of  1024\n",
      "Model  789  out of  1024\n",
      "Model  790  out of  1024\n",
      "Model  791  out of  1024\n",
      "Model  792  out of  1024\n",
      "Model  793  out of  1024\n",
      "Model  794  out of  1024\n",
      "Model  795  out of  1024\n",
      "Model  796  out of  1024\n",
      "Model  797  out of  1024\n",
      "Model  798  out of  1024\n",
      "Model  799  out of  1024\n",
      "Model  800  out of  1024\n",
      "Model  801  out of  1024\n",
      "Model  802  out of  1024\n",
      "Model  803  out of  1024\n",
      "Model  804  out of  1024\n",
      "Model  805  out of  1024\n",
      "Model  806  out of  1024\n",
      "Model  807  out of  1024\n",
      "Model  808  out of  1024\n",
      "Model  809  out of  1024\n",
      "Model  810  out of  1024\n",
      "Model  811  out of  1024\n",
      "Model  812  out of  1024\n",
      "Model  813  out of  1024\n",
      "Model  814  out of  1024\n",
      "Model  815  out of  1024\n",
      "Model  816  out of  1024\n",
      "Model  817  out of  1024\n",
      "Model  818  out of  1024\n",
      "Model  819  out of  1024\n",
      "Model  820  out of  1024\n",
      "Model  821  out of  1024\n",
      "Model  822  out of  1024\n",
      "Model  823  out of  1024\n",
      "Model  824  out of  1024\n",
      "Model  825  out of  1024\n",
      "Model  826  out of  1024\n",
      "Model  827  out of  1024\n",
      "Model  828  out of  1024\n",
      "Model  829  out of  1024\n",
      "Model  830  out of  1024\n",
      "Model  831  out of  1024\n",
      "Model  832  out of  1024\n",
      "Model  833  out of  1024\n",
      "Model  834  out of  1024\n",
      "Model  835  out of  1024\n",
      "Model  836  out of  1024\n",
      "Model  837  out of  1024\n",
      "Model  838  out of  1024\n",
      "Model  839  out of  1024\n",
      "Model  840  out of  1024\n",
      "Model  841  out of  1024\n",
      "Model  842  out of  1024\n",
      "Model  843  out of  1024\n",
      "Model  844  out of  1024\n",
      "Model  845  out of  1024\n",
      "Model  846  out of  1024\n",
      "7\n",
      "Model  847  out of  1024\n",
      "Model  848  out of  1024\n",
      "Model  849  out of  1024\n",
      "Model  850  out of  1024\n",
      "Model  851  out of  1024\n",
      "Model  852  out of  1024\n",
      "Model  853  out of  1024\n",
      "Model  854  out of  1024\n",
      "Model  855  out of  1024\n",
      "Model  856  out of  1024\n",
      "Model  857  out of  1024\n",
      "Model  858  out of  1024\n",
      "Model  859  out of  1024\n",
      "Model  860  out of  1024\n",
      "Model  861  out of  1024\n",
      "Model  862  out of  1024\n",
      "Model  863  out of  1024\n",
      "Model  864  out of  1024\n",
      "Model  865  out of  1024\n",
      "Model  866  out of  1024\n",
      "Model  867  out of  1024\n",
      "Model  868  out of  1024\n",
      "Model  869  out of  1024\n",
      "Model  870  out of  1024\n",
      "Model  871  out of  1024\n",
      "Model  872  out of  1024\n",
      "Model  873  out of  1024\n",
      "Model  874  out of  1024\n",
      "Model  875  out of  1024\n",
      "Model  876  out of  1024\n",
      "Model  877  out of  1024\n",
      "Model  878  out of  1024\n",
      "Model  879  out of  1024\n",
      "Model  880  out of  1024\n",
      "Model  881  out of  1024\n",
      "Model  882  out of  1024\n",
      "Model  883  out of  1024\n",
      "Model  884  out of  1024\n",
      "Model  885  out of  1024\n",
      "Model  886  out of  1024\n",
      "Model  887  out of  1024\n",
      "Model  888  out of  1024\n",
      "Model  889  out of  1024\n",
      "Model  890  out of  1024\n",
      "Model  891  out of  1024\n",
      "Model  892  out of  1024\n",
      "Model  893  out of  1024\n",
      "Model  894  out of  1024\n",
      "Model  895  out of  1024\n",
      "Model  896  out of  1024\n",
      "Model  897  out of  1024\n",
      "Model  898  out of  1024\n",
      "Model  899  out of  1024\n",
      "Model  900  out of  1024\n",
      "Model  901  out of  1024\n",
      "Model  902  out of  1024\n",
      "Model  903  out of  1024\n",
      "Model  904  out of  1024\n",
      "Model  905  out of  1024\n",
      "Model  906  out of  1024\n",
      "Model  907  out of  1024\n",
      "Model  908  out of  1024\n",
      "Model  909  out of  1024\n",
      "Model  910  out of  1024\n",
      "Model  911  out of  1024\n",
      "Model  912  out of  1024\n",
      "Model  913  out of  1024\n",
      "Model  914  out of  1024\n",
      "Model  915  out of  1024\n",
      "Model  916  out of  1024\n",
      "Model  917  out of  1024\n",
      "Model  918  out of  1024\n",
      "Model  919  out of  1024\n",
      "Model  920  out of  1024\n",
      "Model  921  out of  1024\n",
      "Model  922  out of  1024\n",
      "Model  923  out of  1024\n",
      "Model  924  out of  1024\n",
      "Model  925  out of  1024\n",
      "Model  926  out of  1024\n",
      "Model  927  out of  1024\n",
      "Model  928  out of  1024\n",
      "Model  929  out of  1024\n",
      "Model  930  out of  1024\n",
      "Model  931  out of  1024\n",
      "Model  932  out of  1024\n",
      "Model  933  out of  1024\n",
      "Model  934  out of  1024\n",
      "Model  935  out of  1024\n",
      "Model  936  out of  1024\n",
      "Model  937  out of  1024\n",
      "Model  938  out of  1024\n",
      "Model  939  out of  1024\n",
      "Model  940  out of  1024\n",
      "Model  941  out of  1024\n",
      "Model  942  out of  1024\n",
      "Model  943  out of  1024\n",
      "Model  944  out of  1024\n",
      "Model  945  out of  1024\n",
      "Model  946  out of  1024\n",
      "Model  947  out of  1024\n",
      "Model  948  out of  1024\n",
      "Model  949  out of  1024\n",
      "Model  950  out of  1024\n",
      "Model  951  out of  1024\n",
      "Model  952  out of  1024\n",
      "Model  953  out of  1024\n",
      "Model  954  out of  1024\n",
      "Model  955  out of  1024\n",
      "Model  956  out of  1024\n",
      "Model  957  out of  1024\n",
      "Model  958  out of  1024\n",
      "Model  959  out of  1024\n",
      "Model  960  out of  1024\n",
      "Model  961  out of  1024\n",
      "Model  962  out of  1024\n",
      "Model  963  out of  1024\n",
      "Model  964  out of  1024\n",
      "Model  965  out of  1024\n",
      "Model  966  out of  1024\n",
      "8\n",
      "Model  967  out of  1024\n",
      "Model  968  out of  1024\n",
      "Model  969  out of  1024\n",
      "Model  970  out of  1024\n",
      "Model  971  out of  1024\n",
      "Model  972  out of  1024\n",
      "Model  973  out of  1024\n",
      "Model  974  out of  1024\n",
      "Model  975  out of  1024\n",
      "Model  976  out of  1024\n",
      "Model  977  out of  1024\n",
      "Model  978  out of  1024\n",
      "Model  979  out of  1024\n",
      "Model  980  out of  1024\n",
      "Model  981  out of  1024\n",
      "Model  982  out of  1024\n",
      "Model  983  out of  1024\n",
      "Model  984  out of  1024\n",
      "Model  985  out of  1024\n",
      "Model  986  out of  1024\n",
      "Model  987  out of  1024\n",
      "Model  988  out of  1024\n",
      "Model  989  out of  1024\n",
      "Model  990  out of  1024\n",
      "Model  991  out of  1024\n",
      "Model  992  out of  1024\n",
      "Model  993  out of  1024\n",
      "Model  994  out of  1024\n",
      "Model  995  out of  1024\n",
      "Model  996  out of  1024\n",
      "Model  997  out of  1024\n",
      "Model  998  out of  1024\n",
      "Model  999  out of  1024\n",
      "Model  1000  out of  1024\n",
      "Model  1001  out of  1024\n",
      "Model  1002  out of  1024\n",
      "Model  1003  out of  1024\n",
      "Model  1004  out of  1024\n",
      "Model  1005  out of  1024\n",
      "Model  1006  out of  1024\n",
      "Model  1007  out of  1024\n",
      "Model  1008  out of  1024\n",
      "Model  1009  out of  1024\n",
      "Model  1010  out of  1024\n",
      "Model  1011  out of  1024\n",
      "9\n",
      "Model  1012  out of  1024\n",
      "Model  1013  out of  1024\n",
      "Model  1014  out of  1024\n",
      "Model  1015  out of  1024\n",
      "Model  1016  out of  1024\n",
      "Model  1017  out of  1024\n",
      "Model  1018  out of  1024\n",
      "Model  1019  out of  1024\n",
      "Model  1020  out of  1024\n",
      "Model  1021  out of  1024\n",
      "10\n",
      "Model  1022  out of  1024\n"
     ]
    }
   ],
   "source": [
    "# Select random features and train a decision tree on it\n",
    "from time import clock\n",
    "import itertools\n",
    "\n",
    "random.seed(0)\n",
    "total_num_features = train_features_reduced.shape[1]\n",
    "selection = [0,1,2,3,4,14,16,18,19,20]\n",
    "time_limit = 600\n",
    "start_time = clock()\n",
    "feature_selections = []\n",
    "#accs = []\n",
    "#f1s = []\n",
    "model_number = 0\n",
    "for r in range(len(selection)+1):\n",
    "    if r == 0:\n",
    "        continue\n",
    "    print(r)\n",
    "    for c in itertools.combinations(selection,r):\n",
    "        print(\"Model \",model_number,\" out of \",2**len(selection))\n",
    "        model_number += 1\n",
    "        sub_selection = list(c)\n",
    "        #classifier = svm.LinearSVC()\n",
    "        #classifier.fit(train_features_reduced[:,sub_selection], train_true_labels_reduced)\n",
    "        #preds = list(classifier.predict(test_features[:,sub_selection]))\n",
    "\n",
    "        #acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "        #f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "        #accs.append(acc)\n",
    "        #f1s.append(f1)\n",
    "        feature_selections.append(sub_selection)\n",
    "        if acc == max(accs):\n",
    "            print('acc:',acc,'f1:',f1)\n",
    "            print('features: ',sub_selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9456082745033656,\n",
       " 0.9460187161385651,\n",
       " 0.9459694631423412,\n",
       " 0.9457396158266295,\n",
       " 0.9459037924807092,\n",
       " 0.9457067804958135,\n",
       " 0.9459530454769332,\n",
       " 0.9453948448530619,\n",
       " 0.9458052864882613,\n",
       " 0.9459858808077491]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9564110983418158 f1: 0.9593433686048114\n"
     ]
    }
   ],
   "source": [
    "# Adaboost DecisionTrees\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "selection = [1, 2, 4, 11, 14, 18, 20, 21]\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=500,learning_rate=0.1)\n",
    "ada.fit(train_features_reduced[:,selection],train_true_labels_reduced)\n",
    "preds_ada = ada.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_ada)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_ada)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9213429650303727 f1: 0.9255558835868671\n"
     ]
    }
   ],
   "source": [
    "# Adaboost LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "selection = [21, 1, 2, 6, 11, 14, 18, 20]\n",
    "ada = AdaBoostClassifier(lr(),n_estimators=500,learning_rate=1)\n",
    "ada.fit(train_features_reduced[:,selection],train_true_labels_reduced)\n",
    "preds_ada = ada.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_ada)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_ada)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9552782794286653 f1: 0.958655860122029\n"
     ]
    }
   ],
   "source": [
    "# ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "selection = [1, 2, 4, 11, 14, 18, 20, 21]\n",
    "extraTrees = ExtraTreesClassifier(n_estimators=100)\n",
    "extraTrees.fit(train_features_reduced[:,selection],train_true_labels_reduced)\n",
    "preds_extra = extraTrees.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_extra)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_extra)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "class Class_Net():\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, batch_size=60):\n",
    "        ''' initialize the classifier with default (best) parameters '''\n",
    "        tf.reset_default_graph()\n",
    "        self.alpha = learning_rate\n",
    "        self.beta = batch_size\n",
    "        self.warm = False\n",
    "\n",
    "    def fit(self,X,Y,warm_start=True,n_epochs=50):\n",
    "        ''' train the network, and if warm_start, then do not reinit. the network\n",
    "            (if it has already been initialized)\n",
    "        '''\n",
    "        self.epochs=n_epochs\n",
    "\n",
    "        self.n_batch = int(len(X)/self.beta)\n",
    "        \n",
    "        if warm_start==False or self.warm==False:\n",
    "            self.x = tf.placeholder(tf.float32,shape=[None,len(X[0])])\n",
    "            self.y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "            \n",
    "            self.wZero = tf.get_variable('wZero',shape=[len(X[0]),50],initializer=tf.glorot_uniform_initializer())\n",
    "            self.bZero = tf.Variable(tf.zeros([50]))\n",
    "\n",
    "            self.wOne = tf.get_variable('wOne',shape=[50,1],initializer=tf.glorot_uniform_initializer())\n",
    "            self.bOne = tf.Variable(tf.zeros([1]))\n",
    "            self.keep_prob = 0.9\n",
    "            self.drop_out = tf.nn.dropout(self.x, self.keep_prob)\n",
    "            self.model = tf.nn.sigmoid(tf.matmul(tf.nn.relu(tf.matmul(self.drop_out, self.wZero) \n",
    "                                              + self.bZero),self.wOne)+self.bOne)\n",
    "            self.cost = tf.losses.log_loss(self.y,self.model)\n",
    "            \n",
    "#             self.optimizer = tf.train.GradientDescentOptimizer(self.alpha).minimize(self.cost)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.alpha).minimize(self.cost)\n",
    "\n",
    "            #without dropout\n",
    "#             self.model = tf.matmul(tf.nn.relu(tf.matmul(self.x, self.wZero) + self.bZero),self.wOne)+self.bOne\n",
    "\n",
    "    \n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        with tf.Session() as sess:\n",
    "            if warm_start==False or self.warm==False:\n",
    "                sess.run(self.init)\n",
    "            else:\n",
    "                self.saver.restore(sess, './tempVariables.ckpt')\n",
    "            for epoch in range(self.epochs):\n",
    "                self.avg_cost = 0\n",
    "                for i in range(self.n_batch):\n",
    "                    _, self.c = sess.run([self.optimizer,self.cost], feed_dict={self.x: X[i*self.beta:min([(i+1)*\n",
    "                                    self.beta,len(X)]),:],self.y:Y[i*self.beta:min([(i+1)*self.beta,len(X)])]})\n",
    "                    \n",
    "                    self.avg_cost = self.avg_cost+np.mean(self.c)/self.n_batch\n",
    "                print(\"Epoch:\", '%s' % (epoch+1), \"cost=\", \"%s\"% (self.avg_cost))\n",
    "            self.saver.save(sess,'./tempVariables.ckpt')\n",
    "            \n",
    "        self.warm = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        ''' return a matrix P where P[i,j] = P(Y[i,j]=1), \n",
    "        for all instances i, and labels j. '''\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, './tempVariables.ckpt')\n",
    "            self.preds = sess.run(tf.nn.softmax(self.model), feed_dict={self.x: X}) \n",
    "        return self.preds\n",
    "    \n",
    "    def predict(self,X):\n",
    "        ''' return a matrix of predictions for X '''\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Class_Net(learning_rate=0.01,batch_size=250)\n",
    "net.fit(features,list(map(lambda x: [x],true_labels)),n_epochs=35)\n",
    "preds=net.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
