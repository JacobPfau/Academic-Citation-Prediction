{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "n_samples: 27770, n_features: 10000\n",
      "Performing dimensionality reduction using LSA\n",
      "Explained variance of the SVD step: 19%\n"
     ]
    }
   ],
   "source": [
    "import features_nodewise as nw\n",
    "import features_pairwise as pw\n",
    "import preprocessing as prep\n",
    "\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "\n",
    "## Read train / test node pairs\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "with open(\"./data/train_train_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "with open(\"./data/train_test_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "random.seed(0)\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "to_keep_train = random.sample( range(len(training_set)),k=int(round(len(training_set)*0.05)) )\n",
    "training_set_reduced = [training_set[i] for i in to_keep_train]\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "to_keep_test = random.sample( range(len(testing_set)),k=int(round(len(testing_set)*1)) )\n",
    "testing_set_reduced = [testing_set[i] for i in to_keep_test]\n",
    "\n",
    "with open(\"./data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "corpus = [element[5] for element in node_info]\n",
    "t = prep.tfidf(corpus)\n",
    "l = nw.LSA(t)\n",
    "IDs = [element[0] for element in node_info]\n",
    "node_dict = prep.to_dict( [element[0] for element in node_info],range(len(node_info)) )\n",
    "index_dict = prep.to_dict(range(len(IDs)),IDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graphs (train vs test), (reduced vs full)\n",
    "\n",
    "train_IDs = set([training_set[i][0] for i in range(len(training_set))])\n",
    "train_IDs = train_IDs | set([training_set[i][1] for i in range(len(training_set))])\n",
    "train_IDs = list(train_IDs) #igraph doesn't like sets...\n",
    "train_edges = [(element[0],element[1]) for element in training_set]\n",
    "train_graph = prep.article_graph(train_IDs,train_edges)\n",
    "\n",
    "train_IDs_reduced = set([training_set_reduced[i][0] for i in range(len(training_set_reduced))])\n",
    "train_IDs_reduced = train_IDs_reduced | set([training_set_reduced[i][1] for i in range(len(training_set_reduced))])\n",
    "train_IDs_reduced = list(train_IDs_reduced)\n",
    "train_edges_reduced = [(element[0],element[1]) for element in training_set_reduced]\n",
    "train_graph_reduced = prep.article_graph(train_IDs_reduced,train_edges_reduced)\n",
    "\n",
    "test_IDs = set([testing_set[i][0] for i in range(len(testing_set))])\n",
    "test_IDs = test_IDs | set([testing_set[i][1] for i in range(len(testing_set))])\n",
    "test_IDs = list(test_IDs)\n",
    "test_edges = [(element[0],element[1]) for element in testing_set]\n",
    "test_graph = prep.article_graph(test_IDs,test_edges)\n",
    "\n",
    "test_IDs_reduced = set([testing_set_reduced[i][0] for i in range(len(testing_set_reduced))])\n",
    "test_IDs_reduced = test_IDs_reduced | set([testing_set_reduced[i][1] for i in range(len(testing_set_reduced))])\n",
    "test_IDs_reduced = list(test_IDs_reduced)\n",
    "test_edges_reduced = [(element[0],element[1]) for element in testing_set_reduced]\n",
    "test_graph_reduced = prep.article_graph(test_IDs_reduced,test_edges_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9507159'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read all the features that we have stored in files\n",
    "import os.path\n",
    "\n",
    "def to_feature_shape(feat):\n",
    "    feat = np.array(feat)\n",
    "    if len(feat.shape) == 1:#not a real array but just a long list\n",
    "        feat = np.reshape(feat,(feat.shape[0],1))\n",
    "    return feat\n",
    "\n",
    "#This method should throw an error if something goes wrong\n",
    "def read_feature(file_path):\n",
    "    f = open(file_path, 'rb')\n",
    "    feat = to_feature_shape(pickle.load(f))\n",
    "    f.close()\n",
    "    return feat\n",
    "    \n",
    "features_to_read = [\"overlap_title\",\n",
    "                 \"comm_auth\",\n",
    "                 \"temp_diff\",\n",
    "                \"citation_check\",\n",
    "                \"max_sim\",\n",
    "                \"peer_popularity\",\n",
    "                \"edge_check\",\n",
    "                \"LSA_distance\",\n",
    "                \"node_degree\"]\n",
    "\n",
    "train_features_dict = dict()\n",
    "train_features_reduced_dict = dict()\n",
    "test_features_dict = dict()\n",
    "test_features_reduced_dict = dict()\n",
    "for name in features_to_read:\n",
    "    # Train\n",
    "    file_path = './features_train/'+name\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        train_features_dict[name] = this_feat\n",
    "        train_features_reduced_dict[name] = this_feat[to_keep_train,:]\n",
    "    except:\n",
    "        pass\n",
    "    # Train reduced\n",
    "    file_path = './features_train/'+name+'_reduced'\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        train_features_reduced_dict[name] = this_feat\n",
    "    except:\n",
    "        pass\n",
    "    # Test\n",
    "    file_path = './features_test/'+name\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        test_features_dict[name] = this_feat\n",
    "        test_features_reduced_dict[name] = this_feat[to_keep_test,:]\n",
    "    except:\n",
    "        pass\n",
    "    # Test reduced\n",
    "    file_path = './features_test/'+name+'_reduced'\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        test_features_reduced_dict[name] = this_feat\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(554602,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(train_true_labels,(train_true_labels.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true_labels = read_feature('./features_train/true_labels')\n",
    "train_true_labels = np.reshape(train_true_labels,(train_true_labels.shape[0],))\n",
    "train_true_labels_reduced = train_true_labels[to_keep_train]\n",
    "test_true_labels = read_feature('./features_test/true_labels')\n",
    "test_true_labels = np.reshape(test_true_labels,(test_true_labels.shape[0],))\n",
    "test_true_labels_reduced = test_true_labels[to_keep_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 554602\n",
      "1000 / 554602\n",
      "2000 / 554602\n",
      "3000 / 554602\n",
      "4000 / 554602\n",
      "5000 / 554602\n",
      "6000 / 554602\n",
      "7000 / 554602\n",
      "8000 / 554602\n",
      "9000 / 554602\n",
      "10000 / 554602\n",
      "11000 / 554602\n",
      "12000 / 554602\n",
      "13000 / 554602\n",
      "14000 / 554602\n",
      "15000 / 554602\n",
      "16000 / 554602\n",
      "17000 / 554602\n",
      "18000 / 554602\n",
      "19000 / 554602\n",
      "20000 / 554602\n",
      "21000 / 554602\n",
      "22000 / 554602\n",
      "23000 / 554602\n",
      "24000 / 554602\n",
      "25000 / 554602\n",
      "26000 / 554602\n",
      "27000 / 554602\n",
      "28000 / 554602\n",
      "29000 / 554602\n",
      "30000 / 554602\n",
      "31000 / 554602\n",
      "32000 / 554602\n",
      "33000 / 554602\n",
      "34000 / 554602\n",
      "35000 / 554602\n",
      "36000 / 554602\n",
      "37000 / 554602\n",
      "38000 / 554602\n",
      "39000 / 554602\n",
      "40000 / 554602\n",
      "41000 / 554602\n",
      "42000 / 554602\n",
      "43000 / 554602\n",
      "44000 / 554602\n",
      "45000 / 554602\n",
      "46000 / 554602\n",
      "47000 / 554602\n",
      "48000 / 554602\n",
      "49000 / 554602\n",
      "50000 / 554602\n",
      "51000 / 554602\n",
      "52000 / 554602\n",
      "53000 / 554602\n",
      "54000 / 554602\n",
      "55000 / 554602\n",
      "56000 / 554602\n",
      "57000 / 554602\n",
      "58000 / 554602\n",
      "59000 / 554602\n",
      "60000 / 554602\n",
      "61000 / 554602\n",
      "62000 / 554602\n",
      "63000 / 554602\n",
      "64000 / 554602\n",
      "65000 / 554602\n",
      "66000 / 554602\n",
      "67000 / 554602\n",
      "68000 / 554602\n",
      "69000 / 554602\n",
      "70000 / 554602\n",
      "71000 / 554602\n",
      "72000 / 554602\n",
      "73000 / 554602\n",
      "74000 / 554602\n",
      "75000 / 554602\n",
      "76000 / 554602\n",
      "77000 / 554602\n",
      "78000 / 554602\n",
      "79000 / 554602\n",
      "80000 / 554602\n",
      "81000 / 554602\n",
      "82000 / 554602\n",
      "83000 / 554602\n",
      "84000 / 554602\n",
      "85000 / 554602\n",
      "86000 / 554602\n",
      "87000 / 554602\n",
      "88000 / 554602\n",
      "89000 / 554602\n",
      "90000 / 554602\n",
      "91000 / 554602\n",
      "92000 / 554602\n",
      "93000 / 554602\n",
      "94000 / 554602\n",
      "95000 / 554602\n",
      "96000 / 554602\n",
      "97000 / 554602\n",
      "98000 / 554602\n",
      "99000 / 554602\n",
      "100000 / 554602\n",
      "101000 / 554602\n",
      "102000 / 554602\n",
      "103000 / 554602\n",
      "104000 / 554602\n",
      "105000 / 554602\n",
      "106000 / 554602\n",
      "107000 / 554602\n",
      "108000 / 554602\n",
      "109000 / 554602\n",
      "110000 / 554602\n",
      "111000 / 554602\n",
      "112000 / 554602\n",
      "113000 / 554602\n",
      "114000 / 554602\n",
      "115000 / 554602\n",
      "116000 / 554602\n",
      "117000 / 554602\n",
      "118000 / 554602\n",
      "119000 / 554602\n",
      "120000 / 554602\n",
      "121000 / 554602\n",
      "122000 / 554602\n",
      "123000 / 554602\n",
      "124000 / 554602\n",
      "125000 / 554602\n",
      "126000 / 554602\n",
      "127000 / 554602\n",
      "128000 / 554602\n",
      "129000 / 554602\n",
      "130000 / 554602\n",
      "131000 / 554602\n",
      "132000 / 554602\n",
      "133000 / 554602\n",
      "134000 / 554602\n",
      "135000 / 554602\n",
      "136000 / 554602\n",
      "137000 / 554602\n",
      "138000 / 554602\n",
      "139000 / 554602\n",
      "140000 / 554602\n",
      "141000 / 554602\n",
      "142000 / 554602\n",
      "143000 / 554602\n",
      "144000 / 554602\n",
      "145000 / 554602\n",
      "146000 / 554602\n",
      "147000 / 554602\n",
      "148000 / 554602\n",
      "149000 / 554602\n",
      "150000 / 554602\n",
      "151000 / 554602\n",
      "152000 / 554602\n",
      "153000 / 554602\n",
      "154000 / 554602\n",
      "155000 / 554602\n",
      "156000 / 554602\n",
      "157000 / 554602\n",
      "158000 / 554602\n",
      "159000 / 554602\n",
      "160000 / 554602\n",
      "161000 / 554602\n",
      "162000 / 554602\n",
      "163000 / 554602\n",
      "164000 / 554602\n",
      "165000 / 554602\n",
      "166000 / 554602\n",
      "167000 / 554602\n",
      "168000 / 554602\n",
      "169000 / 554602\n",
      "170000 / 554602\n",
      "171000 / 554602\n",
      "172000 / 554602\n",
      "173000 / 554602\n",
      "174000 / 554602\n",
      "175000 / 554602\n",
      "176000 / 554602\n",
      "177000 / 554602\n",
      "178000 / 554602\n",
      "179000 / 554602\n",
      "180000 / 554602\n",
      "181000 / 554602\n",
      "182000 / 554602\n",
      "183000 / 554602\n",
      "184000 / 554602\n",
      "185000 / 554602\n",
      "186000 / 554602\n",
      "187000 / 554602\n",
      "188000 / 554602\n",
      "189000 / 554602\n",
      "190000 / 554602\n",
      "191000 / 554602\n",
      "192000 / 554602\n",
      "193000 / 554602\n",
      "194000 / 554602\n",
      "195000 / 554602\n",
      "196000 / 554602\n",
      "197000 / 554602\n",
      "198000 / 554602\n",
      "199000 / 554602\n",
      "200000 / 554602\n",
      "201000 / 554602\n",
      "202000 / 554602\n",
      "203000 / 554602\n",
      "204000 / 554602\n",
      "205000 / 554602\n",
      "206000 / 554602\n",
      "207000 / 554602\n",
      "208000 / 554602\n",
      "209000 / 554602\n",
      "210000 / 554602\n",
      "211000 / 554602\n",
      "212000 / 554602\n",
      "213000 / 554602\n",
      "214000 / 554602\n",
      "215000 / 554602\n",
      "216000 / 554602\n",
      "217000 / 554602\n",
      "218000 / 554602\n",
      "219000 / 554602\n",
      "220000 / 554602\n",
      "221000 / 554602\n",
      "222000 / 554602\n",
      "223000 / 554602\n",
      "224000 / 554602\n",
      "225000 / 554602\n",
      "226000 / 554602\n",
      "227000 / 554602\n",
      "228000 / 554602\n",
      "229000 / 554602\n",
      "230000 / 554602\n",
      "231000 / 554602\n",
      "232000 / 554602\n",
      "233000 / 554602\n",
      "234000 / 554602\n",
      "235000 / 554602\n",
      "236000 / 554602\n",
      "237000 / 554602\n",
      "238000 / 554602\n",
      "239000 / 554602\n",
      "240000 / 554602\n",
      "241000 / 554602\n",
      "242000 / 554602\n",
      "243000 / 554602\n",
      "244000 / 554602\n",
      "245000 / 554602\n",
      "246000 / 554602\n",
      "247000 / 554602\n",
      "248000 / 554602\n",
      "249000 / 554602\n",
      "250000 / 554602\n",
      "251000 / 554602\n",
      "252000 / 554602\n",
      "253000 / 554602\n",
      "254000 / 554602\n",
      "255000 / 554602\n",
      "256000 / 554602\n",
      "257000 / 554602\n",
      "258000 / 554602\n",
      "259000 / 554602\n",
      "260000 / 554602\n",
      "261000 / 554602\n",
      "262000 / 554602\n",
      "263000 / 554602\n",
      "264000 / 554602\n",
      "265000 / 554602\n",
      "266000 / 554602\n",
      "267000 / 554602\n",
      "268000 / 554602\n",
      "269000 / 554602\n",
      "270000 / 554602\n",
      "271000 / 554602\n",
      "272000 / 554602\n",
      "273000 / 554602\n",
      "274000 / 554602\n",
      "275000 / 554602\n",
      "276000 / 554602\n",
      "277000 / 554602\n",
      "278000 / 554602\n",
      "279000 / 554602\n",
      "280000 / 554602\n",
      "281000 / 554602\n",
      "282000 / 554602\n",
      "283000 / 554602\n",
      "284000 / 554602\n",
      "285000 / 554602\n",
      "286000 / 554602\n",
      "287000 / 554602\n",
      "288000 / 554602\n",
      "289000 / 554602\n",
      "290000 / 554602\n",
      "291000 / 554602\n",
      "292000 / 554602\n",
      "293000 / 554602\n",
      "294000 / 554602\n",
      "295000 / 554602\n",
      "296000 / 554602\n",
      "297000 / 554602\n",
      "298000 / 554602\n",
      "299000 / 554602\n",
      "300000 / 554602\n",
      "301000 / 554602\n",
      "302000 / 554602\n",
      "303000 / 554602\n",
      "304000 / 554602\n",
      "305000 / 554602\n",
      "306000 / 554602\n",
      "307000 / 554602\n",
      "308000 / 554602\n",
      "309000 / 554602\n",
      "310000 / 554602\n",
      "311000 / 554602\n",
      "312000 / 554602\n",
      "313000 / 554602\n",
      "314000 / 554602\n",
      "315000 / 554602\n",
      "316000 / 554602\n",
      "317000 / 554602\n",
      "318000 / 554602\n",
      "319000 / 554602\n",
      "320000 / 554602\n",
      "321000 / 554602\n",
      "322000 / 554602\n",
      "323000 / 554602\n",
      "324000 / 554602\n",
      "325000 / 554602\n",
      "326000 / 554602\n",
      "327000 / 554602\n",
      "328000 / 554602\n",
      "329000 / 554602\n",
      "330000 / 554602\n",
      "331000 / 554602\n",
      "332000 / 554602\n",
      "333000 / 554602\n",
      "334000 / 554602\n",
      "335000 / 554602\n",
      "336000 / 554602\n",
      "337000 / 554602\n",
      "338000 / 554602\n",
      "339000 / 554602\n",
      "340000 / 554602\n",
      "341000 / 554602\n",
      "342000 / 554602\n",
      "343000 / 554602\n",
      "344000 / 554602\n",
      "345000 / 554602\n",
      "346000 / 554602\n",
      "347000 / 554602\n",
      "348000 / 554602\n",
      "349000 / 554602\n",
      "350000 / 554602\n",
      "351000 / 554602\n",
      "352000 / 554602\n",
      "353000 / 554602\n",
      "354000 / 554602\n",
      "355000 / 554602\n",
      "356000 / 554602\n",
      "357000 / 554602\n",
      "358000 / 554602\n",
      "359000 / 554602\n",
      "360000 / 554602\n",
      "361000 / 554602\n",
      "362000 / 554602\n",
      "363000 / 554602\n",
      "364000 / 554602\n",
      "365000 / 554602\n",
      "366000 / 554602\n",
      "367000 / 554602\n",
      "368000 / 554602\n",
      "369000 / 554602\n",
      "370000 / 554602\n",
      "371000 / 554602\n",
      "372000 / 554602\n",
      "373000 / 554602\n",
      "374000 / 554602\n",
      "375000 / 554602\n",
      "376000 / 554602\n",
      "377000 / 554602\n",
      "378000 / 554602\n",
      "379000 / 554602\n",
      "380000 / 554602\n",
      "381000 / 554602\n",
      "382000 / 554602\n",
      "383000 / 554602\n",
      "384000 / 554602\n",
      "385000 / 554602\n",
      "386000 / 554602\n",
      "387000 / 554602\n",
      "388000 / 554602\n",
      "389000 / 554602\n",
      "390000 / 554602\n",
      "391000 / 554602\n",
      "392000 / 554602\n",
      "393000 / 554602\n",
      "394000 / 554602\n",
      "395000 / 554602\n",
      "396000 / 554602\n",
      "397000 / 554602\n",
      "398000 / 554602\n",
      "399000 / 554602\n",
      "400000 / 554602\n",
      "401000 / 554602\n",
      "402000 / 554602\n",
      "403000 / 554602\n",
      "404000 / 554602\n",
      "405000 / 554602\n",
      "406000 / 554602\n",
      "407000 / 554602\n",
      "408000 / 554602\n",
      "409000 / 554602\n",
      "410000 / 554602\n",
      "411000 / 554602\n",
      "412000 / 554602\n",
      "413000 / 554602\n",
      "414000 / 554602\n",
      "415000 / 554602\n",
      "416000 / 554602\n",
      "417000 / 554602\n",
      "418000 / 554602\n",
      "419000 / 554602\n",
      "420000 / 554602\n",
      "421000 / 554602\n",
      "422000 / 554602\n",
      "423000 / 554602\n",
      "424000 / 554602\n",
      "425000 / 554602\n",
      "426000 / 554602\n",
      "427000 / 554602\n",
      "428000 / 554602\n",
      "429000 / 554602\n",
      "430000 / 554602\n",
      "431000 / 554602\n",
      "432000 / 554602\n",
      "433000 / 554602\n",
      "434000 / 554602\n",
      "435000 / 554602\n",
      "436000 / 554602\n",
      "437000 / 554602\n",
      "438000 / 554602\n",
      "439000 / 554602\n",
      "440000 / 554602\n",
      "441000 / 554602\n",
      "442000 / 554602\n",
      "443000 / 554602\n",
      "444000 / 554602\n",
      "445000 / 554602\n",
      "446000 / 554602\n",
      "447000 / 554602\n",
      "448000 / 554602\n",
      "449000 / 554602\n",
      "450000 / 554602\n",
      "451000 / 554602\n",
      "452000 / 554602\n",
      "453000 / 554602\n",
      "454000 / 554602\n",
      "455000 / 554602\n",
      "456000 / 554602\n",
      "457000 / 554602\n",
      "458000 / 554602\n",
      "459000 / 554602\n",
      "460000 / 554602\n",
      "461000 / 554602\n",
      "462000 / 554602\n",
      "463000 / 554602\n",
      "464000 / 554602\n",
      "465000 / 554602\n",
      "466000 / 554602\n",
      "467000 / 554602\n",
      "468000 / 554602\n",
      "469000 / 554602\n",
      "470000 / 554602\n",
      "471000 / 554602\n",
      "472000 / 554602\n",
      "473000 / 554602\n",
      "474000 / 554602\n",
      "475000 / 554602\n",
      "476000 / 554602\n",
      "477000 / 554602\n",
      "478000 / 554602\n",
      "479000 / 554602\n",
      "480000 / 554602\n",
      "481000 / 554602\n",
      "482000 / 554602\n",
      "483000 / 554602\n",
      "484000 / 554602\n",
      "485000 / 554602\n",
      "486000 / 554602\n",
      "487000 / 554602\n",
      "488000 / 554602\n",
      "489000 / 554602\n",
      "490000 / 554602\n",
      "491000 / 554602\n",
      "492000 / 554602\n",
      "493000 / 554602\n",
      "494000 / 554602\n",
      "495000 / 554602\n",
      "496000 / 554602\n",
      "497000 / 554602\n",
      "498000 / 554602\n",
      "499000 / 554602\n",
      "500000 / 554602\n",
      "501000 / 554602\n",
      "502000 / 554602\n",
      "503000 / 554602\n",
      "504000 / 554602\n",
      "505000 / 554602\n",
      "506000 / 554602\n",
      "507000 / 554602\n",
      "508000 / 554602\n",
      "509000 / 554602\n",
      "510000 / 554602\n",
      "511000 / 554602\n",
      "512000 / 554602\n",
      "513000 / 554602\n",
      "514000 / 554602\n",
      "515000 / 554602\n",
      "516000 / 554602\n",
      "517000 / 554602\n",
      "518000 / 554602\n",
      "519000 / 554602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520000 / 554602\n",
      "521000 / 554602\n",
      "522000 / 554602\n",
      "523000 / 554602\n",
      "524000 / 554602\n",
      "525000 / 554602\n",
      "526000 / 554602\n",
      "527000 / 554602\n",
      "528000 / 554602\n",
      "529000 / 554602\n",
      "530000 / 554602\n",
      "531000 / 554602\n",
      "532000 / 554602\n",
      "533000 / 554602\n",
      "534000 / 554602\n",
      "535000 / 554602\n",
      "536000 / 554602\n",
      "537000 / 554602\n",
      "538000 / 554602\n",
      "539000 / 554602\n",
      "540000 / 554602\n",
      "541000 / 554602\n",
      "542000 / 554602\n",
      "543000 / 554602\n",
      "544000 / 554602\n",
      "545000 / 554602\n",
      "546000 / 554602\n",
      "547000 / 554602\n",
      "548000 / 554602\n",
      "549000 / 554602\n",
      "550000 / 554602\n",
      "551000 / 554602\n",
      "552000 / 554602\n",
      "553000 / 554602\n",
      "554000 / 554602\n",
      "overlap_title 554602\n",
      "comm_auth 554602\n",
      "temp_diff 554602\n",
      "peer_popularity 554602\n",
      "edge_check 554602\n",
      "LSA_distance 554602\n",
      "citation_check 0\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "###  Construct features on TRAINING_SET  ###\n",
    "############################################\n",
    "\n",
    "#Build KDTree on training_set\n",
    "train_l = [l[node_dict[i]] for i in train_IDs]\n",
    "train_kdtree = nw.KDTree(train_l)\n",
    "\n",
    "train_true_labels = []\n",
    "features_to_create = ['citation_check']\n",
    "# Where to insert created features\n",
    "insert_features_dict = train_features_dict\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "set_to_use = training_set\n",
    "\n",
    "for i,triple in enumerate(set_to_use):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "    # convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    # remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\") \n",
    "    \n",
    "    \n",
    "    # Creating features\n",
    "    # Baseline #\n",
    "    #overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "    #insert_features_dict[\"overlap_title\"].append(overlap_title)\n",
    "    #temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    #insert_features_dict[\"temp_diff\"].append(temp_diff)\n",
    "    #comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    #insert_features_dict[\"comm_auth\"].append(comm_auth)\n",
    "\n",
    "    #peer_pop = pw.peer_popularity(train_graph,source,target)\n",
    "    #insert_features_dict[\"peer_popularity\"].append(peer_pop)\n",
    "\n",
    "    #max_sim = pw.Max_Sim(source,target,l,train_graph,node_dict)\n",
    "    #insert_features_dict[\"max_sim\"].append(max_sim)\n",
    "\n",
    "    #edge_check = pw.edge_check(source,target,train_graph)\n",
    "    #insert_features_dict[\"edge_check\"].append(edge_check)\n",
    "\n",
    "    #LSA_dist = pw.LSA_distance(source,target,node_dict,l)\n",
    "    #insert_features_dict[\"LSA_distance\"].append(LSA_dist)\n",
    "    \n",
    "    #citation_check = pw.Citation_Check(source,target,train_kdtree,l,train_graph,node_dict,index_dict,k=500)\n",
    "    #insert_features_dict[\"citation_check\"].append(citation_check)\n",
    "\n",
    "    #degree = pw.node_degree(source,target,train_graph)\n",
    "    #insert_features_dict[\"node_degree\"].append(degree)\n",
    "\n",
    "    train_true_labels.append(triple[2])\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(i,\"/\",len(set_to_use))\n",
    "\n",
    "# Reshape features into np column arrays, one row per node pair\n",
    "for (name,value) in insert_features_dict.items():\n",
    "    print(name,len(value))\n",
    "    insert_features_dict[name] = to_feature_shape(value)\n",
    "        \n",
    "# Concatenate all features\n",
    "feats_train = np.concatenate([feat for feat in insert_features_dict.values()])\n",
    "train_true_labels = np.array(train_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current wd:  /home/lucas/Documents/École Polytechnique/Courses/ML1/Challenge/git_repository/ML_project\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "###   Write features to disk - Training  ###\n",
    "############################################\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def write_feature_to_disk(feat,file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(feat,file)\n",
    "\n",
    "print(\"Current wd: \",os.getcwd())\n",
    "write_feature_to_disk(train_features_dict['citation_check'],'./features_train/citation_check_reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8af1c397ccd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_features_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'citation_check'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 60910\n",
      "1000 / 60910\n",
      "2000 / 60910\n",
      "3000 / 60910\n",
      "4000 / 60910\n",
      "5000 / 60910\n",
      "6000 / 60910\n",
      "7000 / 60910\n",
      "8000 / 60910\n",
      "9000 / 60910\n",
      "10000 / 60910\n",
      "11000 / 60910\n",
      "12000 / 60910\n",
      "13000 / 60910\n",
      "14000 / 60910\n",
      "15000 / 60910\n",
      "16000 / 60910\n",
      "17000 / 60910\n",
      "18000 / 60910\n",
      "19000 / 60910\n",
      "20000 / 60910\n",
      "21000 / 60910\n",
      "22000 / 60910\n",
      "23000 / 60910\n",
      "24000 / 60910\n",
      "25000 / 60910\n",
      "26000 / 60910\n",
      "27000 / 60910\n",
      "28000 / 60910\n",
      "29000 / 60910\n",
      "30000 / 60910\n",
      "31000 / 60910\n",
      "32000 / 60910\n",
      "33000 / 60910\n",
      "34000 / 60910\n",
      "35000 / 60910\n",
      "36000 / 60910\n",
      "37000 / 60910\n",
      "38000 / 60910\n",
      "39000 / 60910\n",
      "40000 / 60910\n",
      "41000 / 60910\n",
      "42000 / 60910\n",
      "43000 / 60910\n",
      "44000 / 60910\n",
      "45000 / 60910\n",
      "46000 / 60910\n",
      "47000 / 60910\n",
      "48000 / 60910\n",
      "49000 / 60910\n",
      "50000 / 60910\n",
      "51000 / 60910\n",
      "52000 / 60910\n",
      "53000 / 60910\n",
      "54000 / 60910\n",
      "55000 / 60910\n",
      "56000 / 60910\n",
      "57000 / 60910\n",
      "58000 / 60910\n",
      "59000 / 60910\n",
      "60000 / 60910\n",
      "overlap_title 60910\n",
      "comm_auth 60910\n",
      "temp_diff 60910\n",
      "citation_check 60910\n",
      "max_sim 60910\n",
      "peer_popularity 60910\n",
      "edge_check 60910\n",
      "LSA_distance 60910\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4c65aa409e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Concatenate all features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mfeats_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minsert_features_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0mtest_true_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_true_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "###  Construct features on TESTING_SET  ###\n",
    "###########################################\n",
    "\n",
    "#Build KDTree on training_set\n",
    "train_l = [l[node_dict[i]] for i in train_IDs]\n",
    "train_kdtree = nw.KDTree(train_l)\n",
    "\n",
    "train_true_labels = []\n",
    "features_to_create = []\n",
    "\n",
    "# Where to insert created features\n",
    "insert_features_dict = test_features_dict\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "set_to_use = testing_set\n",
    "\n",
    "test_true_labels = []\n",
    "for i,triple in enumerate(set_to_use):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "    # convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    # remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\") \n",
    "    \n",
    "    \n",
    "    # Creating features\n",
    "    # Baseline #\n",
    "    #overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "    #insert_features_dict[\"overlap_title\"].append(overlap_title)\n",
    "    #temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    #insert_features_dict[\"temp_diff\"].append(temp_diff)\n",
    "    #comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    #insert_features_dict[\"comm_auth\"].append(comm_auth)\n",
    "\n",
    "    #peer_pop = pw.peer_popularity(train_graph,source,target)\n",
    "    #insert_features_dict[\"peer_popularity\"].append(peer_pop)\n",
    "\n",
    "    #max_sim = pw.Max_Sim(source,target,l,train_graph,node_dict)\n",
    "    #insert_features_dict[\"max_sim\"].append(max_sim)\n",
    "\n",
    "    #edge_check = pw.edge_check(source,target,train_graph)\n",
    "    #insert_features_dict[\"edge_check\"].append(edge_check)\n",
    "\n",
    "    #LSA_dist = pw.LSA_distance(source,target,node_dict,l)\n",
    "    #insert_features_dict[\"LSA_distance\"].append(LSA_dist)\n",
    "    \n",
    "    #citation_check = pw.Citation_Check(source,target,train_kdtree,l,train_graph,node_dict,index_dict,k=500)\n",
    "    #insert_features_dict[\"citation_check\"].append(citation_check)\n",
    "\n",
    "    #degree = pw.node_degree(source,target,train_graph)\n",
    "    #insert_features_dict[\"node_degree\"].append(degree)\n",
    "\n",
    "    test_true_labels.append(triple[2])\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(i,\"/\",len(set_to_use))\n",
    "\n",
    "# Reshape features into np column arrays, one row per node pair\n",
    "for (name,value) in insert_features_dict.items():\n",
    "    print(name,len(value))\n",
    "    insert_features_dict[name] = to_feature_shape(value)\n",
    "        \n",
    "# Concatenate all features\n",
    "feats_test = np.concatenate([feat for feat in insert_features_dict.values()])\n",
    "test_true_labels = np.array(test_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current wd:  /home/lucas/Documents/École Polytechnique/Courses/ML1/Challenge/git_repository/ML_project\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "###   Write features to disk - Testing  ###\n",
    "###########################################\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def write_feature_to_disk(feat,file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(feat,file)\n",
    "\n",
    "print(\"Current wd: \",os.getcwd())\n",
    "write_feature_to_disk(test_features_dict['overlap_title'],'./features_test/overlap_title')\n",
    "write_feature_to_disk(test_features_dict['temp_diff'],'./features_test/temp_diff')\n",
    "write_feature_to_disk(test_features_dict['comm_auth'],'./features_test/comm_auth')\n",
    "write_feature_to_disk(test_features_dict['peer_popularity'],'./features_test/peer_popularity')\n",
    "write_feature_to_disk(test_features_dict['edge_check'],'./features_test/edge_check')\n",
    "write_feature_to_disk(test_features_dict['LSA_distance'],'./features_test/LSA_distance')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap_title (27730, 1)\n",
      "comm_auth (27730, 1)\n",
      "temp_diff (27730, 1)\n",
      "citation_check (27730, 2)\n",
      "max_sim (27730, 9)\n",
      "peer_popularity (27730, 1)\n",
      "edge_check (27730, 1)\n",
      "LSA_distance (27730, 1)\n"
     ]
    }
   ],
   "source": [
    "#Combine all features to one vector\n",
    "train_features_reduced_dict.keys()\n",
    "test_features_dict.keys()\n",
    "for key,feat in train_features_reduced_dict.items():\n",
    "    print(key,feat.shape)\n",
    "train_features_reduced = np.concatenate([#train_features_reduced_dict['overlap_title'],\n",
    "                                         #train_features_reduced_dict['comm_auth'],\n",
    "                                         train_features_reduced_dict['temp_diff'],\n",
    "                                         train_features_reduced_dict['citation_check'],\n",
    "                                         train_features_reduced_dict['max_sim'],\n",
    "                                         train_features_reduced_dict['peer_popularity'],\n",
    "                                         #train_features_reduced_dict['edge_check'],\n",
    "                                         train_features_reduced_dict['LSA_distance']]\n",
    "                                        ,axis = 1)\n",
    "\n",
    "test_features_reduced = np.concatenate([#test_features_reduced_dict['overlap_title'],\n",
    "                                        #test_features_reduced_dict['comm_auth'],\n",
    "                                        test_features_reduced_dict['temp_diff'],\n",
    "                                        test_features_reduced_dict['citation_check'],\n",
    "                                        test_features_reduced_dict['max_sim'],\n",
    "                                        test_features_reduced_dict['peer_popularity'],\n",
    "                                        #test_features_reduced_dict['edge_check'],\n",
    "                                        test_features_reduced_dict['LSA_distance']]\n",
    "                                        ,axis = 1)\n",
    "\n",
    "test_features = np.concatenate([#test_features_dict['overlap_title'],\n",
    "                                        #test_features_dict['comm_auth'],\n",
    "                                        test_features_dict['temp_diff'],\n",
    "                                        test_features_dict['citation_check'],\n",
    "                                        test_features_dict['max_sim'],\n",
    "                                        test_features_dict['peer_popularity'],\n",
    "                                        #test_features_dict['edge_check'],\n",
    "                                        test_features_dict['LSA_distance']]\n",
    "                                        ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27730, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "normalized_train_features = normalize(train_features_reduced,axis=0)\n",
    "normalized_test_features = normalize(test_features_reduced,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8573961582662946 f1: 0.8677205165692008\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.LinearSVC()\n",
    "\n",
    "# train\n",
    "classifier.fit(train_features_reduced, train_true_labels_reduced)\n",
    "preds= list(classifier.predict(test_features))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7751436545723198 f1: 0.7900416973264657\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "model = lr().fit(train_features_reduced, train_true_labels_reduced)\n",
    "preds = list(model.predict(test_features))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nNhbr = KNeighborsClassifier()\n",
    "nNhbr.fit(features,true_labels) # do Ytrain.ravel() for length one Y values\n",
    "preds = nNhbr.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dTree = DecisionTreeClassifier()\n",
    "dTree.fit(features,true_labels) # do Ytrain.ravel() for length one Y values\n",
    "preds = dTree.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "class Class_Net():\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, batch_size=60):\n",
    "        ''' initialize the classifier with default (best) parameters '''\n",
    "        tf.reset_default_graph()\n",
    "        self.alpha = learning_rate\n",
    "        self.beta = batch_size\n",
    "        self.warm = False\n",
    "\n",
    "    def fit(self,X,Y,warm_start=True,n_epochs=50):\n",
    "        ''' train the network, and if warm_start, then do not reinit. the network\n",
    "            (if it has already been initialized)\n",
    "        '''\n",
    "        self.epochs=n_epochs\n",
    "\n",
    "        self.n_batch = int(len(X)/self.beta)\n",
    "        \n",
    "        if warm_start==False or self.warm==False:\n",
    "            self.x = tf.placeholder(tf.float32,shape=[None,len(X[0])])\n",
    "            self.y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "            \n",
    "            self.wZero = tf.get_variable('wZero',shape=[len(X[0]),50],initializer=tf.glorot_uniform_initializer())\n",
    "            self.bZero = tf.Variable(tf.zeros([50]))\n",
    "\n",
    "            self.wOne = tf.get_variable('wOne',shape=[50,1],initializer=tf.glorot_uniform_initializer())\n",
    "            self.bOne = tf.Variable(tf.zeros([1]))\n",
    "            self.keep_prob = 0.9\n",
    "            self.drop_out = tf.nn.dropout(self.x, self.keep_prob)\n",
    "            self.model = tf.nn.sigmoid(tf.matmul(tf.nn.relu(tf.matmul(self.drop_out, self.wZero) \n",
    "                                              + self.bZero),self.wOne)+self.bOne)\n",
    "            self.cost = tf.losses.log_loss(self.y,self.model)\n",
    "            \n",
    "#             self.optimizer = tf.train.GradientDescentOptimizer(self.alpha).minimize(self.cost)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.alpha).minimize(self.cost)\n",
    "\n",
    "            #without dropout\n",
    "#             self.model = tf.matmul(tf.nn.relu(tf.matmul(self.x, self.wZero) + self.bZero),self.wOne)+self.bOne\n",
    "\n",
    "    \n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        with tf.Session() as sess:\n",
    "            if warm_start==False or self.warm==False:\n",
    "                sess.run(self.init)\n",
    "            else:\n",
    "                self.saver.restore(sess, './tempVariables.ckpt')\n",
    "            for epoch in range(self.epochs):\n",
    "                self.avg_cost = 0\n",
    "                for i in range(self.n_batch):\n",
    "                    _, self.c = sess.run([self.optimizer,self.cost], feed_dict={self.x: X[i*self.beta:min([(i+1)*\n",
    "                                    self.beta,len(X)]),:],self.y:Y[i*self.beta:min([(i+1)*self.beta,len(X)])]})\n",
    "                    \n",
    "                    self.avg_cost = self.avg_cost+np.mean(self.c)/self.n_batch\n",
    "                print(\"Epoch:\", '%s' % (epoch+1), \"cost=\", \"%s\"% (self.avg_cost))\n",
    "            self.saver.save(sess,'./tempVariables.ckpt')\n",
    "            \n",
    "        self.warm = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        ''' return a matrix P where P[i,j] = P(Y[i,j]=1), \n",
    "        for all instances i, and labels j. '''\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, './tempVariables.ckpt')\n",
    "            self.preds = sess.run(tf.nn.softmax(self.model), feed_dict={self.x: X}) \n",
    "        return self.preds\n",
    "    \n",
    "    def predict(self,X):\n",
    "        ''' return a matrix of predictions for X '''\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Class_Net(learning_rate=0.01,batch_size=250)\n",
    "net.fit(features,list(map(lambda x: [x],true_labels)),n_epochs=35)\n",
    "preds=net.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
