{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'journal_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-129a5f742a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mpublication_years\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_info\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mjournals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_info\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mjournal_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjournal_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_info\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_info\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'journal_dict' is not defined"
     ]
    }
   ],
   "source": [
    "import features_nodewise as nw\n",
    "import features_pairwise as pw\n",
    "import preprocessing as prep\n",
    "\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "\n",
    "## Read train / test node pairs\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "with open(\"./data/train_train_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "with open(\"./data/train_test_set.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "random.seed(0)\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "to_keep_train = random.sample( range(len(training_set)),k=int(round(len(training_set)*0.05)) )\n",
    "training_set_reduced = [training_set[i] for i in to_keep_train]\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "to_keep_test = random.sample( range(len(testing_set)),k=int(round(len(testing_set)*1)) )\n",
    "testing_set_reduced = [testing_set[i] for i in to_keep_test]\n",
    "\n",
    "with open(\"./data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "publication_years = [int(info[1]) for info in node_info]\n",
    "\n",
    "corpus = [element[5] for element in node_info]\n",
    "titles = [element[2] for element in node_info]\n",
    "\n",
    "t_titles = prep.tfidf(titles)\n",
    "t = prep.tfidf(corpus)\n",
    "l = nw.LSA(t)\n",
    "IDs = [element[0] for element in node_info]\n",
    "node_dict = prep.to_dict( [element[0] for element in node_info],range(len(node_info)) )\n",
    "index_dict = prep.to_dict(range(len(IDs)),IDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from '/home/lucas/Documents/École Polytechnique/Courses/ML1/Challenge/git_repository/ML_project/preprocessing.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#title_sim = t_titles*t_titles.transpose()\n",
    "importlib.reload(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'features_pairwise' from '/home/lucas/Documents/École Polytechnique/Courses/ML1/Challenge/git_repository/ML_project/features_pairwise.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(pw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GOLD STANDARD graphs (train vs test), (reduced vs full)\n",
    "\n",
    "train_IDs = set([training_set[i][0] for i in range(len(training_set))])\n",
    "train_IDs = train_IDs | set([training_set[i][1] for i in range(len(training_set))])\n",
    "train_IDs = list(train_IDs) #igraph doesn't like sets...\n",
    "train_edges = [(element[0],element[1]) for element in training_set]\n",
    "train_graph = prep.article_graph(train_IDs,train_edges)\n",
    "\n",
    "train_IDs_reduced = set([training_set_reduced[i][0] for i in range(len(training_set_reduced))])\n",
    "train_IDs_reduced = train_IDs_reduced | set([training_set_reduced[i][1] for i in range(len(training_set_reduced))])\n",
    "train_IDs_reduced = list(train_IDs_reduced)\n",
    "train_edges_reduced = [(element[0],element[1]) for element in training_set_reduced]\n",
    "train_graph_reduced = prep.article_graph(train_IDs_reduced,train_edges_reduced)\n",
    "\n",
    "test_IDs = set([testing_set[i][0] for i in range(len(testing_set))])\n",
    "test_IDs = test_IDs | set([testing_set[i][1] for i in range(len(testing_set))])\n",
    "test_IDs = list(test_IDs)\n",
    "test_edges = [(element[0],element[1]) for element in testing_set]\n",
    "test_graph = prep.article_graph(test_IDs,test_edges)\n",
    "\n",
    "test_IDs_reduced = set([testing_set_reduced[i][0] for i in range(len(testing_set_reduced))])\n",
    "test_IDs_reduced = test_IDs_reduced | set([testing_set_reduced[i][1] for i in range(len(testing_set_reduced))])\n",
    "test_IDs_reduced = list(test_IDs_reduced)\n",
    "test_edges_reduced = [(element[0],element[1]) for element in testing_set_reduced]\n",
    "test_graph_reduced = prep.article_graph(test_IDs_reduced,test_edges_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9507159'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read all the features that we have stored in files\n",
    "import os.path\n",
    "\n",
    "def to_feature_shape(feat):\n",
    "    feat = np.array(feat)\n",
    "    if len(feat.shape) == 1:#not a real array but just a long list\n",
    "        feat = np.reshape(feat,(feat.shape[0],1))\n",
    "    return feat\n",
    "\n",
    "#This method should throw an error if something goes wrong\n",
    "def read_feature(file_path):\n",
    "    f = open(file_path, 'rb')\n",
    "    feat = to_feature_shape(pickle.load(f))\n",
    "    f.close()\n",
    "    return feat\n",
    "    \n",
    "features_to_read = [\"overlap_title\",\n",
    "                 \"comm_auth\",\n",
    "                 \"temp_diff\",\n",
    "                \"citation_check\",\n",
    "                \"max_sim\",\n",
    "                \"peer_popularity\",\n",
    "                \"edge_check\",\n",
    "                \"LSA_distance\",\n",
    "                \"node_degree\",\n",
    "                \"succ_pred\",\n",
    "                \"temporal_fit\"]\n",
    "\n",
    "train_features_dict = dict()\n",
    "train_features_reduced_dict = dict()\n",
    "test_features_dict = dict()\n",
    "test_features_reduced_dict = dict()\n",
    "for name in features_to_read:\n",
    "    # Train\n",
    "    file_path = './features_train/'+name\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        train_features_dict[name] = this_feat\n",
    "        train_features_reduced_dict[name] = this_feat[to_keep_train,:]\n",
    "    except:\n",
    "        pass\n",
    "    # Train reduced\n",
    "    file_path = './features_train/'+name+'_reduced'\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        train_features_reduced_dict[name] = this_feat\n",
    "    except:\n",
    "        pass\n",
    "    # Test\n",
    "    file_path = './features_test/'+name\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        test_features_dict[name] = this_feat\n",
    "        test_features_reduced_dict[name] = this_feat[to_keep_test,:]\n",
    "    except:\n",
    "        pass\n",
    "    # Test reduced\n",
    "    file_path = './features_test/'+name+'_reduced'\n",
    "    try:\n",
    "        this_feat = read_feature(file_path)\n",
    "        test_features_reduced_dict[name] = this_feat\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c747d028dd67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_true_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_true_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_true_labels' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true_labels = read_feature('./features_train/true_labels')\n",
    "train_true_labels = np.reshape(train_true_labels,(train_true_labels.shape[0],))\n",
    "train_true_labels_reduced = train_true_labels[to_keep_train]\n",
    "test_true_labels = read_feature('./features_test/true_labels')\n",
    "test_true_labels = np.reshape(test_true_labels,(test_true_labels.shape[0],))\n",
    "test_true_labels_reduced = test_true_labels[to_keep_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3047619047619048\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "### Precompute Journal features ###\n",
    "###################################\n",
    "journals = [info[4] for info in node_info]\n",
    "journal_dict, journal_features = prep.journal_features(journals,node_dict,training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 554602\n",
      "1000 / 554602\n",
      "2000 / 554602\n",
      "3000 / 554602\n",
      "4000 / 554602\n",
      "5000 / 554602\n",
      "6000 / 554602\n",
      "7000 / 554602\n",
      "8000 / 554602\n",
      "9000 / 554602\n",
      "10000 / 554602\n",
      "11000 / 554602\n",
      "12000 / 554602\n",
      "13000 / 554602\n",
      "14000 / 554602\n",
      "15000 / 554602\n",
      "16000 / 554602\n",
      "17000 / 554602\n",
      "18000 / 554602\n",
      "19000 / 554602\n",
      "20000 / 554602\n",
      "21000 / 554602\n",
      "22000 / 554602\n",
      "23000 / 554602\n",
      "24000 / 554602\n",
      "25000 / 554602\n",
      "26000 / 554602\n",
      "27000 / 554602\n",
      "28000 / 554602\n",
      "29000 / 554602\n",
      "30000 / 554602\n",
      "31000 / 554602\n",
      "32000 / 554602\n",
      "33000 / 554602\n",
      "34000 / 554602\n",
      "35000 / 554602\n",
      "36000 / 554602\n",
      "37000 / 554602\n",
      "38000 / 554602\n",
      "39000 / 554602\n",
      "40000 / 554602\n",
      "41000 / 554602\n",
      "42000 / 554602\n",
      "43000 / 554602\n",
      "44000 / 554602\n",
      "45000 / 554602\n",
      "46000 / 554602\n",
      "47000 / 554602\n",
      "48000 / 554602\n",
      "49000 / 554602\n",
      "50000 / 554602\n",
      "51000 / 554602\n",
      "52000 / 554602\n",
      "53000 / 554602\n",
      "54000 / 554602\n",
      "55000 / 554602\n",
      "56000 / 554602\n",
      "57000 / 554602\n",
      "58000 / 554602\n",
      "59000 / 554602\n",
      "60000 / 554602\n",
      "61000 / 554602\n",
      "62000 / 554602\n",
      "63000 / 554602\n",
      "64000 / 554602\n",
      "65000 / 554602\n",
      "66000 / 554602\n",
      "67000 / 554602\n",
      "68000 / 554602\n",
      "69000 / 554602\n",
      "70000 / 554602\n",
      "71000 / 554602\n",
      "72000 / 554602\n",
      "73000 / 554602\n",
      "74000 / 554602\n",
      "75000 / 554602\n",
      "76000 / 554602\n",
      "77000 / 554602\n",
      "78000 / 554602\n",
      "79000 / 554602\n",
      "80000 / 554602\n",
      "81000 / 554602\n",
      "82000 / 554602\n",
      "83000 / 554602\n",
      "84000 / 554602\n",
      "85000 / 554602\n",
      "86000 / 554602\n",
      "87000 / 554602\n",
      "88000 / 554602\n",
      "89000 / 554602\n",
      "90000 / 554602\n",
      "91000 / 554602\n",
      "92000 / 554602\n",
      "93000 / 554602\n",
      "94000 / 554602\n",
      "95000 / 554602\n",
      "96000 / 554602\n",
      "97000 / 554602\n",
      "98000 / 554602\n",
      "99000 / 554602\n",
      "100000 / 554602\n",
      "101000 / 554602\n",
      "102000 / 554602\n",
      "103000 / 554602\n",
      "104000 / 554602\n",
      "105000 / 554602\n",
      "106000 / 554602\n",
      "107000 / 554602\n",
      "108000 / 554602\n",
      "109000 / 554602\n",
      "110000 / 554602\n",
      "111000 / 554602\n",
      "112000 / 554602\n",
      "113000 / 554602\n",
      "114000 / 554602\n",
      "115000 / 554602\n",
      "116000 / 554602\n",
      "117000 / 554602\n",
      "118000 / 554602\n",
      "119000 / 554602\n",
      "120000 / 554602\n",
      "121000 / 554602\n",
      "122000 / 554602\n",
      "123000 / 554602\n",
      "124000 / 554602\n",
      "125000 / 554602\n",
      "126000 / 554602\n",
      "127000 / 554602\n",
      "128000 / 554602\n",
      "129000 / 554602\n",
      "130000 / 554602\n",
      "131000 / 554602\n",
      "132000 / 554602\n",
      "133000 / 554602\n",
      "134000 / 554602\n",
      "135000 / 554602\n",
      "136000 / 554602\n",
      "137000 / 554602\n",
      "138000 / 554602\n",
      "139000 / 554602\n",
      "140000 / 554602\n",
      "141000 / 554602\n",
      "142000 / 554602\n",
      "143000 / 554602\n",
      "144000 / 554602\n",
      "145000 / 554602\n",
      "146000 / 554602\n",
      "147000 / 554602\n",
      "148000 / 554602\n",
      "149000 / 554602\n",
      "150000 / 554602\n",
      "151000 / 554602\n",
      "152000 / 554602\n",
      "153000 / 554602\n",
      "154000 / 554602\n",
      "155000 / 554602\n",
      "156000 / 554602\n",
      "157000 / 554602\n",
      "158000 / 554602\n",
      "159000 / 554602\n",
      "160000 / 554602\n",
      "161000 / 554602\n",
      "162000 / 554602\n",
      "163000 / 554602\n",
      "164000 / 554602\n",
      "165000 / 554602\n",
      "166000 / 554602\n",
      "167000 / 554602\n",
      "168000 / 554602\n",
      "169000 / 554602\n",
      "170000 / 554602\n",
      "171000 / 554602\n",
      "172000 / 554602\n",
      "173000 / 554602\n",
      "174000 / 554602\n",
      "175000 / 554602\n",
      "176000 / 554602\n",
      "177000 / 554602\n",
      "178000 / 554602\n",
      "179000 / 554602\n",
      "180000 / 554602\n",
      "181000 / 554602\n",
      "182000 / 554602\n",
      "183000 / 554602\n",
      "184000 / 554602\n",
      "185000 / 554602\n",
      "186000 / 554602\n",
      "187000 / 554602\n",
      "188000 / 554602\n",
      "189000 / 554602\n",
      "190000 / 554602\n",
      "191000 / 554602\n",
      "192000 / 554602\n",
      "193000 / 554602\n",
      "194000 / 554602\n",
      "195000 / 554602\n",
      "196000 / 554602\n",
      "197000 / 554602\n",
      "198000 / 554602\n",
      "199000 / 554602\n",
      "200000 / 554602\n",
      "201000 / 554602\n",
      "202000 / 554602\n",
      "203000 / 554602\n",
      "204000 / 554602\n",
      "205000 / 554602\n",
      "206000 / 554602\n",
      "207000 / 554602\n",
      "208000 / 554602\n",
      "209000 / 554602\n",
      "210000 / 554602\n",
      "211000 / 554602\n",
      "212000 / 554602\n",
      "213000 / 554602\n",
      "214000 / 554602\n",
      "215000 / 554602\n",
      "216000 / 554602\n",
      "217000 / 554602\n",
      "218000 / 554602\n",
      "219000 / 554602\n",
      "220000 / 554602\n",
      "221000 / 554602\n",
      "222000 / 554602\n",
      "223000 / 554602\n",
      "224000 / 554602\n",
      "225000 / 554602\n",
      "226000 / 554602\n",
      "227000 / 554602\n",
      "228000 / 554602\n",
      "229000 / 554602\n",
      "230000 / 554602\n",
      "231000 / 554602\n",
      "232000 / 554602\n",
      "233000 / 554602\n",
      "234000 / 554602\n",
      "235000 / 554602\n",
      "236000 / 554602\n",
      "237000 / 554602\n",
      "238000 / 554602\n",
      "239000 / 554602\n",
      "240000 / 554602\n",
      "241000 / 554602\n",
      "242000 / 554602\n",
      "243000 / 554602\n",
      "244000 / 554602\n",
      "245000 / 554602\n",
      "246000 / 554602\n",
      "247000 / 554602\n",
      "248000 / 554602\n",
      "249000 / 554602\n",
      "250000 / 554602\n",
      "251000 / 554602\n",
      "252000 / 554602\n",
      "253000 / 554602\n",
      "254000 / 554602\n",
      "255000 / 554602\n",
      "256000 / 554602\n",
      "257000 / 554602\n",
      "258000 / 554602\n",
      "259000 / 554602\n",
      "260000 / 554602\n",
      "261000 / 554602\n",
      "262000 / 554602\n",
      "263000 / 554602\n",
      "264000 / 554602\n",
      "265000 / 554602\n",
      "266000 / 554602\n",
      "267000 / 554602\n",
      "268000 / 554602\n",
      "269000 / 554602\n",
      "270000 / 554602\n",
      "271000 / 554602\n",
      "272000 / 554602\n",
      "273000 / 554602\n",
      "274000 / 554602\n",
      "275000 / 554602\n",
      "276000 / 554602\n",
      "277000 / 554602\n",
      "278000 / 554602\n",
      "279000 / 554602\n",
      "280000 / 554602\n",
      "281000 / 554602\n",
      "282000 / 554602\n",
      "283000 / 554602\n",
      "284000 / 554602\n",
      "285000 / 554602\n",
      "286000 / 554602\n",
      "287000 / 554602\n",
      "288000 / 554602\n",
      "289000 / 554602\n",
      "290000 / 554602\n",
      "291000 / 554602\n",
      "292000 / 554602\n",
      "293000 / 554602\n",
      "294000 / 554602\n",
      "295000 / 554602\n",
      "296000 / 554602\n",
      "297000 / 554602\n",
      "298000 / 554602\n",
      "299000 / 554602\n",
      "300000 / 554602\n",
      "301000 / 554602\n",
      "302000 / 554602\n",
      "303000 / 554602\n",
      "304000 / 554602\n",
      "305000 / 554602\n",
      "306000 / 554602\n",
      "307000 / 554602\n",
      "308000 / 554602\n",
      "309000 / 554602\n",
      "310000 / 554602\n",
      "311000 / 554602\n",
      "312000 / 554602\n",
      "313000 / 554602\n",
      "314000 / 554602\n",
      "315000 / 554602\n",
      "316000 / 554602\n",
      "317000 / 554602\n",
      "318000 / 554602\n",
      "319000 / 554602\n",
      "320000 / 554602\n",
      "321000 / 554602\n",
      "322000 / 554602\n",
      "323000 / 554602\n",
      "324000 / 554602\n",
      "325000 / 554602\n",
      "326000 / 554602\n",
      "327000 / 554602\n",
      "328000 / 554602\n",
      "329000 / 554602\n",
      "330000 / 554602\n",
      "331000 / 554602\n",
      "332000 / 554602\n",
      "333000 / 554602\n",
      "334000 / 554602\n",
      "335000 / 554602\n",
      "336000 / 554602\n",
      "337000 / 554602\n",
      "338000 / 554602\n",
      "339000 / 554602\n",
      "340000 / 554602\n",
      "341000 / 554602\n",
      "342000 / 554602\n",
      "343000 / 554602\n",
      "344000 / 554602\n",
      "345000 / 554602\n",
      "346000 / 554602\n",
      "347000 / 554602\n",
      "348000 / 554602\n",
      "349000 / 554602\n",
      "350000 / 554602\n",
      "351000 / 554602\n",
      "352000 / 554602\n",
      "353000 / 554602\n",
      "354000 / 554602\n",
      "355000 / 554602\n",
      "356000 / 554602\n",
      "357000 / 554602\n",
      "358000 / 554602\n",
      "359000 / 554602\n",
      "360000 / 554602\n",
      "361000 / 554602\n",
      "362000 / 554602\n",
      "363000 / 554602\n",
      "364000 / 554602\n",
      "365000 / 554602\n",
      "366000 / 554602\n",
      "367000 / 554602\n",
      "368000 / 554602\n",
      "369000 / 554602\n",
      "370000 / 554602\n",
      "371000 / 554602\n",
      "372000 / 554602\n",
      "373000 / 554602\n",
      "374000 / 554602\n",
      "375000 / 554602\n",
      "376000 / 554602\n",
      "377000 / 554602\n",
      "378000 / 554602\n",
      "379000 / 554602\n",
      "380000 / 554602\n",
      "381000 / 554602\n",
      "382000 / 554602\n",
      "383000 / 554602\n",
      "384000 / 554602\n",
      "385000 / 554602\n",
      "386000 / 554602\n",
      "387000 / 554602\n",
      "388000 / 554602\n",
      "389000 / 554602\n",
      "390000 / 554602\n",
      "391000 / 554602\n",
      "392000 / 554602\n",
      "393000 / 554602\n",
      "394000 / 554602\n",
      "395000 / 554602\n",
      "396000 / 554602\n",
      "397000 / 554602\n",
      "398000 / 554602\n",
      "399000 / 554602\n",
      "400000 / 554602\n",
      "401000 / 554602\n",
      "402000 / 554602\n",
      "403000 / 554602\n",
      "404000 / 554602\n",
      "405000 / 554602\n",
      "406000 / 554602\n",
      "407000 / 554602\n",
      "408000 / 554602\n",
      "409000 / 554602\n",
      "410000 / 554602\n",
      "411000 / 554602\n",
      "412000 / 554602\n",
      "413000 / 554602\n",
      "414000 / 554602\n",
      "415000 / 554602\n",
      "416000 / 554602\n",
      "417000 / 554602\n",
      "418000 / 554602\n",
      "419000 / 554602\n",
      "420000 / 554602\n",
      "421000 / 554602\n",
      "422000 / 554602\n",
      "423000 / 554602\n",
      "424000 / 554602\n",
      "425000 / 554602\n",
      "426000 / 554602\n",
      "427000 / 554602\n",
      "428000 / 554602\n",
      "429000 / 554602\n",
      "430000 / 554602\n",
      "431000 / 554602\n",
      "432000 / 554602\n",
      "433000 / 554602\n",
      "434000 / 554602\n",
      "435000 / 554602\n",
      "436000 / 554602\n",
      "437000 / 554602\n",
      "438000 / 554602\n",
      "439000 / 554602\n",
      "440000 / 554602\n",
      "441000 / 554602\n",
      "442000 / 554602\n",
      "443000 / 554602\n",
      "444000 / 554602\n",
      "445000 / 554602\n",
      "446000 / 554602\n",
      "447000 / 554602\n",
      "448000 / 554602\n",
      "449000 / 554602\n",
      "450000 / 554602\n",
      "451000 / 554602\n",
      "452000 / 554602\n",
      "453000 / 554602\n",
      "454000 / 554602\n",
      "455000 / 554602\n",
      "456000 / 554602\n",
      "457000 / 554602\n",
      "458000 / 554602\n",
      "459000 / 554602\n",
      "460000 / 554602\n",
      "461000 / 554602\n",
      "462000 / 554602\n",
      "463000 / 554602\n",
      "464000 / 554602\n",
      "465000 / 554602\n",
      "466000 / 554602\n",
      "467000 / 554602\n",
      "468000 / 554602\n",
      "469000 / 554602\n",
      "470000 / 554602\n",
      "471000 / 554602\n",
      "472000 / 554602\n",
      "473000 / 554602\n",
      "474000 / 554602\n",
      "475000 / 554602\n",
      "476000 / 554602\n",
      "477000 / 554602\n",
      "478000 / 554602\n",
      "479000 / 554602\n",
      "480000 / 554602\n",
      "481000 / 554602\n",
      "482000 / 554602\n",
      "483000 / 554602\n",
      "484000 / 554602\n",
      "485000 / 554602\n",
      "486000 / 554602\n",
      "487000 / 554602\n",
      "488000 / 554602\n",
      "489000 / 554602\n",
      "490000 / 554602\n",
      "491000 / 554602\n",
      "492000 / 554602\n",
      "493000 / 554602\n",
      "494000 / 554602\n",
      "495000 / 554602\n",
      "496000 / 554602\n",
      "497000 / 554602\n",
      "498000 / 554602\n",
      "499000 / 554602\n",
      "500000 / 554602\n",
      "501000 / 554602\n",
      "502000 / 554602\n",
      "503000 / 554602\n",
      "504000 / 554602\n",
      "505000 / 554602\n",
      "506000 / 554602\n",
      "507000 / 554602\n",
      "508000 / 554602\n",
      "509000 / 554602\n",
      "510000 / 554602\n",
      "511000 / 554602\n",
      "512000 / 554602\n",
      "513000 / 554602\n",
      "514000 / 554602\n",
      "515000 / 554602\n",
      "516000 / 554602\n",
      "517000 / 554602\n",
      "518000 / 554602\n",
      "519000 / 554602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520000 / 554602\n",
      "521000 / 554602\n",
      "522000 / 554602\n",
      "523000 / 554602\n",
      "524000 / 554602\n",
      "525000 / 554602\n",
      "526000 / 554602\n",
      "527000 / 554602\n",
      "528000 / 554602\n",
      "529000 / 554602\n",
      "530000 / 554602\n",
      "531000 / 554602\n",
      "532000 / 554602\n",
      "533000 / 554602\n",
      "534000 / 554602\n",
      "535000 / 554602\n",
      "536000 / 554602\n",
      "537000 / 554602\n",
      "538000 / 554602\n",
      "539000 / 554602\n",
      "540000 / 554602\n",
      "541000 / 554602\n",
      "542000 / 554602\n",
      "543000 / 554602\n",
      "544000 / 554602\n",
      "545000 / 554602\n",
      "546000 / 554602\n",
      "547000 / 554602\n",
      "548000 / 554602\n",
      "549000 / 554602\n",
      "550000 / 554602\n",
      "551000 / 554602\n",
      "552000 / 554602\n",
      "553000 / 554602\n",
      "554000 / 554602\n",
      "overlap_title 554602\n",
      "comm_auth 554602\n",
      "temp_diff 554602\n",
      "peer_popularity 554602\n",
      "edge_check 554602\n",
      "LSA_distance 554602\n",
      "succ_pred 554602\n",
      "temporal_fit 554602\n",
      "journal_cites 554602\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-6d17f961b410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Concatenate all features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mfeats_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minsert_features_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mtrain_true_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_true_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "###  Construct features on TRAINING_SET  ###\n",
    "############################################\n",
    "\n",
    "#Build KDTree on training_set\n",
    "train_l = [l[node_dict[i]] for i in train_IDs]\n",
    "train_kdtree = nw.KDTree(train_l)\n",
    "\n",
    "train_true_labels = []\n",
    "features_to_create = ['journal_cites']\n",
    "# Where to insert created features\n",
    "insert_features_dict = train_features_dict\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "set_to_use = training_set\n",
    "\n",
    "for i,triple in enumerate(set_to_use):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "    # convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    # remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\") \n",
    "    \n",
    "    source_journal = source_info[4]\n",
    "    target_journal = target_info[4]\n",
    "    \n",
    "    # Creating features\n",
    "    # Baseline #\n",
    "    #overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "    #insert_features_dict[\"overlap_title\"].append(overlap_title)\n",
    "    #temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    #insert_features_dict[\"temp_diff\"].append(temp_diff)\n",
    "    #comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    #insert_features_dict[\"comm_auth\"].append(comm_auth)\n",
    "\n",
    "    #peer_pop = pw.peer_popularity(train_graph,source,target)\n",
    "    #insert_features_dict[\"peer_popularity\"].append(peer_pop)\n",
    "\n",
    "    #max_sim = pw.Max_Sim(source,target,l,train_graph,node_dict)\n",
    "    #insert_features_dict[\"max_sim\"].append(max_sim)\n",
    "\n",
    "    #edge_check = pw.edge_check(source,target,train_graph)\n",
    "    #insert_features_dict[\"edge_check\"].append(edge_check)\n",
    "\n",
    "    #LSA_dist = pw.LSA_distance(source,target,node_dict,l)\n",
    "    #insert_features_dict[\"LSA_distance\"].append(LSA_dist)\n",
    "    \n",
    "    #title_weighted = title_sim[index_source,index_target]\n",
    "    #insert_features_dict[\"title_sim\"].append(title_weighted)\n",
    "    \n",
    "    #citation_check = pw.Citation_Check(source,target,train_kdtree,l,train_graph,node_dict,index_dict,k=500)\n",
    "    #insert_features_dict[\"citation_check\"].append(citation_check)\n",
    "\n",
    "    #degree = pw.node_degree(source,target,train_graph)\n",
    "    #insert_features_dict[\"node_degree\"].append(degree)\n",
    "\n",
    "    #succ_pred = pw.succ_pred(source,target,train_graph)\n",
    "    #insert_features_dict[\"succ_pred\"].append(succ_pred)\n",
    "    \n",
    "    #temp_fit = pw.temp_fit(source,target,train_graph,node_dict,publication_years)\n",
    "    #insert_features_dict['temporal_fit'].append(temp_fit)\n",
    "    \n",
    "    journal_cites = journal_features[journal_dict[source_journal],journal_dict[target_journal]]\n",
    "    insert_features_dict[\"journal_cites\"].append(journal_cites)\n",
    "    \n",
    "    train_true_labels.append(triple[2])\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(i,\"/\",len(set_to_use))\n",
    "\n",
    "# Reshape features into np column arrays, one row per node pair\n",
    "for (name,value) in insert_features_dict.items():\n",
    "    print(name,len(value))\n",
    "    insert_features_dict[name] = to_feature_shape(value)\n",
    "        \n",
    "# Concatenate all features\n",
    "feats_train = np.concatenate([feat for feat in insert_features_dict.values()])\n",
    "train_true_labels = np.array(train_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27730, 1)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   train_features_reduced_dict['LSA_distance'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current wd:  /home/lucas/Documents/École Polytechnique/Courses/ML1/Challenge/git_repository/ML_project\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "###   Write features to disk - Training  ###\n",
    "############################################\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def write_feature_to_disk(feat,file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(feat,file)\n",
    "\n",
    "print(\"Current wd: \",os.getcwd())\n",
    "write_feature_to_disk(train_features_dict['journal_cites'],'./features_train/journal_cites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'features_pairwise' from '/home/lucas/Documents/École Polytechnique/Courses/ML1/Challenge/git_repository/ML_project/features_pairwise.py'>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(pw)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 60910\n",
      "1000 / 60910\n",
      "2000 / 60910\n",
      "3000 / 60910\n",
      "4000 / 60910\n",
      "5000 / 60910\n",
      "6000 / 60910\n",
      "7000 / 60910\n",
      "8000 / 60910\n",
      "9000 / 60910\n",
      "10000 / 60910\n",
      "11000 / 60910\n",
      "12000 / 60910\n",
      "13000 / 60910\n",
      "14000 / 60910\n",
      "15000 / 60910\n",
      "16000 / 60910\n",
      "17000 / 60910\n",
      "18000 / 60910\n",
      "19000 / 60910\n",
      "20000 / 60910\n",
      "21000 / 60910\n",
      "22000 / 60910\n",
      "23000 / 60910\n",
      "24000 / 60910\n",
      "25000 / 60910\n",
      "26000 / 60910\n",
      "27000 / 60910\n",
      "28000 / 60910\n",
      "29000 / 60910\n",
      "30000 / 60910\n",
      "31000 / 60910\n",
      "32000 / 60910\n",
      "33000 / 60910\n",
      "34000 / 60910\n",
      "35000 / 60910\n",
      "36000 / 60910\n",
      "37000 / 60910\n",
      "38000 / 60910\n",
      "39000 / 60910\n",
      "40000 / 60910\n",
      "41000 / 60910\n",
      "42000 / 60910\n",
      "43000 / 60910\n",
      "44000 / 60910\n",
      "45000 / 60910\n",
      "46000 / 60910\n",
      "47000 / 60910\n",
      "48000 / 60910\n",
      "49000 / 60910\n",
      "50000 / 60910\n",
      "51000 / 60910\n",
      "52000 / 60910\n",
      "53000 / 60910\n",
      "54000 / 60910\n",
      "55000 / 60910\n",
      "56000 / 60910\n",
      "57000 / 60910\n",
      "58000 / 60910\n",
      "59000 / 60910\n",
      "60000 / 60910\n",
      "overlap_title 60910\n",
      "comm_auth 60910\n",
      "temp_diff 60910\n",
      "citation_check 60910\n",
      "max_sim 60910\n",
      "peer_popularity 60910\n",
      "edge_check 60910\n",
      "LSA_distance 60910\n",
      "succ_pred 60910\n",
      "temporal_fit 60910\n",
      "journal_cites 60910\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-7e9c5fc6db70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Concatenate all features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mfeats_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minsert_features_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0mtest_true_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_true_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "###  Construct features on TESTING_SET  ###\n",
    "###########################################\n",
    "\n",
    "#Build KDTree on training_set\n",
    "train_l = [l[node_dict[i]] for i in train_IDs]\n",
    "train_kdtree = nw.KDTree(train_l)\n",
    "\n",
    "test_true_labels = []\n",
    "features_to_create = ['journal_cites']\n",
    "\n",
    "# Where to insert created features\n",
    "insert_features_dict = test_features_dict\n",
    "for feat in features_to_create:\n",
    "    insert_features_dict[feat] = []\n",
    "set_to_use = testing_set\n",
    "\n",
    "test_true_labels = []\n",
    "for i,triple in enumerate(set_to_use):\n",
    "    source = triple[0]\n",
    "    target = triple[1]\n",
    "    index_source = node_dict[source]\n",
    "    index_target = node_dict[target]\n",
    "    \n",
    "    source_info = node_info[index_source]\n",
    "    target_info = node_info[index_target]\n",
    "\n",
    "    # convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    # remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\") \n",
    "    \n",
    "    source_journal = source_info[4]\n",
    "    target_journal = target_info[4]\n",
    "    \n",
    "    \n",
    "    # Creating features\n",
    "    # Baseline #\n",
    "    #overlap_title = len(set(source_title).intersection(set(target_title)))\n",
    "    #insert_features_dict[\"overlap_title\"].append(overlap_title)\n",
    "    #temp_diff = int(source_info[1]) - int(target_info[1])\n",
    "    #insert_features_dict[\"temp_diff\"].append(temp_diff)\n",
    "    #comm_auth = len(set(source_auth).intersection(set(target_auth)))\n",
    "    #insert_features_dict[\"comm_auth\"].append(comm_auth)\n",
    "\n",
    "    #peer_pop = pw.peer_popularity(train_graph,source,target)\n",
    "    #insert_features_dict[\"peer_popularity\"].append(peer_pop)\n",
    "\n",
    "    #max_sim = pw.Max_Sim(source,target,l,train_graph,node_dict)\n",
    "    #insert_features_dict[\"max_sim\"].append(max_sim)\n",
    "\n",
    "    #edge_check = pw.edge_check(source,target,train_graph)\n",
    "    #insert_features_dict[\"edge_check\"].append(edge_check)\n",
    "\n",
    "    #LSA_dist = pw.LSA_distance(source,target,node_dict,l)\n",
    "    #insert_features_dict[\"LSA_distance\"].append(LSA_dist)\n",
    "    \n",
    "    #title_weighted = title_sim[index_source,index_target]\n",
    "    #insert_features_dict[\"title_sim\"].append(title_weighted)\n",
    "    \n",
    "    #citation_check = pw.Citation_Check(source,target,train_kdtree,l,train_graph,node_dict,index_dict,k=500)\n",
    "    #insert_features_dict[\"citation_check\"].append(citation_check)\n",
    "    \n",
    "    #succ_pred = pw.succ_pred(source,target,train_graph)\n",
    "    #insert_features_dict[\"succ_pred\"].append(succ_pred)\n",
    "    \n",
    "    #degree = pw.node_degree(source,target,train_graph)\n",
    "    #insert_features_dict[\"node_degree\"].append(degree)\n",
    "\n",
    "    #temp_fit = pw.temp_fit(source,target,train_graph,node_dict,publication_years)\n",
    "    #insert_features_dict['temporal_fit'].append(temp_fit)\n",
    "    \n",
    "    journal_cites = journal_features[journal_dict[source_journal],journal_dict[target_journal]]\n",
    "    insert_features_dict[\"journal_cites\"].append(journal_cites)\n",
    "    \n",
    "    test_true_labels.append(triple[2])\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(i,\"/\",len(set_to_use))\n",
    "\n",
    "# Reshape features into np column arrays, one row per node pair\n",
    "for (name,value) in insert_features_dict.items():\n",
    "    print(name,len(value))\n",
    "    insert_features_dict[name] = to_feature_shape(value)\n",
    "        \n",
    "# Concatenate all features\n",
    "feats_test = np.concatenate([feat for feat in insert_features_dict.values()])\n",
    "test_true_labels = np.array(test_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current wd:  /home/lucas/Documents/École Polytechnique/Courses/ML1/Challenge/git_repository/ML_project\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "###   Write features to disk - Testing  ###\n",
    "###########################################\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def write_feature_to_disk(feat,file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(feat,file)\n",
    "\n",
    "print(\"Current wd: \",os.getcwd())\n",
    "write_feature_to_disk(test_features_dict['journal_cites'],'./features_test/journal_cites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27730, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap_title 1\n",
      "comm_auth 1\n",
      "temp_diff 1\n",
      "citation_check 4\n",
      "max_sim 9\n",
      "peer_popularity 1\n",
      "edge_check 1\n",
      "LSA_distance 1\n",
      "succ_pred 4\n",
      "temporal_fit 2\n",
      "journal_cites 2\n"
     ]
    }
   ],
   "source": [
    "#Combine all features to one vector\n",
    "train_features_reduced_dict.keys()\n",
    "test_features_dict.keys()\n",
    "for key,feat in train_features_reduced_dict.items():\n",
    "    print(key,feat.shape[1])\n",
    "train_features_reduced = np.concatenate([train_features_reduced_dict['overlap_title'],\n",
    "                                         train_features_reduced_dict['comm_auth'],\n",
    "                                         train_features_reduced_dict['temp_diff'],\n",
    "                                         train_features_reduced_dict['citation_check'],\n",
    "                                         train_features_reduced_dict['max_sim'],\n",
    "                                         train_features_reduced_dict['peer_popularity'],\n",
    "                                         train_features_reduced_dict['edge_check'],\n",
    "                                         train_features_reduced_dict['succ_pred'],\n",
    "                                         train_features_reduced_dict['LSA_distance'],\n",
    "                                         train_features_reduced_dict['temporal_fit'],\n",
    "                                         train_features_reduced_dict['journal_cites']]\n",
    "                                        ,axis = 1)\n",
    "\n",
    "test_features = np.concatenate(        [test_features_dict['overlap_title'],\n",
    "                                        test_features_dict['comm_auth'],\n",
    "                                        test_features_dict['temp_diff'],\n",
    "                                        test_features_dict['citation_check'],\n",
    "                                        test_features_dict['max_sim'],\n",
    "                                        test_features_dict['peer_popularity'],\n",
    "                                        test_features_dict['edge_check'],\n",
    "                                        test_features_dict['succ_pred'],\n",
    "                                        test_features_dict['LSA_distance'],\n",
    "                                        test_features_dict['temporal_fit'],\n",
    "                                        test_features_dict['journal_cites']]\n",
    "                                        ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'features_pairwise' from '/home/lucas/Documents/École Polytechnique/Courses/ML1/Challenge/git_repository/ML_project/features_pairwise.py'>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27730, 27)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_features_reduced)\n",
    "normalized_train_features = scaler.transform(train_features_reduced)\n",
    "scaler.fit(test_features)\n",
    "normalized_test_features = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6700377606304384 f1: 0.7147520508671832\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.LinearSVC()\n",
    "#selection = [0, 1, 16, 20, 22, 23, 24]\n",
    "selection = [25,26]\n",
    "\n",
    "classifier.fit(normalized_train_features[:,selection], train_true_labels_reduced)\n",
    "preds_svm = list(classifier.predict(normalized_test_features[:,selection]))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_svm)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_svm)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9511410277458545 f1: 0.9543754215980866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "selection = [0, 3, 4, 8, 13, 16, 20, 22, 23, 24]\n",
    "model = lr().fit(normalized_train_features[:,selection], train_true_labels_reduced)\n",
    "preds_lg = list(model.predict(normalized_test_features[:,selection]))\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_lg)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_lg)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9420456411098341 f1: 0.9455901846542741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "selection = [1, 2, 6, 13, 16, 20, 21, 22]\n",
    "nNhbr = KNeighborsClassifier()\n",
    "nNhbr.fit(train_features_reduced[:,selection],train_true_labels_reduced) # do Ytrain.ravel() for length one Y values\n",
    "preds_knn = nNhbr.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_knn)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_knn)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9338368084058447 f1: 0.9387426278348634\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "selection = [0, 1, 2, 8, 13, 16, 20, 22]\n",
    "dTree = DecisionTreeClassifier()\n",
    "dTree.fit(train_features_reduced[:,selection],train_true_labels_reduced) # do Ytrain.ravel() for length one Y values\n",
    "preds_dt = dTree.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_dt)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_dt)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9540788267644363 f1: 0.9573653306101607\n"
     ]
    }
   ],
   "source": [
    "# Joined forces\n",
    "joined_DTree = DecisionTreeClassifier()\n",
    "\n",
    "preds_test_svm = np.reshape(preds_svm,(len(preds_svm),1))\n",
    "preds_test_lg = np.reshape(preds_lg,(len(preds_lg),1))\n",
    "preds_test_knn = np.reshape(preds_knn,(len(preds_knn),1))\n",
    "preds_test_dt = np.reshape(preds_dt,(len(preds_dt),1))\n",
    "combined_preds = np.concatenate([preds_test_svm,preds_test_lg,preds_test_knn,preds_test_dt],axis=1)\n",
    "\n",
    "joined_DTree.fit(combined_preds[0:50000,:], test_true_labels[0:50000])\n",
    "preds_joined = joined_DTree.predict(combined_preds[50000:,:])\n",
    "\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels[50000:])), list(map(int,preds_joined)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels[50000:])), list(map(int,preds_joined)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7900837300935807 f1: 0.7648075932602457\n",
      "features:  [13, 1, 8, 16, 15, 12, 14]\n",
      "acc: 0.8700213429650304 f1: 0.8682498211046579\n",
      "features:  [15, 11, 18, 6, 16]\n",
      "acc: 0.9041208340174027 f1: 0.9051424487541825\n",
      "features:  [20, 0, 15, 10, 7, 17, 11]\n",
      "acc: 0.9397143326219012 f1: 0.942607064707721\n",
      "features:  [2, 20, 4, 18, 1, 19, 14, 11]\n",
      "acc: 0.9424560827450337 f1: 0.9457673799687447\n",
      "features:  [11, 6, 14, 20, 2, 1, 18, 0]\n",
      "\n",
      "number of classifiers:  1359\n",
      "number of accs > 0.9: 118\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select random features and train a decision tree on it\n",
    "from time import clock\n",
    "\n",
    "random.seed(0)\n",
    "total_num_features = train_features_reduced.shape[1]\n",
    "time_limit = 600\n",
    "start_time = clock()\n",
    "feature_selections = []\n",
    "accs = []\n",
    "f1s = []\n",
    "while clock() < start_time + time_limit:\n",
    "    num_select_features = random.randint(1,10)\n",
    "    selection = random.sample(range(total_num_features),num_select_features)\n",
    "    \n",
    "    model = lr().fit(train_features_reduced[:,selection], train_true_labels_reduced)\n",
    "    preds = list(model.predict(test_features[:,selection]))\n",
    "\n",
    "    #nNhbr = KNeighborsClassifier()\n",
    "    #nNhbr.fit(train_features_reduced[:,selection],train_true_labels_reduced) # do Ytrain.ravel() for length one Y values\n",
    "    #preds = nNhbr.predict(test_features[:,selection])\n",
    "    \n",
    "    #classifier = svm.LinearSVC()\n",
    "    #classifier.fit(train_features_reduced[:,selection], train_true_labels_reduced)\n",
    "    #preds = list(classifier.predict(test_features[:,selection]))\n",
    "\n",
    "    #dTree = DecisionTreeClassifier()\n",
    "    #dTree.fit(train_features_reduced[:,selection],train_true_labels_reduced)\n",
    "    #preds = dTree.predict(test_features[:,selection])\n",
    "    \n",
    "    acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "    f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "    accs.append(acc)\n",
    "    f1s.append(f1)\n",
    "    feature_selections.append(selection)\n",
    "    if acc == max(accs):\n",
    "        print('acc:',acc,'f1:',f1)\n",
    "        print('features: ',selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of classifiers:  1023\n",
      "number of accs > 0.93 :  335\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Which features are important?\n",
    "# Histogram of the feature frequency for all selections that reached > 90% acc\n",
    "frequency = [0]*total_num_features\n",
    "\n",
    "num_good_preds = 0\n",
    "min_acc = 0.93\n",
    "for i,acc in enumerate(accs):\n",
    "    if acc > min_acc:\n",
    "        num_good_preds += 1\n",
    "        for f in feature_selections[i]:\n",
    "            frequency[f] += 1\n",
    "#frequency = [freq/num_good_preds for freq in frequency]\n",
    "print(\"\")\n",
    "print(\"number of classifiers: \",len(accs))\n",
    "print(\"number of accs >\",min_acc,\": \",sum([1 for acc in accs if acc > min_acc]))\n",
    "plt.figure()\n",
    "plt.bar(x=range(len(frequency)),height=frequency)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Model  0  out of  1024\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-45456d70c915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#f1s.append(f1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mfeature_selections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_selection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'f1:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'features: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_selection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "# Select random features and train a decision tree on it\n",
    "from time import clock\n",
    "import itertools\n",
    "\n",
    "random.seed(0)\n",
    "total_num_features = train_features_reduced.shape[1]\n",
    "selection = [0,1,2,5,6,16,18,20,21,22]\n",
    "time_limit = 600\n",
    "start_time = clock()\n",
    "feature_selections = []\n",
    "#accs = []\n",
    "#f1s = []\n",
    "model_number = 0\n",
    "for r in range(len(selection)+1):\n",
    "    if r == 0:\n",
    "        continue\n",
    "    print(r)\n",
    "    for c in itertools.combinations(selection,r):\n",
    "        print(\"Model \",model_number,\" out of \",2**len(selection))\n",
    "        model_number += 1\n",
    "        sub_selection = list(c)\n",
    "        #classifier = svm.LinearSVC()\n",
    "        #classifier.fit(train_features_reduced[:,sub_selection], train_true_labels_reduced)\n",
    "        #preds = list(classifier.predict(test_features[:,sub_selection]))\n",
    "\n",
    "        #acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "        #f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "        #accs.append(acc)\n",
    "        #f1s.append(f1)\n",
    "        feature_selections.append(sub_selection)\n",
    "        if acc == max(accs):\n",
    "            print('acc:',acc,'f1:',f1)\n",
    "            print('features: ',sub_selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9456082745033656,\n",
       " 0.9460187161385651,\n",
       " 0.9459694631423412,\n",
       " 0.9457396158266295,\n",
       " 0.9459037924807092,\n",
       " 0.9457067804958135,\n",
       " 0.9459530454769332,\n",
       " 0.9453948448530619,\n",
       " 0.9458052864882613,\n",
       " 0.9459858808077491]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9572812346084387 f1: 0.9604270592529505\n"
     ]
    }
   ],
   "source": [
    "# Adaboost DecisionTrees\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "selection = [1, 2, 6, 13, 16, 20, 22, 0]\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=500,learning_rate=0.1)\n",
    "ada.fit(normalized_train_features[:,selection],train_true_labels_reduced)\n",
    "preds_ada = ada.predict(normalized_test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_ada)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_ada)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9196847808241668 f1: 0.9239593372089409\n"
     ]
    }
   ],
   "source": [
    "# Adaboost LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "selection = [0, 1, 2, 8, 13, 16, 20, 22]\n",
    "ada = AdaBoostClassifier(lr(),n_estimators=500,learning_rate=1)\n",
    "ada.fit(train_features_reduced[:,selection],train_true_labels_reduced)\n",
    "preds_ada = ada.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_ada)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_ada)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9540305368576588 f1: 0.9574519815220034\n"
     ]
    }
   ],
   "source": [
    "# ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "selection = [1, 2, 6, 13, 16, 20, 22, 0]\n",
    "extraTrees = ExtraTreesClassifier(n_estimators=100)\n",
    "extraTrees.fit(train_features_reduced[:,selection],train_true_labels_reduced)\n",
    "preds_extra = extraTrees.predict(test_features[:,selection])\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds_extra)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds_extra)))\n",
    "print(\"acc:\",acc,\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "class Class_Net():\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, batch_size=60):\n",
    "        ''' initialize the classifier with default (best) parameters '''\n",
    "        tf.reset_default_graph()\n",
    "        self.alpha = learning_rate\n",
    "        self.beta = batch_size\n",
    "        self.warm = False\n",
    "\n",
    "    def fit(self,X,Y,warm_start=True,n_epochs=50):\n",
    "        ''' train the network, and if warm_start, then do not reinit. the network\n",
    "            (if it has already been initialized)\n",
    "        '''\n",
    "        self.epochs=n_epochs\n",
    "\n",
    "        self.n_batch = int(len(X)/self.beta)\n",
    "        \n",
    "        if warm_start==False or self.warm==False:\n",
    "            self.x = tf.placeholder(tf.float32,shape=[None,len(X[0])])\n",
    "            self.y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "            \n",
    "            self.wZero = tf.get_variable('wZero',shape=[len(X[0]),50],initializer=tf.glorot_uniform_initializer())\n",
    "            self.bZero = tf.Variable(tf.zeros([50]))\n",
    "\n",
    "            self.wOne = tf.get_variable('wOne',shape=[50,1],initializer=tf.glorot_uniform_initializer())\n",
    "            self.bOne = tf.Variable(tf.zeros([1]))\n",
    "            self.keep_prob = 0.9\n",
    "            self.drop_out = tf.nn.dropout(self.x, self.keep_prob)\n",
    "            self.model = tf.nn.sigmoid(tf.matmul(tf.nn.relu(tf.matmul(self.drop_out, self.wZero) \n",
    "                                              + self.bZero),self.wOne)+self.bOne)\n",
    "            self.cost = tf.losses.log_loss(self.y,self.model)\n",
    "            \n",
    "#             self.optimizer = tf.train.GradientDescentOptimizer(self.alpha).minimize(self.cost)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.alpha).minimize(self.cost)\n",
    "\n",
    "            #without dropout\n",
    "#             self.model = tf.matmul(tf.nn.relu(tf.matmul(self.x, self.wZero) + self.bZero),self.wOne)+self.bOne\n",
    "\n",
    "    \n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        with tf.Session() as sess:\n",
    "            if warm_start==False or self.warm==False:\n",
    "                sess.run(self.init)\n",
    "            else:\n",
    "                self.saver.restore(sess, './tempVariables.ckpt')\n",
    "            for epoch in range(self.epochs):\n",
    "                self.avg_cost = 0\n",
    "                for i in range(self.n_batch):\n",
    "                    _, self.c = sess.run([self.optimizer,self.cost], feed_dict={self.x: X[i*self.beta:min([(i+1)*\n",
    "                                    self.beta,len(X)]),:],self.y:Y[i*self.beta:min([(i+1)*self.beta,len(X)])]})\n",
    "                    \n",
    "                    self.avg_cost = self.avg_cost+np.mean(self.c)/self.n_batch\n",
    "                print(\"Epoch:\", '%s' % (epoch+1), \"cost=\", \"%s\"% (self.avg_cost))\n",
    "            self.saver.save(sess,'./tempVariables.ckpt')\n",
    "            \n",
    "        self.warm = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        ''' return a matrix P where P[i,j] = P(Y[i,j]=1), \n",
    "        for all instances i, and labels j. '''\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, './tempVariables.ckpt')\n",
    "            self.preds = sess.run(tf.nn.softmax(self.model), feed_dict={self.x: X}) \n",
    "        return self.preds\n",
    "    \n",
    "    def predict(self,X):\n",
    "        ''' return a matrix of predictions for X '''\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Class_Net(learning_rate=0.01,batch_size=250)\n",
    "net.fit(features,list(map(lambda x: [x],true_labels)),n_epochs=35)\n",
    "preds=net.predict(test_features)\n",
    "acc = metrics.accuracy_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "f1 = metrics.f1_score(list(map(int,test_true_labels)), list(map(int,preds)))\n",
    "print('acc:',acc,'f1:',f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
